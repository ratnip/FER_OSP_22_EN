[["index.html", "Let’s program in R (“Statistical Programming Fundamentals” course book) Foreword", " Let’s program in R Damir Pintar 2023-01-16 (“Statistical Programming Fundamentals” course book) ** NOTE: During the current academic year, the “Statistical Programming Fundamentals” is revised chapter by chapter. After the subject is New chapters will be dynamically added to this HTML document. If you need to have access to all the materials, contact the author of the tutorial at damir.pintar@fer.hr** Foreword This tutorial is based on interactive lessons used in the “Statistical Programming Fundamentals” at the Faculty of Electrical Engineering and Computing at the University of Zagreb. But the topics discussed here are not only useful to the students of the mentioned faculty - knowledge of the language R will be good both in academia and in the business world. Although R is known as a “programming language made up of statisticians, for statisticians” and is most often associated with the field of data science within which it is used for complex statistical analysis and data mining, it can be very useful for tasks related to the management of smaller or larger data at tasks that are not necessarily strictly oriented to advanced analytics. Namely, popular graphical tools with their interactive tabular presentation are very intuitive and excellent for simpler tasks, but as the need for more complex tasks appears, they quickly lose their efficiency and simplicity; on the other hand, the interactive program approach offered by R is initially somewhat more demanding but long-term highly cost-effective because complex tasks can be dealt with in an efficient, consistent and insightful way. For this reason, in the business world there is a clear shifting trend from classic GUI office tools to platforms with better support for more complex calculations and the creation of attractive visualizations. This is evidenced by a strong increase in the popularity of R language and other platforms with similar approach to data analysis. The aforementioned popularity of R language results in an increased need for learning resources, which are not currently very much present in Croatia. This coursebook will try to make learning R as easy and interesting as possible through its “learning through examples” approach. Emphasis will be placed primarily on mastering R as a programming language. For this reason, the initial chapters will focus on “programmatical aspects”, followed by a review of available tools presumed to be useful for the widest set of readers - tools for data gathering, extracting useful information, and visualizations. Since R is a domain-oriented language, R’s role in its support for statistical analysis will be reviewed followed byexamining selected machine learning methods and their applications. Although there will be enough information to put all the presented methods into context, the idea of this textbook is not to teach readers statistics nor deeply enter the field of machine learning - the intention of the author is to intrigue readers to continue exploring this interesting area, adequately armed with platform knowledge that will enable all new knowledge is immediately practically applied in further research. "],["introduction.html", "1 Introduction 1.1 What is R? 1.2 Installing Software Support 1.3 Overview of the development interface RStudio 1.4 How to use this coursebook?", " 1 Introduction 1.1 What is R? 1.1.1 General facts about R Programming language R came from the programming language S, developed for the needs of Bell Telephone Laboratory owned by AT &amp; T corporation. It was designed as an internal statistical analysis tool. The basic philosophy of the S language (inherited by the programming language R) was domain orientation - ie facilitating work with data analysts without the need to adapt conventions to traditional programming languages. Language S gained significant popularity in business analysts and statisticians within the 80s and 90s, but is now only available through a commercial variant called S-PLUS. The programming language R was created at the University of Auckland (NZ) and is released under the GNU Open Code Code. The standard distribution of the R programming language consists of: “core” R, with basic functions and so called “core” base package that provides basic functionality a collection of additional packages (“base” - base and “recommended” - recommended) for data management, visualization and statistical analysis We must not ignore the excellent integration of R with a rich repository called CRAN (Comprehensive R Archive Network) that enables fast and easy installation of any packet from that repository, after which it becomes part of the local R installation. Since the R community is extremely productive when it comes to the development of new packages, often after the introduction of new experimental methods and algorithms, CRAN quickly offers packages that implement them. Also, strong and continuous enthusiasm of the R community for the enhancement of existing R elements alleviates or eliminates a large number of various deficiencies of the base language. R as a language can often be seen as a DIY project of sorts where, after getting acquainted with the supplied “factory” components (in this case basic functions and packages), the user begins to adapt his development environment by choosing a package that exactly matches their needs and preferences. Creativity and flexibility in using R is considered to be of great advantage, even though it may result in a certain informality and liberal approach to programming. This is occasionally not favoured by users from a more strict and formal programming background, used to having a clear set of guidelines and rules to be followed. Despite the exceptionally high acceptance of the R language for data analysis and the variety of options offered to the user, it is necessary to be aware of some of its limitations: R intensely uses RAM which has been considered a serious restriction for a long time. With the increase of the memory capacity in modern hardware systems, as well as the rise of processing engines which can integrate with R and relieve it of the intense memory requirements when it comes to large datasets, this limitation is much less important today. Still, the fact remains that R can quickly hog the available RAM of your machine, though it is often the result of the neglect or ignorance of a developer who has not adopted the “R” approach when it comes to programming, opting to leverage patterns borrowed from other programming languages which result in suboptimal R code. R is quite unconventional so the learning curve is initially steeper, especially for programmers accustomed to standard conventions of other programming languages. On the other hand, if viewed long-term, programming in R eventually becomes very quite simple and user-friendly since most of the complex tasks are abstracted into high-level functions that transparently perform low-level operative tasks. It is often said that R is more focused towards the goal we want to achieve and cares less about the exact way to reach it. R is not a “fast” language; although it is a language that is expected to work over large data sets, R is not optimized for performance speed or even for parallelism; although there is a great deal of effort to implement virtually all key routines in C which prevents significant slowdowns, and there are a number of packages that offer support for multithreading, the fact remains that R is not designed to have its scripts get executed as quickly as possible; If speed is a priority, it is often necessary to look for alternative solutions - which is why it is often said that R is primarily a research language, not a production language. R is primarily intended for interactive work, i.e. performing a series of machine instructions that are dynamically executed with the help of a program console. This is tailored to the standard process of data analysis where the analyst can download data, clean it, transform, develop models, test, etc. while getting continuous feedback from a computer, providing the opportunity to dynamically adapt the analysis process. This does not mean that programming language can not be programmed in a classical “procedural” way by developing algorithms encapsulated in functions that automatically perform their tasks after being called, however where R truly shines is while performing interactive tasks. This principle is also reflected when it comes to learning R; it’s a programming language which is much easier to learn through the interactive approach of performing specific, goal-oriented tasks, experimenting with data sets, trying out data exploration methods, and so on, rather than using a “classic” approach by designing scripts that implement some low-level programming jobs. 1.1.2 R alternatives Programming Language R is a popular but not the only solution for interactive data analysis and statistical programming. Below we will give a brief overview of some of the more popular technologies and solutions used today for this purpose, with a brief comparison and a review of the advantages and disadvantages compared to language R. SAS and SPSS - SAS (Statistical Analysis System, developed by SAS Institute) and SPSS (Software Package for Statistical Analysis, developed by IBM) are two different software packages that we put under the same paragraph primarily because they are commercial tools, ie tools that require full payment for their full functionality. Similarly, SAS and SPSS are relatively easy to learn and their functionality is largely based on carefully designed user interfaces. These tools emphasize efficiency and are an excellent option for large companies looking for a consistent, robust solution for their analytics, not bothered by the commercial nature of such solutions. Weka and Orange - Weka (Waikato Environment for Knowledge Analysis, developed by Waikato University in New Zealand) and Orange* (deep data analysis software developed at the University of Ljubljana) are free software for exploratory data analysis and data mining that base their functionality on relatively simple graphing interfaces and visual programming approach. These solutions are very good for users who are not too demanding in terms of the flexibility and complexity of their analysis because they allow the implementation of defined steps in the analysis process in a very accessible and clear way. This does not mean that these tools can not do more complex analysis, but they are still more suited to analyses through the predefined functionality of the provided graphical interface. Python (Numpy / Pandas / Scikit) - in the last couple of years, Python is the most serious competitor of language R, primarily because Python is a very popular programming language with a very similar process approach to data analysis compared to one used by language R. The discussion of which language to choose is very common in the field of data science, usually without a clear final conclusion. The actual differences are not as big as those discussions may make it seem - while R is strongly domain-oriented and emphasis is placed on ease of use with a wide range of available overlapping packages to enable the user to choose the one that best suits him, Python emphasizes the rigid formal structure and principle “for one job one way of doing”. Therefore, it could be said that R is somewhat more suitable for “data research” while Python’s advantage is easier development and integration of analytical modules in a production environment, especially if said environment is already implemented in Python. But with the strong development of both languages and the mutual overlapping of functionality, this distinction becomes less relevant - it is no longer a problem to integrate R scripts into existing systems regardless of the platform they are running on, and the Python community is developing its versions of popular packages from R that faithfully emulate their functionality. Ultimately, it can be said that the initial choice between these two alternatives is not so important - the approach they use is so similar and the functionality sharing is so pronounced that learned concepts are easily transferable between both languages. It also must be said that the RStudio environment allows for mixing Python and R code in a single report, further closing the divide between these languages. Rapidminer - this is a cloud-based data science software platform that provides an integrated environment for data preparation, machine learning, deep learning, text mining, and predictive analytics. It is used for business and commercial applications as well as for research, education, training, rapid prototyping, and application development and supports all steps of the machine learning process including data preparation, results visualization, model validation and optimization. Rapidminer is primarily template-based, however it is possible to write extensions for it using R or Python. 1.2 Installing Software Support Installing the R language support software is pretty simple, especially if the recommended development interface RStudio is used as a platform. This is not the only option - one of the popular alternatives is the multilingual platform Jupyter Notebook which offers its own R support. Readers are encouraged to explore all available options and choose the final selection of the interface that personally matches their needs best; however this coursebook strongly recommends choosing RStudio mainly because of a clear, user-friendly and functionality-rich interface, easy installation and a very rich support for a variety of functionalities - from installing new packages, easy retrieval of documentation, creating visualizations and publishing reports, to integrating with other data science-related software. Therefore the rest of the coursebook will assume that readers are using the RStudio IDE. To successfully set up R software, you need to install the following: R language distribution RStudio integrated development environment It is recommended to use the latest available versions. At the time of writing this book, they are R 4.0 and RStudio 1.3. If these versions differ from those on your computer, there will probably be no problem if the version numbers are higher than the above; otherwise, their upgrade is recommended. The procedure for installing this software on the operating system Microsoft Windows will be described below. If you are working on some other operating system, such as a Linux distribution or Mac OS, the procedure is somewhat different, but still not too complex - it’s enough to do a web search focused on installing R and RStudio on a particular OS and follow further instructions. To find the software mentioned in the search engine, type the following terms: download R download RStudio In both cases, you will get links to the executable files that you have to run to install the software on your computer. In the case of R, this can be a file R-4.2.2-win.exe (exact numbers may differ). In the interface RStudio you can see more options - choose a free “desktop” version. Commercial versions have some additional functionalities that are mostly oriented to use in professional multi-user environments and are not essential to normal work. You can run the executable files and let the wizard install all the necessary components on your computer. It is recommended to accept all of the default installation options except for the choice of the installation folder - instead of the subfolder Program Files “it is better to install R directly in the root folder (eg”C:\\R\\R-4.2.2”), if possible. This way, it will be easier to check the currently installed version and potentially update it later. For the same reason, it is recommended that RStudio be installed in the folder “C:\\R\\RStudio”. If you do not have the option or you do not want to choose these folders, you can use others or leave the default options - this choice should not ultimately affect the further work. After installing RStudio it is highly recommended you create a separate subfolder where you will be doing most of your R related work (for example, folder “C:\\R\\projects”). After launching RStudio, the application should look similar to the following image: Figure 1.1: RStudio interface layout If there are any problems, make sure that you have completed all of the installation steps listed above. Below we will deal with the details of the interface shown. Before continuing, you might consider creating a new “project” (File -&gt; New Project -&gt; New Directory) and create a project called “IntroToR” in the projects folder mentioned above. You can then later put all the resources related to this course in this project folder. When you want to open RStudio and continue working on tasks from this course, it will be enough to double-click the IntroToR.Rproj file in the above subfolder, or choose the “IntroToR” project in the upper right corner of the screen. 1.3 Overview of the development interface RStudio Let’s look at the interface RStudio. We see it is being divided into three windows - the left part is the “work area” with the programming console, waiting for us to enter instructions. On the right, there are auxiliary windows that show different things, depending on the selected tab; In the upper right hand corner, we can see, among other things, variables in our current working environment (which is initially empty). We can also choose to see command history, control connections to other data storage and analysis software etc.. The bottom part serves to display documentation, file structures, installed packages, visualizations, etc. 1.3.1 Interactive console Let’s go back to the left part of the interface, the “interactive console”. R, by its nature, is an “interpreter language” in the sense that the programming console expects from the user to enter commands which are then immediately interpreted and executed. Though it is possible to create larger scripts that are then executed “all at once”, working with the R language often boils down to using command-by-command principle. This is precisely why we are talking about “data analysis via interactive programming” - the analyst is “programming” by entering commands and can at any time study the obtained results, deciding on further steps. Let’s see how the interactive console works. With the help of a keyboard, you can type a simple math expression - eg 3 + 2 - and press the ENTER key. We will see that R will immediately deliver the result - if we want, we can easily use it as a calculator! For mathematical expressions that do not have predefined operator symbols we need to use functions. For example, a square root can be calculated using the sqrt () function. Let’s try typing sqrt(10) in the console and pressing ENTER. R again shows the result immediately. At this point, the screen should look like the next picture. Figure 1.2: R as a calculator One of the problems of using R this way is the messy mixing of commands and results, not allowing us an easy high-level overview of the sequence of commands we were performing. Furthermore, if for some reason the command that we execute results in an error (and we then keep trying to correct it by entering the proper command), the console soon becomes cluttered with error reports. This is why analysts often prefer using “R scripts” that allows for entering commands in a separate place before sending them to the console, visually distinguishing the procedure we want to execute with the obtained results gained after the planned instructions are executed. 1.3.2 Writing R scripts In the toolbar, select File -&gt; New File -&gt; R Script (or press the CTRL + SHIFT + N key combination). We see that the “working area” on the left becomes divided into two parts. The upper part represents the space for our “script” - actually the series of commands we want to execute - while the interactive console now occupies the lower part of the work surface. If we want, we can tweak the size of these (and other windows) by moving the bordering bars between them. Also, there’s an easy way of switching the focus between the two, through the usage of CTRL + 1 and CTRL + 2 key combinations. Try to write two simple commands in the scripting window - the first one should be print(\"hello!\") And underneath it a simple mathematical expression 3 + 4. Return the cursor to the first line and press the CTRL + ENTER key combination. If we have correctly followed these steps, the command at the cursor site will automatically be copied to the interactive console and executed. The cursor will now be the next command that you can do with CTRL + ENTER. The screen should now look similar to the next image. Figure 1.3: R script This is one of the common ways of working in RStudio- first we create some programming instructions in the script space, after which we send them to the console. If something is wrong with the command, we can easily modify it and perform it again. If you want to execute a block of commands, you can select them by clicking and dragging and then pressing the CTRL + ENTER key. Scripts can also contain comments (starting with the # character that R interprets as “ignore this line”). Finally, we can store our scripts in a file - by convention, R scripts have a simple extension R, for example myScript.R. However, we can go one step further. Despite the fact that the R scripts are quite adequate for comfortable work in the R language, there is an additional technology that gives us even more flexibility in working with the R - R Markdown. 1.3.3 R Markdown Writing R scripts is very similar to the classic concept of “programming” - we write commands that are (usually) executed sequentially, and optionally we add comments for the purpose of the documentation. However, data analysis process commonly involves dissemination of results via data science reports, either for the analysts themselves or to the intended audience. RStudio interface supports a technology that provides an effective combination of programming and structured documentation using the principle of “interactive notebooks”. Interactive notebooks allow the user to combine formatted text (optionally with formulas and figures) with executable code, and text formatting, and then insert executable code, ending up with a format resembling a notebook. For this RStudio uses the R Markdown technology, as well as its slightly updated cousin, R Notebook. Since the differences between these two are relatively superficial, we will focus on the slightly simpler one R Markdown. This technology is easiest to demonstrate via example - in the toolbar, select File -&gt; New File -&gt; R Markdown ... and in the next window choose an arbitrary title (eg \"Testing\"). You can optionally edit the metadata, such as add the author’s name, and finally choose one of the options for final form of report (HTML is recommended due to its lowest dependency on additional software). Unlike the R script, which will initially appear empty, when creating an RMD document RStudio will create a new pre-filled document. This is done in this way for the simple reason that the user gets an easily modifiable template with all the basic elements included as a reminder. We will delete most of this template for our needs - so feel free to remove everything after the initial header, i.e. below the second ---. Then, we can write any text below. We can also experiment with headers which start with #, ##, ### characters - these are NOT “R comments”, since we are now not programming, we are writing formatted text. By using * and ** characters in front and back of the words we can make letters become bold and italicized, respectively. Everything mentioned so far in this paragraph is pure “markdown”, which basically means “plain text with added formatting information that can be converted into formatted text with the help of additional tools, if desired”. When we want to incorporate executable code into our “report”, we need to create so-called “code chunks”. This can be done by selecting Insert -&gt; R on the toolbar or using the CTRL + ALT + I combination of keys. Notice that the chunk begins and ends with a specially selected string of characters - three so called “backticks” (they look like simple apostrophes, but are angled towards the left). Likewise, the beginning of the chunk contains description of certain parameters in the opening brackets, which can affect the way RStudio is processing this “code chunk”, most notably by having us state which programming language we will use. In this coursebook, we will almost exclusively use the language R, although other options are available if they had been previously installed on the platform running RStudio. The code chunk behaves the same as the standard R script - we can enter commands and execute them. The difference is just that the results can be seen immediately in the R Markdown document itself (in addition to them showing up in the console). If this is not something that we want, we can turn this functionality off (click on the gear in the toolbar and select Chunk output in console), however we usually prefer to have the results right below the code that created them, notebook-style. If we follow the instructions, the screen may look similar to the following image: Figure 1.4: R Markdown document We can now try to create a “report” from the current document. First, we must save it under a particular name (eg Testing.rmd), and then we can click on the Knit button to convert the document from pure text to an HTML file. (Note - if our platform is missing some packages, we will see that in the warning toolbar which will show up - we just need to pick the option “Install” and wait until RStudio downloads the required packages from the CRAN repository) R Markdown documents are much more powerful than it may seem judging by the elements that have been presented so far. Chunks can have plenty of different parameters which influence their behaviour. The output format can be PDF, DOCX as well as other forms such as presentation slides, books intended for mobile devices, interactive web applications etc. The coursebook you are reading is actually nothing more than a series of RMD files converted into the appropriate form you are currently using. As we will explain in the next chapter, RMD files are also the main way for you to use this coursebook effectively and try out the examples and tasks that follow. The universality and flexibility of technology R Markdown is exceptionally great, and is very well accepted by the R community. 1.4 How to use this coursebook? The basic idea of this coursebook is “learning through application”. Therefore, the lessons below will not focus on merely showing and talking about new concepts, but rather encouraging the reader to learn out each new concept by solving a series of easy or intermediate tasks. Each chapter that follows has an accompanying “workbook”. Simply put, it is an RMD file that contains all the examples from the lecture, accompanied by a concise text for easier reference to the concepts being dealt with. The basic idea is that the reader reads the coursebook and solves the workbook in parallel, looking at the task solution only after it is solved within the programming interface. Some exercises will be trivial, require simply removing the # sign (meaning “comment”) from the start of the command and executing it. In spite of the trivial approach, in this way, however, the reader is encouraged to independently test the command rather than just look at its result. Other exercises will be a bit more involved. Each exercise will also have a solution underneath (which will be hidden if using the interactive version of the coursebook). Readers are encouraged to only look at the solution after first solving the exercise themselves. Finally, each chapter will end with a set of additional exercises which will not have accompanying. It is recommended to successfully solve all these exercises, since the lessons that follow presume the well-accepted knowledge of all the topics that are discussed previously (and some exercises will sneakily introduce some helpful functions or tricks which may become very useful later on). Let’s get to know the concept of workbooks more closely. First, you need to download and open a workbook that matches the lesson you are reading. It is easy to recognize it by looking at the chapter number - the notebook for this chapter should be named 01_Introduction_WB.Rmd. It is recommended that all workbooks that you plan to use are copied somewhere to the local computer together with all the accompanying files, and then perhaps create a backup copy of the original workbook (since solving a workbook will change its contents). As stated previously, the workbook will typically contain all the program code in the chapter to which it refers, but usually only the minimal amount of text sufficient for easier understanding and solving the exercises. Workbooks will leverage two types of tasks: Examples and Exercises. Examples will be pieces of code that will just need to be executed, without any modification. Exercises on the other hand expect at least some changes, and often require entering completely new segments of program code. An Example might look like this: Example 1.1 - a few simple mathematical expressions 3 + 2 # adding log(10) # natural logarithm! log10(10) # this is a base-10 logarithm! By the way, we comment with the &quot;#&quot; sin(0.5 * pi) # pi is one of the built-in constants ## [1] 5 ## [1] 2.302585 ## [1] 1 ## [1] 1 You can execute the commands from the examples individually, or the entire chunk at once with the CTRL + SHIFT + ENTER key combination. No modification of the code is necessary (although you should always feel free to experiment further with the given commands, either in the chunk itself or in the console!). As stated, Exercises on the other hand always require a certain - even minimal - intervention. Exercise 1.1 - Commands for checking/changing the working directory # Make the following commands by removing the comment character #getwd() # current working directory #setwd(&quot;.&quot;) # here we can specify a new working folder if desired # usually not a good idea when working with RMD files getwd() # directory in which we are currently working setwd(&quot;.&quot;) # here we can specify a new working folder if desired # usually not a good idea when working with RMD files The exercise will often be related to the most recently introduced concept. For example, we might be learning the assignment operator, and explaining that although language R supports the usage of = operator for assigning a value to a variable, it is recommended to use the &lt;- operator for that purpose, for various reasons to be explained later (use ALT + - to quickly write &lt;-). Also, note that R supports the so-called autoprint functionality, when a command or a set of commands is executed, the value of the last expression will be shown on screen. This means that if we create a new variable x and want to print it on the screen, we do not have to put print(x)as the last command, but rather just x. Also, using an assignment operator does not return a value in itself, which is why we do not see anything printed out on screen after assigning a value to a variable. Let’s try to experiment with these concepts in the following exercise. Exercise 1.2 - Assignment operator # store `5` in a variable called `x` # then print the variable `x` on the screen x &lt;- 5 x ## [1] 5 Now that we have some basic knowledge about language R and the programming interface we will be using, we can begin by learning the basic elements of the R programming language. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],["tipovi.html", "2 Basic data types and operators 2.1 Basic data types 2.2 Operators 2.3 Missing, unknown, and non-existent values Homework exercises", " 2 Basic data types and operators “Basic” or “primitive” types of data are the underlying building blocks of programming languages. They are basically embedded mechanisms that allow storing basic information - most commonly of logical, numeric, or character nature. Most programming languages use very similar methods of storing such information, which means that we can expect to see similar basic data types in almost every programming language - the difference is often in certain implementation specifics such as the name of the basic type, the default number of bytes is uses etc. In any case, the most common first step in learning a new programming language is getting to know the basic types of data that it supports. The next thing that may interest us is the language syntax, that is, the specific way programming instructions are written so the programming language can adequately interpret them and execute. The R language syntax follows similar conventions seen in languages such as Python, Ruby, or Java, but with its own specifics. Some basic syntax rules are: each command must, as a rule, go to its own line, but the indentation and semicolons at the end are optional; programming blocks are delinated with brackets; variables types need not be declared in advance, their type is inferred based on the assigned value; comments start with #; etc. The rest of the syntax will be best learned through examples. We will start with basic operators and functions and work from there. 2.1 Basic data types R offers six basic data types: type examples logical TRUE, FALSE ili T, F integer 2L, 5L, 123456789L double 4, 6, 3.14, 2e5 complex 5 + 2i, 7 + 1i character \"A\", \"B\", \"Pero\", \"ABCDEFGHijklmnoPQRSTUVwyz\" raw as.raw(2), charToRaw(\"1\") Some remarks: integer and real types are often treated together as a numeric type (although this is not entirely consistent!) complex type must have a declared imaginary constant even if it is 1 (2 + i is not a good record!) The type of “raw” bytes is relatively rarely used Checking whether a variable is of a certain type can be done with the help of is.&lt;type_name&gt; functions. Before we get to test this function in the upcoming exercise, we will introduce one small trick: in exercises where we print more things on the screen, it is useful to visually separate the different result segments so that we can easily understand which part of the code is referenced. For this purpose, we will use the cat(\"-----------\\n\") command that simply prints a dashed line followed with the newline character. We opted for this solution instead of simply using the “empty” print(\"\") function because that function always prints out the index of the result as well as the string quotation marks, while the cat command is just a “raw” print, which is in this case more appropriate. Exercise 2.1 - checking data types # try the following commands: #is.logical(FALSE) #is.integer(2L) #is.double(1.11) # perform the following checks: # is 5L numeric? # is 3.14 numeric? # is &quot;ABC&quot; character? # is 4 + 2i complex? # is 5 integer? # try the following commands: is.logical(FALSE) is.integer(2L) is.double(1.11) cat(&quot;-----------\\n&quot;) # perform the following checks: # is 5L numeric? is.numeric(5L) # is 3.14 numeric? is.numeric(3.14) # is &quot;ABC&quot; character? is.character(&quot;ABC&quot;) # is 4 + 2i complex? is.complex(4 + 2i) # is 5 integer? is.integer(5) ## [1] TRUE ## [1] TRUE ## [1] TRUE ## ----------- ## [1] TRUE ## [1] TRUE ## [1] TRUE ## [1] TRUE ## [1] FALSE Types of some variables or constants can be retrieved with the help of typeof orclass functions. The difference between these functions is the following: typeof answers to the question “in which underlying format is this variable stored?”; hence, it will return the “primitive” or “basic” type ( integer, double) class answers to the question “what IS this variable?”; it will return the “object type”, which is actually the value stored in the class attribute of this variable Exercise 2.2 - data type retrieval # print the types of the following constants: TRUE, 2L, F, 3.14, &quot;ABC&quot; # print the classes of the same constants. Do you see any differences? # print the types of the following constants: TRUE, 2L, F, 3.14, &quot;ABC&quot; typeof(TRUE) typeof(2L) typeof(F) typeof(3.14) typeof(&quot;ABC&quot;) cat(&quot;-----------\\n&quot;) # print the classes of the same constants. Do you see any differences? class(TRUE) class(2L) class(F) class(3.14) class(&quot;ABC&quot;) ## [1] &quot;logical&quot; ## [1] &quot;integer&quot; ## [1] &quot;logical&quot; ## [1] &quot;double&quot; ## [1] &quot;character&quot; ## ----------- ## [1] &quot;logical&quot; ## [1] &quot;integer&quot; ## [1] &quot;logical&quot; ## [1] &quot;numeric&quot; ## [1] &quot;character&quot; Data can be explicitly converted from one type to another using the functions as.&lt;type_name&gt;: Exercise 2.3 - conversion of data types # perform the following conversions and print the result # 2.35 to integer # TRUE to ntomeric # 100L to character # 2.35 to character # 2e2 to character # 0 to logical # 2.75 to logical # perform the following conversions and print the result # 2.35 to integer as.integer(2.35) # TRUE to ntomeric as.numeric(TRUE) # 100L to character as.character(100L) # 2.35 to character as.character(2.35) # 2e2 to character as.character(2e2) # 0 to logical as.logical(0) # 2.75 to logical as.logical(2.75) ## [1] 2 ## [1] 1 ## [1] &quot;100&quot; ## [1] &quot;2.35&quot; ## [1] &quot;200&quot; ## [1] FALSE ## [1] TRUE R will always perform an implicit conversion of types if possible: Exercise 2.4 - implicit conversion # Write the following phrases and print the results: # arithmetic operator between logical and numeric variables # arithmetic operator between integer and numeric variables # logical operator negation (!) applied to numeric variable # arithmetic operator between logical and numeric variables TRUE + 5 # arithmetic operator between integer and numeric variables 5L + 3.14 # logical operator negation (!) applied to numeric variable !25 ## [1] 6 ## [1] 8.14 ## [1] FALSE Implicit conversion will only be performed if it is meaningful - for example, we cannot perform arithmetic operations between a character type and a numeric type. 2.2 Operators As in other programming languages, R allows using various operators when forming expressions. Some of the more frequently used operators are: arithmetic +, -, *, /, ^, %% (division reminder), %/% comparison &lt;, &lt;=, &gt;, &gt; =, ==, != logical ! (negation), &amp;&amp; (scalar AND), || (scalar OR), &amp; (vector AND), | (vector OR) assignment &lt;- or= Exercise 2.5 - operators # try the `5 / 2` and` 5 %/% 2` expressions # calculate the &quot;square of 17&quot; and &quot;the remainder of 101 divided by 12&quot; # check what is the result of the following expressions: `17 &gt; 13`,`!TRUE`, `5 &amp;&amp; 0`,`0. || 2` # try the `5 / 2` and` 5 %/% 2` expressions 5 / 2 5 %/% 2 cat(&quot;-----------\\n&quot;) # calculate the &quot;square of 17&quot; and &quot;the remainder of 101 divided by 12&quot; 17 ^ 2 101 %% 12 cat(&quot;-----------\\n&quot;) # check what is the result of the following expressions: `17 &gt; 13`,`!TRUE`, `5 &amp;&amp; 0`,`0. || 2` 17&gt; 13 ! TRUE 5 &amp;&amp; 0 0. || 2 ## [1] 2.5 ## [1] 2 ## ----------- ## [1] 289 ## [1] 5 ## ----------- ## [1] TRUE ## [1] FALSE ## [1] FALSE ## [1] TRUE Logical values and comparison operators will most commonly be used with the conditional expressions, better known as “if-else” commands. In R, the syntax of such commands looks like this: if (expression) {block} else {block} Let’s try this on the following task: Exercise 2.6 - conditional expressions # Write a command that performs the following: # &quot;if 100 is an even number print &#39;Success!&#39;&quot; # Write a command that performs the following: # &quot;if 100 is an even number print &#39;Success!&#39;&quot; if (100 %% 2 == 0) print(&quot;Success!&quot;) ## [1] &quot;Success!&quot; We have noted above that we have two types of logical operators for “and” and “or”. We will explain the difference later, for now it is enough to know that instructions that control the program flow exclusively use the “C++”-like operators &amp;&amp; i ||. Before, we have already mentioned that R offers two assignment operators, &lt;- and =. There are some minor differences, but those mostly pertain on using these operators in other contexts - when used purely for assigning a result of an expression to a new variable, their functionality is almost identical. In the literature, both versions can be seen for assigning values, but we will primarily and consistently use &lt;-, if nothing than because that way the code is visually more distinctive from code from the other programming languages. NOTE: an easy way to type in the &lt;- operator is to press the keys ALT and - The operands in the expression that uses the assignment operator are called the “lvalue” (left value) and “rvalue” (right value), respectively. Rvalue is an expression that needs to be calculated, while the left (lvalue) is interpreted as “the place where the result of the expression on the right will be stored”. This means that examples like this x + 1 &lt;- 2 # error !!!] do not work - it is not unambiguous what is being stored where. Therefore, as a rule, the lvalue is most commonly a (new or existing) variable, though in some occasions we will actually be seeing a function call. This may appear confusing, but there is a perfectly intuitive explanation for this which we will bring later. Naming the variables mostly follows the rules from other programming languages, with some slight peculiarities - we can use letters, numbers, underscores and dots (the dot . is treated like a regular character!). The first symbol must be a letter or a dot. .myVarijable &lt;- 5 #OK my.Variable &lt;- 5 #OK _myVariable &lt;- 5 # not OK 123Variable &lt;- 5 # not OK If we use variable names which consist of multiple words, it is customary tp pick one of the following conventions: myVariable &lt;- 1 # camelcase my_Variable &lt;- 2 # underscore seperation or my.variable &lt;- 3 # point separation Sticking consistently to one of these choice will result in a nicer, more readable code. R also allows a way to go around the variable-naming conventions, and to use any printable character in a variable name. If we want to use this functionality, we then must put this name inside “backticks”: Exercise 2.7 - variable name with special characters # Enter an arbitrary name with special characters inside the left apostrophes # and print the value of the variable # `` &lt;- 2 # Enter an arbitrary name with special characters inside the left apostrophes # and print the value of the variable # `` &lt;- 2 `!% ^$*@ __ =` &lt;- 2 `!% ^$*@ __ =` ## [1] 2 This way of naming variables is not too useful in practice, but it has its purpose - since the operators in R are actually functions (whose names are literally +, ^ etc.), by using left backticks we can directly reference them in their original form, which can be very practical in the so-called functional programming (which we will talk about in one of the future lessons). Assigning values to new variable names results in creation of new variables in the working environment (called the “global environment”). All variables we have created so far can be seen with the ls() function. If we want to delete some variables, we just provide their names in the call function rm(), without quotation marks (e.g. rm (x, y, z)). To delete all variables from the working environment, we can use the call rm(list=ls()), but this should be done with caution (there’s no “undo”!). Exercise 2.8 - printing and deleting global environment variables # print all of the global environment variables that have been created so far # delete some of the above variables - eg rm(x, y, z) # list all remaining variables # delete ALL variables from the global environment # (cautiously with this call in practice!) # Make sure the global environment is empty # print all of the global environment variables that have been created so far ls() # delete some of the above-written variables - eg rm(x, y, z) # list all remaining variables rm(x, y) ls() # delete ALL variables from the global environment # (cautiously with this call in practice!) # Make sure the global environment is empty rm(list=ls()) ls() Finally, whenever we need help with some function, we have the following options available: write only &lt;function_name&gt; (without parenthesis with parameters) and press - if the function is written in R (and not just proxy to a C implementation) the screen will print the original source code of the function Write help(&lt;function_name&gt;) or ?&lt;function_name&gt; to get a documentation page with the list of parameters, examples, and so on. Write example(&lt;function_name&gt;) where we get a list of examples of using said function The following code chunk shows how to use the above methods (to save space, we will not show the result, but feel free to try out these commands in the console). #program code for `ls` function ls # help for `ls` function ?ls # or help(ls) # examples of using the `ls` function example(ls) 2.3 Missing, unknown, and non-existent values In R there are three ways of modeling “non-existent” values: NA - (not available) - for missing or unknown values of a particular type NaN - (not a number) - “impossible” number, e.g. 0 / 0 NULL - non-existent value, literally “nothing” NaN is a subclass of NA. The difference between NA and NULL is subtle, but important. NA means that the value is possibly there, we just don’t know what it is. NA values still have a type, so an unknown numeric value is still numeric. NULL on the other hand is a placeholder for “nothing” - it is its own class, and it denotes that whatever we are referencing is empty. One important note for SQL users - what is NULL in SQL is NA in R. Do not be mixing SQL’s NULL and R’s NULL, since they are usually completely different things! Exercise 2.9 - working with NA, NaN and NULL # how much is &quot;5 + unknown number&quot; # how much is &quot;5 + non-existent number&quot; # check classes of the following: # NA # arithmetic operation between numeric and NA # NaN # NULL # how much is &quot;5 + unknown number&quot; 5 + NaN # how much is &quot;5 + non-existent number&quot; 5 + NA cat(&quot;-----------\\n&quot;) # check classes of the following: # NA # arithmetic operation between numeric and NA # NaN # NULL class(NA) # logical type is &quot;weakest&quot;! class(5 + NA) class(NaN) class(NULL) ## [1] NaN ## [1] NA ## ----------- ## [1] &quot;logical&quot; ## [1] &quot;numeric&quot; ## [1] &quot;numeric&quot; ## [1] &quot;NULL&quot; Checking missing values is similar to checking data types - we use the is.na,is.nan and is.null functions. Exercise 2.10 - check NA, NaN and NULL # which of the following is NA? NA, NaN, NULL, &quot;&quot;, 0 # which of the following is NaN? NA, NaN, NULL # which of the following is NULL? NA, NaN, NULL # which of the following is NA? NA, NaN, NULL, &quot;&quot;, 0 is.na(NA) is.na(NaN) is.na(NULL) is.na(&quot;&quot;) is.na(0) cat(&quot;-----------\\n&quot;) # which of the following is NaN? NA, NaN, NULL is.nan(NA) is.nan(NaN) is.nan(NULL) cat(&quot;-----------\\n&quot;) # which of the following is NULL? NA, NaN, NULL is.null(NA) is.null(NaN) is.null(NULL) ## [1] TRUE ## [1] TRUE ## logical(0) ## [1] FALSE ## [1] FALSE ## ----------- ## [1] FALSE ## [1] TRUE ## logical(0) ## ----------- ## [1] FALSE ## [1] FALSE ## [1] TRUE To end, we dedicate some room to a discussion of the NA value, since we will often encounter it in practice. Simply put, if the NA values appear, we can expect the following side effects: results of arithmetic expressions which contain NA as operands will result in NA values the results function calls which get NA as parameters can result with NA (but sometimes there is an option to use the parameter na.rm = T which actually means ‘ignore NA values’) logical expressions which contain NA may or may not necessarily result in a NA value, depending on whether the term depends on NA or not (eg TRUE || NA has the result of TRUE, but FALSE || NA has the result NA) With this last one, we must be especially careful as the NA in the conditional term results in a mistake: if (NA &lt; 2) print (&quot;Success!&quot;) # error !! In this lesson we have learned the basic elements of the language R. However, in practice, we much more often work with complex data types - vectors, matrices, data frames and lists - which are the subject of the following lesson. Homework exercises What is the result of the following commands? Try to predict the result before executing each command. as.complex(2) as.integer(-3.25) as.logical(&quot;0&quot;) as.numeric(TRUE) as.character(11.5 + 2i) as.numeric(&quot;ABC&quot;) How do the following expressions look like in R: “ten to the power of nine multiplied by 3” “natural logarithm of 5” “integer division of 10 by 3” “the remainder of integer division of 10 by 3” “tangent of 75 degrees” R has a special constant for “infinity”. Try to find what it is by using an arithmetic expression. Then, by using if, check whether reducing “infinity” from “infinity” results in a NaN value. Store NULL in a variable called x. Then try to convert this variable into a numeric type. Explain what you think has happened (full explanation will be given in the following chapter). Program in Ru &lt;/ span&gt; by Damir Pintar is licensed under Creative Commons Attribution-NonCommercial-NoDerivative 4.0 International License Based on a work at https://ratnip.github.io/FER_OPJR/ "],["vektori.html", "3 Vectors, matrices and lists 3.1 Vector 3.2 Index vectors 3.3 Matrices and arrays 3.4 Matrix slicing 3.5 Example 3.2 - matrix slicing 3.6 Lists Homework exercises", " 3 Vectors, matrices and lists 3.1 Vector The vector is the primary “complex” data types in the language R, in the sense that it can contain more values of the same type. It is similar to the term “array” in the C language. However, there is one important difference, which may be not immediately obvious. Unlike C, R does not really have “basic data types”, even though the last chapter may have given the impression it is so. In R, (almost) each variable type is actually a vector. Even the variables and constants we were showing in the previous lesson were actually single-element vectors. This revelation has far-reaching consequences to be discussed in detail below, but before delving into specifics we first need to get acquainted with the basic syntax of creating and managing vectors. 3.1.1 Creating a vector When we want to create a new vector variable which stores more then one value we must use the function simply named c (from combine or concatenate). # numeric vector m &lt;- c(1, 2, 3, 4, 5) # logic vector v &lt;- c(T, F, T) # character vector names &lt;- c(&quot;Ivo&quot;, &quot;Pero&quot;, &quot;Ana&quot;) So, simply stated, a vector is an arranged set of elements of the same type. If we create a new vector with elements of different types, R will automatically convert all elements into the “strongest” type, which will eventually become the type of vector itself (the term “stronger” type in this context means the type able to store everything without any loss of information which might happen if it was stored in a “weaker” type). Generally the conversion goes in the direction of logic -&gt; numeric -&gt; character types. Exercise 3.1 - creating vectors # create a new vector `x` with four arbitrary elements of the following types: # logical, double, character and integer # print the vector content and its class on screen # create a new vector `x` with four arbitrary elements of the following types: # logical, double, character and integer x &lt;- c(T, 1.25, &quot;Ivo&quot;, 10L) # print the vector content and its class on screen x class(x) ## [1] &quot;TRUE&quot; &quot;1.25&quot; &quot;Ivo&quot; &quot;10&quot; ## [1] &quot;character&quot; A vector can be explicitly converted to another type with the help of as.&lt;type_name&gt; functions, already introduced in the previous chapter. If unambiguous conversion is impossible, the element will be converted to NA, and the conversion will be accompanied with a suitable warning. Exercise 3.2 - explicit vector type conversion x &lt;- c(1, T, 2L) y &lt;- c(1L, 2L, 3L) z &lt;- c(1.25, TRUE, &quot;Ana&quot;) # consider what might be the result first, and then perform the following conversions # vector `x` in numeric type # vector `y` in character type # vector `z` in an integer type x &lt;- c(1, T, 2L) y &lt;- c(1L, 2L, 3L) z &lt;- c(1.25, TRUE, &quot;Ana&quot;) # consider what might be the result first, and then perform the following conversions # vector `x` in numeric type # vector `y` in character type # vector `z` in an integer type as.numeric(x) as.character(y) as.integer(z) ## Warning: NAs introduced by coercion ## [1] 1 1 2 ## [1] &quot;1&quot; &quot;2&quot; &quot;3&quot; ## [1] 1 NA NA Think about why in the last example the value TRUE became NA instead of 1L that you might have expected. If you cannot figure out the reason, try to print the z vector itself and notice the results of the implicit conversion you might have neglected (which converts the TRUE logical value to a string of \"TRUE\" that can no longer be interpreted as its logical counterpart, which would otherwise result in the numeric value 1L). 3.1.2 Vector concatenation Remember that we said c can mean “combine elements” but also concatenate? With the c function we can also concatenate multiple vectors to one: a &lt;- c(1, 2, 3) b &lt;- c(4, 5) c &lt;- c(6, 7, 8) # variable can be called &quot;c&quot; in spite of the function c() d &lt;- c(a, b, c) # d is now c(1, 2, 3, 4, 5, 6, 7, 8) When creating vectors, we are not confined to exclusively using the c function. In addition to it, R also offers multiple convenient ways of creating new vectors: : - “range” operator, giving the range from upper to lower bound, both included seq - sequence function, similar to the range operator but with additional options rep - replicate function, repeats the provided elements a certain number of times Exercise 3.3 - vector creation helper functions # print the results of the following commands # 1:5 # rep(c(1, 2, 3), times = 3) # rep(c(1, 2, 3), each = 3) # seq(1, 5, by = 0.5) # print the results of the following commands 1:5 rep(c(1, 2, 3), times = 3) rep(c(1, 2, 3), each = 3) seq(1, 5, by = 0.5) ## [1] 1 2 3 4 5 ## [1] 1 2 3 1 2 3 1 2 3 ## [1] 1 1 1 2 2 2 3 3 3 ## [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 Vectors can also be created by means of functions corresponding to the names of the vector types (numeric,character, etc.) whereby as a parameter we specify the desired length of the vector. This is often done as a “storage preparation” for the elements which will subsequently be filled in. If the length is not zero, the vector will be filled with the chosen number of appropriate zero-equivalents, depending on the actual type. Interestingly, we can also create an “empty” vector of a certain type, and it will still be considered a vector, only having the length of zero (and by using the c function we can easily add elements to it later). x &lt;- numeric(2) # vector is filled with &quot;zero&quot; elements, in this case (0, 0) y &lt;- character(5) z &lt;- integer(0) # &quot;empty&quot; vector! z &lt;- c(z, 1) # add to the vector the element 1 (actually &quot;merge empty vector and element 1&quot;) It is sometimes possible to get a zero-length vector as a result of an expression - for example, this often happens in conjunction with a NULL value, where the expression adds a type to literally “nothing”, resulting in an vector which is empty but still has a certain type. Finally, to check if some vector contains a certain element we can use the operator %in%: 4 %in% seq(1, 10, 2) # returns FALSE &quot;d&quot; %in% c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) # returns TRUE We learned how to create vectors, now let’s see how we can access their elements. 3.1.3 Operator [ The vector element is accessed through the index operator [, with which we can retrieve but also modify vector elements: a &lt;- c(2, 4, 6) a[1] # prints a value of 2 a[2] &lt;- 5 # element on the 2nd place becomes 5 a[5] &lt;- 7 # we add 7 to the 5th place, and the &quot;hole&quot; is filled with NA a ## [1] 2 ## [1] 2 5 6 NA 7 Notice a somewhat unusual fact - the first element of the vector in R has the index 1, not 0! This is an important difference compared to the indexing of elements in most other programming languages. The reason for this peculiarity is simple - R is primarily a language for data analysis, and when referring to data, especially in tabular form. When data analysts refers to their data, it is much easier to count rows or columns in order they appear in a data set than to constantly do a mental “add 1” calculation - so the first row is always “row 1”, never “row 0”, even though someone from programming background would readily recognize it as such. The example above actually shows a very simplified case for retrieving vector elements and modifying them. However, there is an extremely important feature of language R called vectorization, that - informally speaking - states that R often prefers doing “more things at once”. This has nothing to do with parallel execution of tasks, but rather with the declarative approach to defining tasks which would in other languages require low-level programming, most commonly using loops. Specifically, in the case of referencing vector elements, we rarely retrieve or modify elements one by one, but rather we refer to multiple elements at once using the principles of vectorization and recycling. Understanding these terms is crucial for learning the R language, so we will explain them in detail below. 3.1.4 Principles of vectorization and recycling The notion of vectorization or more precisely vectorized operator and functions simply means that most operator and functions by design work on multiple elements at once. If we ask R to make an operation or function over a vector of values, R will perform the function or operation over each element separately and package the results in a new, resulting vector. Likewise, if we use a binary operator on two vectors as operands, the corresponding operation will be performed over the “paired” or “aligned” elements of both vectors (supposing for now that the vectors are of the same length). Let’s demonstrate this in the following exercise. Exercise 3.4 - vectorization principle x &lt;- seq(-5, 5, 1) a &lt;- 1:3 b &lt;- 4:6 # call the abs function to calculate the absolute value # over the vector `x` and print the result # add vectors `a` and` b` with operator `+` # and print the result # multiply vectors `a` and `b` with operator `*` # and print the result # call the abs function to calculate the absolute value # over the vector `x` and print the result abs(x) cat(&quot;-----------\\n&quot;) # add vectors `a` and `b` with operator `+` # and print the result a + b cat(&quot;-----------\\n&quot;) # multiply vectors `a` and `b` with operator `*` # and print the result a * b ## [1] 5 4 3 2 1 0 1 2 3 4 5 ## ----------- ## [1] 5 7 9 ## ----------- ## [1] 4 10 18 Carefully consider the results of the previous task. If necessary, draw representations of the vector a and b on paper (with the elements arranged vertically) and notice how R does the “pairing” of the elements. Notice also that we are not really talking about “vector operations” here in a strict mathematical sense, but about aligning the elements of two vectors and performing simple operations over each of these pairs. This is especially evident in the last example where there is no “vector multiplication” in some of the mathematical interpretations, but rather simple multiplication of the parallel elements of the two vectors. What if the vectors are not of the same length? R in this case uses the principle of recycling. The Recycling Principle states that when the vectors are not of the same length, the shorter vector is “recycled” as many times as needed to reach the length of the longer vector. The most common scenarios of using this principle are operations where on one side we have a vector with multiple elements and on the other a single-element vector. What we should be trying to avoid though is a recycling scenario where the length of a “big” vector is not a multiple of the “small” length - R will still recycle a shorter vector, only it will have to be “cut off”, which will result in an appropriate warning. Exercise 3.5 - recycling principle a &lt;- 1:4 b &lt;- c(1, 2) c &lt;- rep(5, 3) # multiply vector `a` with the number 2 and print the result # divide vector `a` with vector `b` and print the result # multiply vectors `a` and `c` and print the result a &lt;- 1:4 b &lt;- c(1, 2) c &lt;- rep(5, 3) # duplicate vector `a` and print the result 2 * a # divide vector `a` with vector `b` and print result a / b # multiply vectors `a` and `c` and print the result a * c ## Warning in a * c: longer object length is not a multiple of shorter object ## length ## [1] 2 4 6 8 ## [1] 1 1 3 2 ## [1] 5 10 15 20 Now we can finally demystify the difference between “scalar” and “vector” logical operators. scalar logical operators &amp;&amp; and || are intended for use with single-element vectors, they return unique values of TRUE or FALSE and are suitable for use in various conditional terms. vector logical operators &amp; and | use standard R’s vectorization and recycling principles, i.e., they are intended to work with logical vectors and as a result give a logical vector Exercise 3.6 - scalar and vector logical operators a &lt;- c(T, F, F) b &lt;- c(T, T, F) # apply scalar and vector version of logical operator &quot;or&quot; # over the `a` and `b` vectors and print the result # apply scalar and vector version of logical operator &quot;or&quot; # over the `a` and `b` vectors and print the result a || b ## Warning in a || b: &#39;length(x) = 3 &gt; 1&#39; in coercion to &#39;logical(1)&#39; a | b ## [1] TRUE ## [1] TRUE TRUE FALSE We see that the scalar version will “use” only the first pair of logic vector elements. This means while it could theoretically be used conditional expressions, there is no justifiable reason for it, and R will in this case warn us to address the fact that we are probably using the “wrong” operator. The next example with the relational operators may initially seem trivial, but it is important to pay special attention to the results we get since they will have very important implications later on. Let’s take a look what happens when vectorization is applied in conjunction with relational operators. Exercise 3.7 - vectorization of relational operators x &lt;- 1:5 y &lt;- seq(-10, 10, 5) # print x and y # print the result of the x &gt; y command and explain the result # print the result of the x &lt; 3 command and explain the result # print x and y x y cat(&quot;-----------\\n&quot;) # print the result of the x &gt; y command and explain the result x&gt; y cat(&quot;-----------\\n&quot;) # print the result of the x &lt; 3 command and explain the result x &lt; 3 ## [1] 1 2 3 4 5 ## [1] -10 -5 0 5 10 ## ----------- ## [1] TRUE TRUE TRUE FALSE FALSE ## ----------- ## [1] TRUE TRUE FALSE FALSE FALSE Thus, by vectorizing the relational operators over the vectors (or combinations of vectors and scalars), as a result we get logical vectors. The interpretation of these results is crucial - it actually answers the question “on what positions is the condition of this expression fulfilled”? In other words, the results actually represent a template that describes how to filter elements of a vector based on a filtering expression. This is the basic foundation of the so-called. logical indexing, which is one of the vector indexing methods that we will learn below. 3.2 Index vectors We have already learned that a vector can be retrieved with a help of a numerical index (and we did not forget the fact that the first element has an index 1). This concept can be expanded by taking multiple elements from the vector at once. which is often referred to as “slicing”. The basic principle of choosing multiple elements at once is simple - we only need to specify the indexes of the elements we want. R offers three basic ways of indexing: integer- or position-based indexing conditional or boolean-based indexing label-based indexing Which indexing we choose depends on whether we want to access the elements based on their position, name, or condition, and each type of indexing essentially amounts to using a particular vector type as a parameter for the indexing operator. Such a vector is then called an “index vector”, based on its role in the expression. Let’s get to know each of the types of indexing in detail. 3.2.1 Positional Indexing Positional Indexing is the generalization of an already familiar indexing principle where we simply write the numeric position (index) of the element we are interested in. If we want more elements, we simply put their indices “packed” into a numeric vector. Try solving the next exercise by using the appropriate numeric vectors as indexing parameters. Exercise 3.8 - Positional indexing x &lt;- 1:10 # print the first element of the vector `x` # print the first three elements of the vector `x` # print the first, fifth, and seventh elements of the vector `x` # print the first element of the vector `x` x[1] # print the first three elements of the vector `x` x[1:3] # print the first, fifth, and seventh elements of the vector `x` x[c(1,5,7)] ## [1] 1 ## [1] 1 2 3 ## [1] 1 5 7 Thus, the positional index vector is simply the ordinary numeric vector we use in conjunction with the index operator [ to determine which elements of another vector we want to “keep”. Let’s look at some of the features of the positional index vector: Exercise 3.9 - Positional indexing (2) x &lt;- 1:10 # answer the following questions with the help of an appropriate example # what does index 0 return? # what does a negative index return? # what happens if you use an index outside of vector boundaries x &lt;- 1:10 # answer the following questions with the help of an appropriate example # what does index 0 return? x[0] # what does a negative index return? x[-1] # what happens if you use an index outside of vector boundaries x[20] ## integer(0) ## [1] 2 3 4 5 6 7 8 9 10 ## [1] NA Indexing is not only used to retrieve elements. By combining the indexing operator and the assignment operator we can change the vector elements (also leveraging the principle of “doing more things at once”): Exercise 3.10 - Positional indexing and assignment a &lt;- 1:10 # set all vector elements of `a` from the second to the eighth place to zero # print vector `a` b &lt;- 1:20 b [2 * 1:5] &lt;- 0 # Consider what the vector `b` looks like after the above command # print the vector `b` and explain the result a &lt;- 1:10 # set all vector elements of `a` from the second to the eighth place to zero # print vector `a` a [2:8] &lt;- 0 a b &lt;- 1:20 b [2 * 1:5] &lt;- NA # Consider what the vector `b` looks like after the above command # print the vector `b` and explain the result b ## [1] 1 0 0 0 0 0 0 0 9 10 ## [1] 1 NA 3 NA 5 NA 7 NA 9 NA 11 12 13 14 15 16 17 18 19 20 3.2.2 Conditional indexing If we carefully considered the results obtained with examples with vectorized relational operators then we can very easily grasp the way conditional indexing works. The principle is simple - for the index vector we choose a logical vector of the same length as the vector whose elements we want to retrieve. The elements of this logical vector determine which elements are retained (the positions where the value is TRUE) and which we throw out (positions where the value is FALSE). Exercise 3.11 - conditional indexing x &lt;- 1:10 # create a logical vector `y` of length 10 with an arbitrary combination of # TRUE and FALSE values # index the vector `x` with the `y` vector, print and explain the result # print all vector elements `x` which are less or equal to 5 # use the appropriate expression as a logical index vector x &lt;- 1:10 # create a logical vector `y` of length 10 with an arbitrary combination of # TRUE and FALSE values y &lt;-c(T, T, F, T, F, F, F, T, F, T) # index the vector `x` with the `y` vector, print and explain the result x[y] # print all vector elements `x` which are less or equal to 5 # use the appropriate expression as a logical index vector x[x &lt;= 5] ## [1] 1 2 4 8 10 ## [1] 1 2 3 4 5 The last command, while simple, is one of the key principles for filtering elements in the language R. The combination of the index operator and the conditional expression represents a concise but very powerful vector filtering mechanism. Let’s try this principle in a few more examples. Exercise 3.12 - conditional indexing y &lt;- seq (1, 100, 7) students &lt;- c(&quot;Ivo&quot;, &quot;Petra&quot;, &quot;Marijana&quot;, &quot;Ana&quot;, &quot;Tomislav&quot;, &quot;Tin&quot;) # print a vector which contains all even, and then all odd vector elements of `y` # (&quot;odd&quot; and &quot;even&quot; refers to element&#39;s value, not position) # print all vector elements from `students` which represent 3-letter names # (note: we use the `nchar` function to count the characters in R) y &lt;- seq (1, 100, 7) students &lt;- c(&quot;Ivo&quot;, &quot;Petra&quot;, &quot;Marijana&quot;, &quot;Ana&quot;, &quot;Tomislav&quot;, &quot;Tin&quot;) # print a vector which contains all even, and then all odd vector elements of `y` # (&quot;odd&quot; and &quot;even&quot; refers to element&#39;s value, not position) c(y[y %% 2 == 0], y[y %% 2 != 0]) # print all vector elements from `students` which represent 3-letter names # (note: we use the `nchar` function to count the characters in R) students[nchar(students) == 3] ## [1] 8 22 36 50 64 78 92 1 15 29 43 57 71 85 99 ## [1] &quot;Ivo&quot; &quot;Ana&quot; &quot;Tin&quot; If the concept of conditional indexing with the help of conditional expressions is still unclear, one of the things that can help is to sketch the intermediate results - simply print the result of the expression within the indexing brackets and then consider how that result affects the final solution. 3.2.3 Label-based indexing Label-based indexing works on the principle of explicitly naming the elements we want to “keep”. In order to be able to use this type of indexing though we must ensure one necessary prerequisite - we need to first assign names to our vector elements. The vectors we used so far did not have named elements. Each element had its predefined position within the vector and its value but did not have any additional special identifiers. Programming language R allows you to attach names to vector elements in a very simple way - by using a combination of the names function, the assignment operator, and the character vector with selected names. We need to make sure however that the vector name is of the same length as the original vector. Exercise 3.13 - label-based indexing height &lt;- c(165, 173, 185, 174, 190) names(height) &lt;- c(&quot;Marica&quot;, &quot;Pero&quot;, &quot;Josip&quot;, &quot;Ivana&quot;, &quot;Stipe&quot;) # print the vector `height` # print the height of Pero and Josip height &lt;- c(165, 173, 185, 174, 190) names(height) &lt;- c(&quot;Marica&quot;, &quot;Pero&quot;, &quot;Josip&quot;, &quot;Ivana&quot;, &quot;Stipe&quot;) # print the vector `height` height # print the height of Pero and Josip height[c(&quot;Pero&quot;, &quot;Josip&quot;)] ## Marica Pero Josip Ivana Stipe ## 165 173 185 174 190 ## Pero Josip ## 173 185 We see that label-based indexing needs a corresponding character vector as the index parameter. (NOTE: A more careful reader will notice an unusual fact in the above code - it uses a function call as an lvalue! Understanding why this is possible requires some slightly advanced knowledge about the inner workings of the R language. For now it is enough to know that the above exercise isn’t really calling a function called names, but rather a function called names&lt;-, a member of a special kind of “assignment” functions which allow using the intuitive and easily readable syntax seen above) If for some reason we want to delete the names of vector elements, simply forward NULL to names(&lt;vector_name&gt;). names(height) &lt;- NULL This will conclude the story of the index vectors. We learned different ways of creating a vector and getting and modifying its elements. Now is the time to try to add the additional “dimension” to the vectors - by getting acquainted with matrices and the arrays. 3.3 Matrices and arrays Out simply - the matrices and the arrays are what you get when you add more dimensions to vectors. Matrix is a two-dimensional vector, i.e. a vector whose elements are organized in “rows” and “columns”. Array is a vector with three or more dimensions. While matrices are used relatively often in practice, the arrays are somewhat limited to very specific use cases. Because of this fact in this chapter we will mainly deal with matrices, although the concepts presented are very easily applicable to arrays too. One common thing about the matrices and arrays, which is a well-known fact to readers who come from programming backgrounds, is that their multidimensionality is actually completely virtual. Both the matrix and the array are actually one-dimensional vectors and it’s only the usage of multidimensional indices which allows these structures to behave as if they are truly multidimensional. On other words, when the programmer uses a multidimensional index, R simply maps it to the “real”, one-dimensional index of the element in the underlying one-dimensional vector. This fact is not limiting in the slightest though - programmatically, we can still in most cases treat the matrix as if it is truly a two-dimensional structure; the knowledge of its underlying one-dimensional nature can only give us additional flexibility in working with the matrices, and also allows us to write better code. There are several ways to create a new matrix: by fitting a one-dimensional vector inside a matrix structure: we use the matrix function with this one-dimensional vector as a parameter, followed by the desired number of rows and/or columns provided through the nrow and ncol parameters “manually” by setting the “dimension attribute” of a one dimensional vector using the dim function with a given two-element numeric vector for dimensions “binding” rows or columns together with functions rbind (row-bind) and cbind (column-bind) Let’s try to see this in action in the following exercise. Exercise 3.14 - the matrix function x &lt;- 1:12 # create a matrix with 3 rows and 4 columns using the `matrix` function # print the result on the screen # repeat the procedure but add the parameter `byrow = T` to the calling function # print the result on the screen and compare it to the previous result # create a matrix with 3 lines and 4 columns using the `matrix` function # print the result on the screen matrix(x, nrow = 3, ncol = 4) # repeat the procedure but add the parameter `byrow = T` to the calling function # print the result on the screen and compare it to the previous result matrix(x, nrow = 3, ncol = 4, byrow = T) ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 Note that unless explicitly requested otherwise, the R matrix is always filled by columns. This is done mostly because in data analysis we usually perform operations by columns. But since we often feel that filling by the rows is more “natural”, we must not forget the option of using the very useful parameter byrow. Exercise 3.15 - the dim function m &lt;- 1:10 # print the result of call of the `dim` function to the vector `m` (checking the dimensions) # use the `dim` function on vector`m` with the parameter `c(2, 5)` # print `m` and comment the result # print the results of calling functions `nrow` and` ncol` on the matrix `m` m &lt;- 1:10 # print the result of call of the `dim` function to the vector `m` (checking the dimensions) dim(m) # use the `dim` function on vector`m` with the parameter `c(2, 5)` dim(m) &lt;- c(2, 5) # print `m` and comment the result m # print the results of calling functions `nrow` and` ncol` on the matrix `m` nrow(m) ncol(m) ## NULL ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 3 5 7 9 ## [2,] 2 4 6 8 10 ## [1] 2 ## [1] 5 We see that the “ordinary” vector does not actually have a dimension, which is manifested by the NULL values we get as a result when asking for it. By invoking the function dim we can add the similarly called attribute dim which formally turns this vector into a matrix (or array in a general case). Provided dimensions dictate how the elements are (virtually!) organized in rows and columns, which means that when choosing the dimensions we must take care they correspond to the current number of elements. Once the matrix has its dimensions added, we can retrieve them again by using the dim function, or just the number of rows or columns with the nrow and ncol functions. The resulting matrix is like the one in the previous example filled in by the columns. Since here we do not have the opportunity to use the byrow parameter, one of the ways to get a matrix filled by rows is to transpose the resulting result with the t function. m &lt;- t(m) # transpose the matrix and store it back in the variable `m` Finally, a matrix can be created by “gluing” rows and columns with the help of rbind andcbind. This is also a convenient way to add new rows and columns to an existing matrix. Exercise 3.16 - functions rbind and cbind a &lt;- 1:4 b &lt;- 5:8 c &lt;- c(0.0) # create a matrix `m` in which vectors `a` and `b` will be the columns # add a new row to the top of the matrix `m` with vector elements taken from`c` # print matrix `m` a &lt;- 1:4 b &lt;- 5:8 c &lt;- c(0.0) # create a matrix `m` in which vectors `a` and `b` will be columns m &lt;- cbind(a, b) # add a new row to the top of the matrix `m` with vector elements taken from`c` # print matrix `m` m &lt;- rbind(c, m) m ## a b ## c 0 0 ## 1 5 ## 2 6 ## 3 7 ## 4 8 3.4 Matrix slicing All the learned principles for “slicing” the vector using index vectors can be applied on matrices. The differences are as follows: we index each dimension individually first we index the rows, then the columns, dividing the index vectors with a , If we want “all rows” or “all columns” we simply omit the corresponding index vector(but keep the ,) 3.5 Example 3.2 - matrix slicing # `m` is a 3 x 5 matrix , with column names from `a` to `e` m &lt;- matrix(1:15, nrow = 3, byrow = T) colnames(m) &lt;- letters[1:5] # note the function and the special R variable called &quot;letters&quot; m[1, 2:5] # first line, all columns from second to fifth m[c(F, T, T), c(&quot;a&quot;, &quot;b&quot;)] # second and third rows, columns `a` and` b` m[,] # all rows and all columns (same as just `m`) ## b c d e ## 2 3 4 5 ## a b ## [1,] 6 7 ## [2,] 11 12 ## a b c d e ## [1,] 1 2 3 4 5 ## [2,] 6 7 8 9 10 ## [3,] 11 12 13 14 15 In practice, the matrix usually uses a combination of positional and label-based indexing; conditional indexing is not too practical because of the two-dimensional nature of the matrix (although it is feasible, we just have to keep in mind that the lengths of logical vectors we use for indexing need to have the proper length of their corresponding dimension). One of the things we need to keep in mind is the R-language tendency to “help” us by simplifying the result. Thus, the result of a slicing operation that leaves only one row or column will automatically become a vector, i.e. it will lose the dim attribute. This is sometimes does not something we want, especially if we are working on scripts that expect a matrix further down the line, even if it has the dimension of rows or columns 1. In this case, we need to put an additional parameter drop = F after the index vectors. This is often somewhat unwieldy, which is why there are many R-language packages that “repair” this by offering the splicing functionality while keeping the result in a consistent format. Still, it’s useful to keep in mind the existence of the drop = FALSE option when there is danger that our programming script will fail if it gets a vector as a result instead of a one-dimensional matrix. Exercise 3.17 - matrix slicing m &lt;- matrix (1:30, 6, 5, T) colnames (m) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) # create a submatrix `m2` which contains everything from the second to the fourth row # and from the third to the fifth column # print `m2` # set all elements in column `c` of `m` to zero # and then print the first two rows of matrix `m` # print only column `d` from `m2` # print only column `d`, but add the `drop = FALSE` parameter when indexing # separate the parameter with a comma (as if it was a &quot;third&quot; indexing dimension) m &lt;- matrix (1:30, 6, 5, T) colnames(m) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;) # create a submatrix `m2` which contains everything from the second to the fourth row # and from the third to the fifth column # print `m2` m2 &lt;- m[2:4, 3:5] m2 # set all elements in column &quot;c&quot; to zero # and then print the first two lines of matrix `m` m[, &quot;c&quot;] &lt;- 0 m[1:2, ] # print only column `d` from `m` m[, &quot;d&quot;] # print only column `d`, but add the `drop = FALSE` parameter when indexing # separate the parameter with a comma (as if it was a &quot;third&quot; indexing dimension) m[, &quot;d&quot;, drop = F] ## c d e ## [1,] 8 9 10 ## [2,] 13 14 15 ## [3,] 18 19 20 ## a b c d e ## [1,] 1 2 0 4 5 ## [2,] 6 7 0 9 10 ## [1] 4 9 14 19 24 29 ## d ## [1,] 4 ## [2,] 9 ## [3,] 14 ## [4,] 19 ## [5,] 24 ## [6,] 29 We will end our overview of the matrices here. These data structures are very useful in solving various linear algebra problems. Additionally, some of the principles used for handling matrices will be useful when we soon introduce the “data frames” - one of the most useful data structures in R. Finally, although we will not extensively cover arrays, we will show an example of the program code that creates a three-dimensional array and then for element retrieval uses standard slicing principles we already learned with the vectors and matrices. myArray &lt;- array(1:24, dim = c(2, 3, 4)) # array of dimension 2 x 3 x 4 myArray[, 1:2, 3, drop = FALSE] # print all rows, first and second columns # 3rd &quot;layer,&quot; with array type retention 3.6 Lists In R lists are primarily used as a “universal data containers”. Unlike vectors (or better said the concept of a vector as we initially defined it), the list may contain different types of data or, more often, sets of different types of data. We can create a list with the list function to which we forward a string of names of elements and their contents as parameters. These elements can be anything, even other lists. myList &lt;- list(a = 1, b = 2:100, c = list(x = 1, y = 2)) Try to create your own list in the following example. Exercise 3.18 - list creation # create a new list called `stuff` that will have the following elements # element called `numbers&#39; with integers from 1 to 3 # element called `letters&#39; with letters &#39;A&#39; and &#39;B&#39; # nameless element with logical vector `c(T, F)` # element called `titles&#39; with the content: &quot;Mr&quot;, &quot;Mrs&quot; and &quot;Ms&quot; # print the `stuff` variable stuff &lt;- list(numbers = c(1,2,3), letters = c(&quot;A&quot;, &quot;B&quot;), c(T, F), titles = c(&quot;Mr&quot;, &quot;Mrs&quot;, &quot;Ms&quot;)) # print the `stuff` variable stuff ## $numbers ## [1] 1 2 3 ## ## $letters ## [1] &quot;A&quot; &quot;B&quot; ## ## [[3]] ## [1] TRUE FALSE ## ## $titles ## [1] &quot;Mr&quot; &quot;Mrs&quot; &quot;Ms&quot; Note that the list is an ordered data structure - the element without the name is shown in position 3. The str (“structure”) function allows us to inspect the properties and contents of a list contents without printing it in its entirety, which is very useful when we deal with large lists. This function is often used by data analysts, not only for lists but also for data frames, which will soon be introduced. Exercise 3.19 - list structure # print the structure of the `stuff` list # print the structure of the `stuff` list str(stuff) ## List of 4 ## $ numbers: num [1:3] 1 2 3 ## $ letters: chr [1:2] &quot;A&quot; &quot;B&quot; ## $ : logi [1:2] TRUE FALSE ## $ titles : chr [1:3] &quot;Mr&quot; &quot;Mrs&quot; &quot;Ms&quot; At the beginning of this lesson we said that one of the primary R principles is “everything is a vector”, where by vectors we mean arranged sets of elements of the same type. Initially it seems that list doesn’t conform to this principle, since it’s defining feature is the fact they may contain elements of different types. However, the surprising truth is in fact the opposite - the lists are actually vectors, and they actually do conform to the aforementioned definition. The answer to this enigma is actually rather simple - the list is in fact a recursive structure, and all the elements of the list are actually also (often small, single-element) lists, which means that formally all elements are truly of the same type. We can easily demonstrate this in the following exercise. Exercise 3.20 - list elements are lists # print the first element of the list `stuff` # check its class # print the first element of the list `stuff` stuff[1] # check its class class(stuff[1]) ## $numbers ## [1] 1 2 3 ## ## [1] &quot;list&quot; So, even though we assumed that the first element of the above list was a vector, we realized it’s actually a small list. This is often quite handy when we want to treat the list as a vector by using R functionality for vector management, however there are many cases where we do not want to work with a list element as a “small list”, but want to have it in its “original” simple vector form. To achieve this, we need to get rid of the list wrapper around the element. This can be done by using the operator [[, i.e. the “double angular brackets” operator. Exercise 3.21 - operator [[ # print the first element of the list `stuff` using the operator `[[` # check its class # print the first element of the list `stuff` using the operator `[[` stuff[[1]] # check its class class(stuff[[1]]) ## [1] 1 2 3 ## [1] &quot;numeric&quot; The aforementioned operator is most often used to retrieve the selected element by using its position in the list or its name (if defined). The second approach though is somewhat inconvenient, since we must use the syntax list[[name_element]] symbol, which is somewhat clumsy for typing since the element name also needs to be put in quotes. To make it easier to pull out elements by name, R offers an alternative way of accessing the list elements in this way bz leveraging the $ operator, i.e. using the syntax list$name_element (no quotes). Exercise 3.22 - operator $ # print the `letters` element of the `stuff` list # using `[[` the operator # print the `letters` element of the `stuff` list # using the `$` operator # print the `letters` element of the `stuff` list # using `[[` the operator stuff[[&quot;letters&quot;]] # print the `letters` element of the `stuff` list # using the `$` operator stuff$letters ## [1] &quot;A&quot; &quot;B&quot; ## [1] &quot;A&quot; &quot;B&quot; The lists are an extremely popular type of object in R, as they represent a universal template for more complex data structures, including more complex objects in the narrower sense (as we will see later). The list is also the “foundation” for the most popular and most commonly used element of the R-language: the data frame - which we will learn in the next chapter. Finally, we need to learn how to add an element to the list. The easiest way to do this is simply by using the aforementioned operator $ - we simply write something like list$newElement &lt;- newElementContents. In a similar fashion, we can delete a list element simply by assigning the NULL value to it. Exercise 3.23 - adding list elements # add the `evenNumbers` element to the `stuff` list which contains # all even numbers from 1 to 100 # delete the third element from this list # print the `stuff` list # in the `stuff` list add the `evenNumbers` element which contains # all even numbers from 1 to 100 stuff$evenNumbers &lt;- seq(2, 100, 2) # delete the third element from the list stuff[[3]] &lt;- NULL # print the `stuff` list print(stuff) ## $numbers ## [1] 1 2 3 ## ## $letters ## [1] &quot;A&quot; &quot;B&quot; ## ## $titles ## [1] &quot;Mr&quot; &quot;Mrs&quot; &quot;Ms&quot; ## ## $evenNumbers ## [1] 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30 32 34 36 38 ## [20] 40 42 44 46 48 50 52 54 56 58 60 62 64 66 68 70 72 74 76 ## [39] 78 80 82 84 86 88 90 92 94 96 98 100 Homework exercises Create the following vectors: (11, 12, 13,…, 99) (0, 0, 0, 0, … , 0) (100 zeros) (0, 0.1, 0.2, …., 1.0) What is the sum of all numbers from 101 to 1001, if we skip all numbers divisible by 10? Use the sum function. Create a 3 x 3 matrix by performing the following commands (the sample function randomly picks elements from a provided vector; it will be covered in more detail in one of the following lessons): # we create a matrix of 3x3 randomly selected elements from 1 to 100 set.seed(1234) m &lt;- matrix(c(sample(1:100, 9, T)), nrow = 3, ncol = 3, byrow = T) Calculate the inverse matrix with the solve function. Make sure the multiplication of the original and inverse matrix result with the unit matrix (use the % *% operator to multiply the matrices). Initialize the stuff list used in the lesson. Do the following: print the class of the second element of the list print the element in the third place of the element of the list named letters check the length of the element called titles and add the title ‘Prof’ to the last position check if the number 4 is contained in the first element of the list add a new list of three vectors a,b and c which all contain elements (1,2,3) to the last place of the list, Program in Ru &lt;/ span&gt; by Damir Pintar is licensed under Creative Commons Attribution-NonCommercial-NoDerivative 4.0 International License Based on a work at https://ratnip.github.io/FER_OPJR/ "],["okviri.html", "4 Data frames and factors 4.1 Data frames 4.2 Selecting rows and columns 4.3 Adding and deleting rows and columns 4.4 Factors Homework exercises", " 4 Data frames and factors 4.1 Data frames The data frame is by far the most popular element of the programming language R. After all, the language R’s primary purpose is data analysis, and the data frame is provides an programmatic representation of the data set we intend to analyze. In other words, the data frame is an object similar in function to a sheet in Microsoft Excel or a table in a relational database. Almost every session in R revolves around manipulating data frames in certain ways - but while in Excel we mostly interact with the table with the help of a graphical interface, and in relational database via the query language SQL, in R we manage data programmatically using data frames. Let’s take for example the following table: zipCode cityName avgSalKn population cityTax 10000 Zagreb 6359.00 790017 18 51000 Rijeka 5418.00 128384 15 21000 Split 5170.00 167121 10 31000 Osijek 4892.00 84104 13 20000 Dubrovnik 5348.00 28434 10 This data set that contains certain parameters related to cities in the Republic of Croatia (values inside are not necessarily accurate and are used for demonstration purposes; also, note that the value of cityTax is expressed as percentage amount). We can easily imagine how to write this data in Excel or create a relational table to store it. Now we will learn how to represent this data in programming language R. In the last lesson we learned that the list is a complex data type which serves as a kind of “universal container” with the help of which we can collect a number of different objects and put them within the same structure. The data frame makes just a tiny tweak of this notion - it’s also a “universal container”, but with a restriction that each element stored within needs to have the same number of its own elements, so the container itself can assume tabular structure when its contents are represented as columns. In other words, data frame is just a list with an additional restriction regarding the length of its elements. The latter has a few very interesting consequences. First, since data frame inherits all features of a list, it allows us to leverage all functions and operators that work with lists. Additionally, the tabular nature of data frame’s contents allows us to treat it as a two-dimensional structure, effectively inheriting functions and operators that work on matrices. In other words, data frames at the same time behave as matrices as lists, allowing us to seamless apply knowledge about those data structures when manipulating this newly introduced type. But first, let’s learn how to create these objects. There are several ways to create data frames, and we’ll show two of the most frequently encountered scenarios in practice: Manual creation via the data.frame function Loading data from an external source using a helper function Let’s see both of these cases. First, we will create a data frame programmatically. Exercise 4.1 - creating data frames programatically # notice the similarity with list creation! cities &lt;- data.frame(zipCode = c(10000, 51000, 21000, 31000, 20000), cityName = c(&quot;Zagreb&quot;, &quot;Rijeka&quot;, &quot;Split&quot;, &quot;Osijek&quot;, &quot;Dubrovnik&quot;), avgSalKn = c(6359., 5418., 5170., 4892., 5348.), population = c(790017, 128384, 167121, 84104, 28434), cityTax = c(18, 15, 10, 13, 10)) # print the variable `cities` # notice the similarity with list creation! cities &lt;- data.frame(zipCode = c(10000, 51000, 21000, 31000, 2000), cityName = c(&quot;Zagreb&quot;, &quot;Rijeka&quot;, &quot;Split&quot;, &quot;Osijek&quot;, &quot;Dubrovnik&quot;), avgSalKn = c(6359., 5418., 5170., 4892., 5348.), population = c(790017, 128384, 167121, 84104, 28434), cityTax = c(18, 15, 10, 13, 10)) # print the variable `cities` cities ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 2000 Dubrovnik 5348 28434 10 If you like, try creating another data frame with differing lengths of subelements (“columns”). As you will see, this operation will result in an error and the data frame will not be created - R tries to keep the matrix nature of the frame always preserved. A little note regarding terminology: the “data frame” is often referred to as simply a “frame” or a “table”. Likewise, for its elements we will interchangeably use terms such as “column”, “variable” or “attribute”. Finally, adhering to data frames tabular structure, we will refer to the horizontally aligned elements as “rows” or “observations”. These terms are in line with the standard way of referencing tabular elements and the common statistical terms referring to the datasets. If there is a chance of ambiguity depending on the context, the term that most unambiguously describes the referenced element will be used. Let’s try to load the table from an external source now. Although R allows different forms of external data, we will assume that the data is obtained in a standard “CSV format” (CSV - comma-separated values). This format is one of the most popular pure text data storage methods that has the advantage of being easy to create manually, as well as being recognized by most data management tools which can often readily import/export data in such format. Below we can see an example of a CSV file that matches the data frame created in the previous example. Suppose the file is named cities.csv. Row values are separated by a comma (no spaces!). Every row (observation) is in its own your line, and the (optional!) first line represents the column names. zipCode,cityName,avgSalKn,population,cityTax 10000,Zagreb,6359.00,790017,18 51000,Rijeka,5418.00,128384,15 21000,Split,5170.00,167121,10 31000,Osijek,4892.00,84104,13 20000,Dubrovnik,5348.00,28434,10 One of the potential problems with CSV files is that they use the comma as separator (delimiter) of column elements, and in certain languages, as a standard, a “decimal comma” is used instead of a decimal point. Because of this fact, there is an “alternative” CSV standard that uses a semi-colon as a separator which would make our CSV file in this case looks like this (let’s call it citiesAlt.csv): zipCode;cityName;avgSalKn;population;cityTax 10000;Zagreb;6359,00;790017;18 51000;Rijeka;5418,00;128384;15 21000;Split;5170,00;167121;10 31000;Osijek;4892,00;84104;13 20000;Dubrovnik;5348,00;28434;10 Since the decimal point is a standard in the Republic of Croatia, in working with CSV files we have to be careful which of the two standards is being used, which cannot be discerned just from the extension alone. Luckily, the R language offers support functions for both standards, so once we identified which standard is being used in a particular file, we simply choose the appropriate function. We will assume existence of the following two files in the same directory as the workbook: cities.csv citiesAlt.csv (If you do not have these files available, you can easily create them with the help of plaintext editors (eg Notepad or gedit) by copy pasting the above rows.) To create data frames from CSV files we use the following functions: - read.csv - for “normal” CSV files with a comma as a separator - read.csv2 - for the alternative CSV standard that uses semi-colons The main input parameter of these functions is the path to the CSV file that we are loading. These functions also have a rich set of additional parameters that allow easy adoption to different scenarios. Also, both of these functions are derived from a more general function called read.table which in itself is even more flexible with respect to the number of different parameters and data load settings. It is recommended to check the documentation to see the entire spectrum of options which can be used for loading tabular data stored in plaintext files, and some of the more important ones will be listed below. As stated, a subset of the parameters (with the example of associated values) of read.csv (or read.table) functions that are useful to know are: header = FALSE - for files without a header sep = \"#\" - for files that use some more “exotic” separator, in this case # na.strings = 'NULL' - the term used in the dataset to represent the missing values that will ultimately become NA in R nrows = 2000 - maximum number of lines to be read, in this case 2000 stringsAsFactors = F - preventing automatic creation of factor columns (which we will learn in the lesson below) encoding = 'UTF-8' - for non-ASCII text encoding standards (especially if we are working with Croatian data which has diacritical characters) Let’s try to load data from available CSV files now. This data will not require special parameters, and will only be loaded by providing the path to the associated files (one that uses the comma and the other that uses the semi-colon as a separator). Exercise 4.2 - reading CSV files # load data from files `cities.csv` and `citiesAlt.csv` # and store it in variables called `cities2` and `cities3` # print `cities2` and` cities3` # load data from files `cities.csv` and `citiesAlt.csv` # and store it in variables called `cities2` and `cities3` cities2 &lt;- read.csv(&quot;cities.csv&quot;) cities3 &lt;- read.csv2(&quot;citiesAlt.csv&quot;) # print `cities2` and` cities3` cities2 cat(&quot;-----------\\n&quot;) cities3 ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 20000 Dubrovnik 5348 28434 10 ## ----------- ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 20000 Dubrovnik 5348 28434 10 We have already mentioned that data frames behave both like lists and like matrices. Let’s look at some useful functions we used with these types of objects which may now become very useful when working with data frames: nrow - number of rows ncol or length - the number of columns (“matrix” and “list” way!) dim - table dimensions names - column names head - prints several rows from the beginning of the table tail - prints several rows from the end of the table str - prints table structure summary - summarizes the statistical information about table columns Let’s try some of these functions: Exercise 4.3 - working with data frames # print the dimensions of the data frame `cities` # print the table structure of `cities` # print the first few rows of the data frame `cities` # print summarized statistical information about `cities` # print the dimensions of the data frame `cities` dim(cities) cat(&quot;-----------\\n&quot;) # print the table structure of `cities` str(cities) cat(&quot;-----------\\n&quot;) # print the first few rows of the data frame `cities` head(cities) cat(&quot;-----------\\n&quot;) # print summarized statistical information about `cities` summary(cities) ## [1] 5 5 ## ----------- ## &#39;data.frame&#39;: 5 obs. of 5 variables: ## $ zipCode : num 10000 51000 21000 31000 2000 ## $ cityName : chr &quot;Zagreb&quot; &quot;Rijeka&quot; &quot;Split&quot; &quot;Osijek&quot; ... ## $ avgSalKn : num 6359 5418 5170 4892 5348 ## $ population: num 790017 128384 167121 84104 28434 ## $ cityTax : num 18 15 10 13 10 ## ----------- ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 2000 Dubrovnik 5348 28434 10 ## ----------- ## zipCode cityName avgSalKn population ## Min. : 2000 Length:5 Min. :4892 Min. : 28434 ## 1st Qu.:10000 Class :character 1st Qu.:5170 1st Qu.: 84104 ## Median :21000 Mode :character Median :5348 Median :128384 ## Mean :23000 Mean :5437 Mean :239612 ## 3rd Qu.:31000 3rd Qu.:5418 3rd Qu.:167121 ## Max. :51000 Max. :6359 Max. :790017 ## cityTax ## Min. :10.0 ## 1st Qu.:10.0 ## Median :13.0 ## Mean :13.2 ## 3rd Qu.:15.0 ## Max. :18.0 4.2 Selecting rows and columns We’ve already said multiple times that data frames behave both as matrices and as lists, and one place where we employ both of these facets simultaneously is when selecting rows and columns of data frames. Specifically, we often leverage the following: two-dimensional indexing - to identify rows and columns we want to keep operator $ - for referencing individual columns In practice, one of the most common indexing methods is combining conditional selection of rows (with filtering expressions which leverage the $ operator), and the label-based indexing of columns (SQL experts will recognize this as a standard combination of WHERE and SELECT). Let’s try to apply this in practice. Exercise 4.4 - selecting rows and columns # print the table `cities` (for reference) # print the first three rows, the third and fifth column # print just the column &quot;cityTax&quot; # print zipCodes and city names of all cities which # have cityTax greater than 12% and a population of more than 100,000 # print the table `cities` (for reference) cities cat(&quot;-----------\\n&quot;) # print the first three rows, the third and fifth column cities[1:3, c(2,5)] cat(&quot;-----------\\n&quot;) # print just the column &quot;cityTax&quot; cities$cityTax cat(&quot;-----------\\n&quot;) # print zipCodes and city names of all cities which # have cityTax greater than 12% and a population of more than 100,000 cities[cities$cityTax &gt; 12 &amp; cities$population &gt; 100000, c(&quot;zipCode&quot;, &quot;cityName&quot;)] ## zipCode cityName avgSalKn population cityTax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 2000 Dubrovnik 5348 28434 10 ## ----------- ## cityName cityTax ## 1 Zagreb 18 ## 2 Rijeka 15 ## 3 Split 10 ## ----------- ## [1] 18 15 10 13 10 ## ----------- ## zipCode cityName ## 1 10000 Zagreb ## 2 51000 Rijeka Notice the similarity between the last expression and the SQL query: SELECT zipCode, cityName FROM cities WHERE cities.cityTax &gt; 12 AND cities.population &gt; 100000 Choosing columns and rows is not difficult if we are well acquainted with index vector knowledge, but as it can be seen in the last example, the final expression is not necessarily easy to visually interpret (as compared to, for example, the SQL syntax that performs the same job). There are R packages which vastly reduce the complexity of writing and interpreting expressions such as these, which will be the subject of one of the future chapters. 4.3 Adding and deleting rows and columns To add and delete rows and columns, we can again try to remember how we would perform a similar task when handling matrices and lists, and then apply this knowledge to data frames. When performing data analysis though, adding individual columns is a slightly more common practice than adding rows - it is uncommon to be manually adding new observations, but creating new columns out of existing ones is a relatively standard data preparation task. The easiest way to add columns to the data frame is to mimic the way we add list elements - we just have to take care that the added column has the same number of elements as the other columns. New columns are often derived from existing columns via logical or arithmetic expressions. Exercise 4.5 - adding new columns to a data frame # add the logical column` highCityTax` to the `cities` table # which will indicate whether the cityTax is greater than 12% # assume the following (fabricated) way of estimating the income city gets from tax # - assume all cities have about 60% of working population # - assume each worker pays a tax that is roughly equal to 10% of their net salary # - assume income from city tax per worker is (cityTax percentage)*(tax income) # # add a `monthlyTaxIncome` column which will use the average salary, city tax # rate and population to estimate the monthly tax income of the cities # (in million Kns) # round the result to two decimals using the `round` function, # (example: round(100.12345, 2) ==&gt; 100.12 ) # print `cities` # add the logical column` highCityTax` to the `cities` table # which will indicate whether the cityTax is greater than 12% cities$highCityTax &lt;- cities$cityTax &gt; 12 # assume the following (fabricated) way of estimating the income city gets from tax # - assume all cities have about 60% of working population # - assume each worker pays a tax that is roughly equal to 10% of their net salary # - assume income from city tax per worker is (cityTax percentage)*(tax income) # # add a `monthlyTaxIncome` column which will use the average salary, city tax # rate and population to estimate the monthly tax income of the cities # (in million Kns) # round the result to two decimals using the `round` function, # (example: round(100.12345, 2) ==&gt; 100.12 ) cities$monthlyTaxIncome&lt;- round(0.6 * cities$population * 0.01 * cities$avgSalKn * 0.01 * cities$cityTax / 1e6 , 2) # print `cities` cities ## zipCode cityName avgSalKn population cityTax highCityTax monthlyTaxIncome ## 1 10000 Zagreb 6359 790017 18 TRUE 5.43 ## 2 51000 Rijeka 5418 128384 15 TRUE 0.63 ## 3 21000 Split 5170 167121 10 FALSE 0.52 ## 4 31000 Osijek 4892 84104 13 TRUE 0.32 ## 5 2000 Dubrovnik 5348 28434 10 FALSE 0.09 You can also add rows and columns similar to adding rows and columns to a matrix - with the rbind andcbind functions. We can use rbind to add new observations to the data frame (respecting the order of columns in the original data frame). With cbind we can add a completely new vector as a column, keeping in mind that the number of elements corresponds to the number of rows of the original data frame. Let’s try these functions on small “artificial” data frames to better demonstrate their functionality. Exercise 4.6 - data frames and rbind/cbind functions df1 &lt;- data.frame(a = c(1,2,3), b = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c = c(T, F, T)) df2 &lt;- data.frame(a = 1, b = &quot;A&quot;, c = 3) # make a data frame which has `df1` and `df2` as rows # name it `df12` # add a `firstNames` columns containing names Ivo, Ana, Pero and Stipe # use `cbind` # print `df12` df1 &lt;- data.frame(a = c(1,2,3), b = c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;), c = c(T, F, T)) df2 &lt;- data.frame(a = 1, b = &quot;A&quot;, c = 3) # make a data frame which has `df1` and `df2` as rows # name it `df12` df12 &lt;- rbind(df1, df2) # add a `firstNames` columns containing names Ivo, Ana, Pero and Stipe # use `cbind` df12 &lt;- cbind(df12, firstNames = c(&quot;Ivo&quot;, &quot;Ana&quot;, &quot;Pero&quot;, &quot;Stipe&quot;)) # print `df12` df12 ## a b c firstNames ## 1 1 A 1 Ivo ## 2 2 B 0 Ana ## 3 3 C 1 Pero ## 4 1 A 3 Stipe For deleting rows and columns we can also use the same methods for managing matrices and lists. Specifically: to delete rows and columns we simply use two-dimensional indexing and store the result in the original datga frame to delete columns can we assign the value NULL to the selected column Let’s try this in the example. Exercise 4.7 - deleting rows and columns # delete the first row and second column from `df12` # use the indexing method # delete the `firstNames` column by using the `NULL` method # print `df12` # delete the first row and second column from `df12` # use the indexing method df12 &lt;- df12[-1, -4] # delete the `firstNames` column by using the `NULL` method df12$firstNames &lt;- NULL # print `df12` df12 ## a b c ## 2 2 B 0 ## 3 3 C 1 ## 4 1 A 3 We will get back to data frame management in one of the future chapters, where we will introduce new packages that completely reinvent most of base R functionality for this purpose. For now, we will end the introduction of R data types with one of the extremely popular derived data types - a factor. 4.4 Factors Factors in R are a data type that represents what is referred to in statistics as a nominal or categorical variable. Namely, the attribute of some observation often takes on a value from a set of previously known categories (e.g. gender, age category, education, city of birth, political party preference, etc.). Categories are often identified by a unique set of characters, and are often leveraged for various aggregations and groupings (for example, in a sports race we can calculate the average time depending on gender or age category) or filtering (we analyze only data pertaining to certain categories). Factors in R are very useful since they inherently carry metadata about their nature, allowing various function to readily recognize factors as such and deal with them accordingly (for example, visualization functions will use a discrete set of colors and a legend to denote categorical values). However, factors can also be a source of contention for R beginners because if used incorrectly they can lead to some inconvenient side-effects. This can all be easily avoided by learning specifics how they work and sticking to certain guidelines we will outline during this chapter. Let’s first demonstrate what factors actually are with the help of a simple example. Let’s imagine that we are doing a study which has ten medical patients (referred to by their ordinal number in study), and that the next character vector describes the blood pressure level in these patients: bloodPressure &lt;- c (&quot;low&quot;, &quot;high&quot;, &quot;high&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;low&quot;, &quot;high&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;normal&quot;) This is obviously a “categorical” variable since it can take one of three discrete values - low, normal and high. Thus, this vector is a typical candidate for “factorizing”, i.e. for converting from a character vector into a factor class object. We can easily create these objects by using the factor function to which we can simply the character vector of category names. Exercise 4.8 - factorizing a character vector bloodPressure &lt;- c (&quot;low&quot;, &quot;high&quot;, &quot;high&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;low&quot;, &quot;high&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;normal&quot;) # print the variable `bloodPressure` # print its class # create a variable `bloodPressure.f` by factorizing # the variable `bloodPressure` # print the variable `bloodPressure.f` # print its class bloodPressure &lt;- c (&quot;low&quot;, &quot;high&quot;, &quot;high&quot;, &quot;normal&quot;, &quot;normal&quot;, &quot;low&quot;, &quot;high&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;normal&quot;) # print the variable `bloodPressure` bloodPressure # print its class class(bloodPressure) # create a variable `bloodPressure.f` by factorizing # the variable `bloodPressure` bloodPressure.f &lt;- factor(bloodPressure) cat(&quot;-----------\\n&quot;) # print the variable `bloodPressure.f` bloodPressure.f # print its class class(bloodPressure.f) ## [1] &quot;low&quot; &quot;high&quot; &quot;high&quot; &quot;normal&quot; &quot;normal&quot; &quot;low&quot; &quot;high&quot; &quot;low&quot; ## [9] &quot;normal&quot; &quot;normal&quot; ## [1] &quot;character&quot; ## ----------- ## [1] low high high normal normal low high low normal normal ## Levels: high low normal ## [1] &quot;factor&quot; Notice the subtle difference between printing a character vector and a factor. The printout of the factor is given an additional attribute called Levels. Also, the category names are now lacking quotation marks, further reinforcing the notion that these are not really character values as such. What have we gained by turning a character vector into a factor? For starters, it will make it harder to add an observation with an invalid category to it. For example, if we try to add a new value to a factor that is not present in the current categories (e.g., “very high”) we will get a warning, and instead of the category we have specified, a new item will have the value of NA. This is usually a desirable behaviour, since we don’t like having “dirty” data with incorrect category values. However, sometimes we may be caught unaware if a perfectly valid category gets stored as NA. This may happen if in the process of categorizing a character vector this original vector did not contain all the possible categories that may generally appear. To alleviate this issue, we have the option of adding the levels parameter in which we will explicitly specify a series of all possible categories with a help of a separate character vector. Exercise 4.9 - using the levels attribute # add an 11th element to `bloodPressure.f` called &quot;very low&quot; # print the variable `bloodPressure.f` # create a variable `bloodPressure.f2` by factorizing # the variable `bloodPressure` # add the `levels` attribute which will also contain # values &quot;very low&quot; and &quot;very high&quot; # add an 11th element to `bloodPressure.f2` called &quot;very low&quot; # print the variable `bloodPressure.f2` # add an 11th element to `bloodPressure.f` called &quot;very low&quot; bloodPressure.f[11] &lt;- &quot;very low&quot; ## Warning in `[&lt;-.factor`(`*tmp*`, 11, value = &quot;very low&quot;): invalid factor level, ## NA generated # print the variable `bloodPressure.f` bloodPressure.f cat(&quot;-----------\\n&quot;) # create a variable `bloodPressure.f2` by factorizing # the variable `bloodPressure` # add the `levels` attribute which will also contain # values &quot;very low&quot; and &quot;very high&quot; bloodPressure.f2 &lt;- factor(bloodPressure, levels = c(&quot;very low&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;high&quot;, &quot;very high&quot;)) # add an 11th element to `bloodPressure.f2` called &quot;very low&quot; bloodPressure.f2[11] &lt;- &quot;very low&quot; # print the variable `bloodPressure.f2` bloodPressure.f2 ## [1] low high high normal normal low high low normal normal ## [11] &lt;NA&gt; ## Levels: high low normal ## ----------- ## [1] low high high normal normal low high low ## [9] normal normal very low ## Levels: very low low normal high very high Since factors are generally very useful, it is recommended that we always take care to properly “factorize” categorical columns in our data frames. Let’s show one simple function which is often used in conjunction with factors. Analysts are often very interested in distribution of categories in a data set. To get this information, we can use the function called table who will get a factor in question as an input parameter. *** Exercise 4.10 - the table function # print the distribution of categories in `bloodPressure.f2` # print the distribution of categories in `bloodPressure.f2` table(bloodPressure.f2) ## bloodPressure.f2 ## very low low normal high very high ## 1 3 4 3 0 The table function does not necessarily require a factor and will work even with the character vector. However in that case we would not get information about categories that were not represented at all (which is often very important). The categorical variable in our examples actually has the so-called ordinal nature. In ordinal category variables the categories have a natural order (“low” blood pressure is lower than “normal” which in turn is lower than “high”). If desired, this fact can be “embedded” in the initialization process by simply adding the order parameter set to TRUE. The advantage of the ordinal factor is that it allows us to compare factor values with the help of comparative operators. Exercise 4.11 - ordinal factor # create a variable `bloodPressure.f3` by factorizing # the variable `bloodPressure` # add the `levels` attribute which will also contain # values &quot;very low&quot; and &quot;very high&quot; # also set the `order` paramater to `TRUE` # watch out for category ordering! # print the variable `bloodPressure.f3` # check if it is in fact the ordinal factor now # using the `is.ordered` function # check if the first patient has lower blood pressure # than the third # create a variable `bloodPressure.f3` by factorizing # the variable `bloodPressure` # add the `levels` attribute which will also contain # values &quot;very low&quot; and &quot;very high&quot; # also set the `order` paramater to `TRUE` # watch out for category ordering! bloodPressure.f3 &lt;- factor(bloodPressure, levels = c(&quot;very low&quot;, &quot;low&quot;, &quot;normal&quot;, &quot;high&quot;, &quot;very high&quot;), order = TRUE) # print the variable `bloodPressure.f3` bloodPressure.f3 # check if it is in fact the ordinal factor now # using the `is.ordered` function is.ordered(bloodPressure.f3) # check if the first patient has lower blood pressure # than the third bloodPressure.f3[1] &lt; bloodPressure.f3[3] ## [1] low high high normal normal low high low normal normal ## Levels: very low &lt; low &lt; normal &lt; high &lt; very high ## [1] TRUE ## [1] TRUE We have already stated that R like using the principle which goes “everything is a vector” - numbers are one-dimensional numeric vectors, matrices are vectors with added dimension parameter, lists are vectors of smaller lists, data frames are lists with added restrictions. In that vein, we can assume that factors are vectors, too. But what do they look like “under the hood”? The implementation of a factor is actually relatively straightforward - it’s simply a numeric vector where each number corresponds to a certain category, meaning that each factor vector also contains an associated “code table” in the form of an accompanying character vector. In other words, the process of factorization entails&gt; placing all categories in a separate character vector (categories are either inferred from the original vector or explicitly set with the levels parameter) assigning numeric values in the order of each category (eg: “low” -&gt; 1, “normal” -&gt; 2 etc.) “packing” together a newly created numeric vector and the associated “code table” Although these steps R works out automatically, the internal structure of a factor can easily be discovered through converting it into a pure numeric or pure character type. Exercise 4.12 - internal structure of a factor # print `bloodPressure.f3` converted to a character # print `bloodPressure.f3` converted to a numeric # print `bloodPressure.f3` converted to a character as.character(bloodPressure.f3) # print `bloodPressure.f3` converted to a numeric as.numeric(bloodPressure.f3) ## [1] &quot;low&quot; &quot;high&quot; &quot;high&quot; &quot;normal&quot; &quot;normal&quot; &quot;low&quot; &quot;high&quot; &quot;low&quot; ## [9] &quot;normal&quot; &quot;normal&quot; ## [1] 2 4 4 3 3 2 4 2 3 3 By converting the factor to the character type we actually do the operation of inverse factorization, i.e., we may go back to the original character vector. On the other hand, by converting the factor into a numerical type, we will get a list of encoded numbers that the internal factor is used to represent each category. One question can now arise - is there any benefit in converting a factor in its character or numeric counterpart? The general answer is “no”. Factor variables will automatically behave as characters wherever necessary (for example if we want to filter out all the three-lettered names). And the numeric representation is meaningless without the information what each number actually corresponds to. However, knowing what happens when we convert a factor to a character or numeric vector can help us avoid a particularly nasty error which may occur if data analyst mishandles factor variables. Good news is that since R 4.0, the possibility of this error is severely diminished, but it’s still worthwhile to know the specifics what it entails, and how to avoid it. The error in question happens when a numeric column is misidentified as a categorical column, which used to be a relatively common occurence when the analyst allowed R to automatically infer the types of columns being loaded from an external data source. Before version 4.0, R was pretty “factor happy”, meaning that it would first discern columns as either numerical (if all values could be converted into a number) or character (all other cases), which would then be followed by automatic factorization of character columns (unless this behaviour was explicitly turned off by setting the stringsAsFactors = FALSE parameter). Now imagine we have tabular data where one entry has an error (for example, 7.07 was manually inserted as 7.O7). Let’s simulate what might have happened in older versions of R. 4.4.1 Example 4.1 - mishandling factorization # simulated &quot;typing error&quot; df &lt;- data.frame(id = 1:5, measures = c(&quot;1.45&quot;, &quot;5.77&quot;, &quot;1.12&quot;, &quot;7.O7&quot;, &quot;3.23&quot;), stringsAsFactors = TRUE) # mimicking the behaviour pre-R 4.0 # trying to treat the &quot;numeric&quot; column as numeric reveals something weird mean(df$measures) ## Warning in mean.default(df$measures): argument is not numeric or logical: ## returning NA # the analyst realizes something is wrong and tries to convert the column to its proper type df$measures &lt;- as.numeric(df$measures) # measure amounts are now irreversibly lost, an error which if not caught can cause serious # problems in subsequent steps of the analysis df ## [1] NA ## id measures ## 1 1 2 ## 2 2 4 ## 3 3 1 ## 4 4 5 ## 5 5 3 How to avoid this scenario? As stated, it has now become far less likely since from R 4.0 the default value of the stringsAsFactors was switched from TRUE to FALSE. Still, it pays off to take certain precautions, especially if there is a chance that our code will be run on platforms with earlier versions of R. Some of the useful guidelines are: When using the read.csv orread.table functions, always put the string stringsAsFactors = FALSE carefully check column type data of loaded data frame perform appropriate column conversions check the results again If we adhere to these steps we will almost never come to the situation which causes us real problems. There are additional methods, like explicitly telling column types when reading from a file (colClasses parameters), or using the as.numeric(as.character(f)) syntax to be sure that a “numeric” factor will not be converted to a series of meaningless integers, but the above steps should in most cases be completely sufficient. Homework exercises Locate the file citiesNOHEADER.csv which represents the file that is the same as cities.csv except for the following features: column names are missing space is used as a separator Try using the documentation to load the data from this file into the variable called citiesNH which must end up identical to the variable cities used in this chapter. Locate the file receipt.csv and load it into the receipt variable. Ensure that character sequences were not automatically converted into factors. Print to the screen: the number of rows in this table the number of columns in the table column names For the receipt table, do the following: factorize the itemCategory column print the code, name and price of all items of the sweets and snacks category cheaper than 12 Kn print how many products each category has in the receipt add a total column that will contain the total price of each purchased item, leveraging the price and quantity calculate the total amount of the receipt Locate the file citiesNULL.csv which, if loaded without thestringsAsFactors = FALSE parameter, can result in a problematic scenario described in the lesson. Try to do the following: load the data from this file into the variable citiesNULL and deliberately set the parameter stringsAsFactors = TRUE add avgSal1 column that will represent the result of using the as.numeric function over the avgSalKn column add avgSal2 column that will represent the result of using the combination of as.character and as.numeric functions over the avgSalKn column (be careful of the order) print out citiesNULL and comment the results Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],["control.html", "5 Conditional statements and programming loops 5.1 Flow control commands 5.2 Loops 5.3 Example 5.1 - the repeat loop Homework exercises", " 5 Conditional statements and programming loops 5.1 Flow control commands Under flow control commands we mostly refer to programming constructs which allow for conditional execution as well as so-called “looping”, where the segment of the program will be continually executed until (optionally!) certain conditions are fulfilled and the loop is exited. We will first remind ourselves how to do conditional execution in R. 5.1.1 Conditional execution For conditional execution we use theif-then block, a construct used in almost every programming language: if (condition) {block} else {block} This command is pretty straightforward. If condition resolves to TRUE the first block will be executed, second block otherwise. We use { and } brackets to denote the start and end of block respectively, and we can do away with them completely if only one instruction is included in the block. Since we have already demonstrated the use of if-then command in Chapter 2, let’s instead show a common bug that may occur in R scripts that leverage this construct. Execute the following code and then try to correct the mistake. Exercise 5.1 - if command # execute the next conditional execution command if (2 &gt; 1) print (&quot;Success!&quot;) # find the error in the next `if-else` command and correct it if (1 &gt; 2) print (&quot;Success!&quot;) else print (&quot;Fail!&quot;) # execute the next conditional execution command if (2 &gt; 1) print (&quot;Success!&quot;) # find the error in the next `if-else` command and correct it if (1 &gt; 2) {print (&quot;Success!&quot;) } else print (&quot;Failed!&quot;) ## [1] &quot;Success!&quot; ## [1] &quot;Failed!&quot; The error occurs because R is an interpreting language that normally executes instructions row by row, and will only proceed to the next row before executing if the command is left “unfinished”. In the previous example, the second if command is actually completed in the first line, so R is “surprised” when the next line starts with else. In order to prevent this scenario, it is sufficient to indicate that the command has not yet been completed, which is most easily accomplished by opening the block in the first line and closing it in the line with else. R also has a vectorized version of conditional execution in the form of a function called, appropriately enough, ifelse. This one however does not control program flow itself, but rather performs conditional assignment based on the provided logical vector. Exercise 5.2 - ifelse function a &lt;- 1:3 b &lt;- c(0, 2, 4) # what does the vector `x` look like after executing the following command? # try to predict the answer and then uncomment and check if you are correct #x &lt;- ifelse (a &lt; b, 2, 5) a &lt;- 1:3 b &lt;- c(0, 2, 4) # what does the vector `x` look like after executing the following command? # try to predict the answer and then uncomment an dcheck if you are correct x &lt;- ifelse(a &lt; b, 2, 5) x ## [1] 5 5 2 This function is particularly suitable for creating new columns of data frames when we want a logical column derived from certain conditions related to the existing columns. 5.2 Loops Loop is a programming construct where depending on a condition a block of code gets repeated multiple times. In programming language R we have three loop variants, each using a different keyword: repeat - conditionless, infinite loop while - loop that checks condition before entering for - an iterator loop (“loop with known number of repetitions”) 5.2.1 Therepeat loop The repeat is arguably the simplest type loop. It has the following syntax: repeat {block} This is an “infinite” loop where, once the block is completed, it gets re-executed again, and so on. The only way to exit this loop is to explicitly “break” out of it using the break command. In addition to break, we also have a next command that will skip the rest of the block, but will then go back to the beginning of the loop. Let’s see this loop in action in the following example. 5.3 Example 5.1 - the repeat loop # answer the following questions before running the next block: # - will the loop run indefinitely? # - what will be printed on the screen? i &lt;- 1 repeat { i &lt;- i + 1 if (i %% 2 == 0) next print(i) if (i &gt; 10) break } This type of loop is not used very frequently, mostly because it is usually more convenient to put the condition for exiting the loop in a more prominent place, which is exactly what the while loop does. 5.3.1 The while loop The while loop uses a simple syntax which means “while the following condition is met, repeat the specified code”: while (condition) {block} The condition should evaluate to TRUE or FALSE, or something that can be implicitly converted to those values. We can also use break and next commands in the while loop; break will immediately exit the loop, and next will go back to checking the condition. Exercise 5.3 - The while loop # add the looping condition so the loop # prints something on screen exactly 7 times i &lt;- 1 while () { print(i) i &lt;- i + 1 } # add the looping condition so the loop # prints something on screen exactly 7 times i &lt;- 1 while (i &lt;= 7) { print(i) i &lt;- i + 1 } ## [1] 1 ## [1] 2 ## [1] 3 ## [1] 4 ## [1] 5 ## [1] 6 ## [1] 7 The while loop can also become an infinite loop if we enter the condition which never becomes FALSE, for example if we literally set while(TRUE). Let’s try to use this loop in a slightly harder exercise. Exercise 5.4 - The while loop (2) n &lt;- 1234 nbin &lt;- numeric(0) # fill vector `nbin` with digits of `n` when turned into a binary form # example: if `n` was 12, `nbin` should be c(1,1,0,0) # use a `while` loop # hint: a function called `rev` can be used to reverse the order of elements in a vector # print resulting `nbin` on screen n &lt;- 682 nbin &lt;- numeric(0) # fill vector `nbin` with digits of `n` when turned into a binary form # example: if `n` was 12, `nbin` should be c(1,1,0,0) # use a `while` loop # hint: a function called `rev` can be used to reverse the order of elements in a vector # print resulting `nbin` on screen while (n != 0) { nbin &lt;- c(nbin, n %% 2) n &lt;- n %/% 2 } nbin &lt;- rev(nbin) nbin ## [1] 1 0 1 0 1 0 1 0 1 0 5.3.2 The for loop In programming languages for loop or “iterator loop” is most commonly used to traverse over a data structure (i.e. a vector) and process each element in turn, often ultimately packaging all the results in a new structure. The syntax for the for looks like this: for (i in v) {do something with i} In this expression v is a structure we are traversing over and i is a new variable which will be used to refer to the currently referenced element while inside the loop. Also, be careful not to mistake the keyword in with an operator %in% which you may recall is used for checking whether some element exists in a certain structure. Using the for loop this way will give us access to the contents of the element, but we will not be able to infer information about the original structure (for example if we want to change the element in place). There’s an alternative way of using the for loop where instead of iterating over elements themselves, we are iterating over integers we then use inside the loop for positional indexing: for (i in 1:length (a)) {do something with a[i] This way, while less elegant, is more flexible and often preferable. Exercise 5.5 - for loop a &lt;- seq(-10, 10, 4) # print vector elements of `a` one by one # with the help of `for` loop # access the elements directly # do the same, but iterate by indexes a &lt;- seq(-10, 10, 4) # print vector elements of `a` one by one # with the help of `for` loop # access the elements directly for (i in a) print(i) # do the same, but iterate by indexes for (i in 1:length (a)) print(a[i]) ## [1] -10 ## [1] -6 ## [1] -2 ## [1] 2 ## [1] 6 ## [1] 10 ## [1] -10 ## [1] -6 ## [1] -2 ## [1] 2 ## [1] 6 ## [1] 10 Notice how the second way is better if you want to change the vector elements or need information on where the element is located in the original vector. Let’s now try a slightly more complex exercise. Exercise 5.6 - for loop (2) # read data from &quot;people.csv&quot; file into a data frame called `people` # then, using a `for` loop, for each numeric column # print its name and the mean of its values (function `mean`) # read data from &quot;people.csv&quot; file into a data frame called `people` # then, using a `for` loop, for each numeric column # print first its name and then the mean of its values (function `mean`) people &lt;- read.csv(&quot;people.csv&quot;) for (i in 1:length(people)) { if (is.numeric(people[[i]])) { print(names(people)[i]) print(mean(people[[i]])) } } ## [1] &quot;birthyear&quot; ## [1] 1978.404 ## [1] &quot;weight&quot; ## [1] 75.1223 ## [1] &quot;height&quot; ## [1] 160.5191 Be careful not to forget the difference between the [ and [[ operators. Also, note how we retrieved the name of the current element simply by using a names function to get the vector names (list is a vector, as you recall), and then leveraging positional indexing and i to get the actual name. Now that now that we have learned the loop syntax, it is important to emphasize one fact - in programming language R, it is generally not recommended to use program loops. Although this may initially seem unexpected and somewhat unorthodox, the reason is simple - as you may recall, R language is designed to work by the declarative “all at once” principle. We have already seen that the principle of vectorization and recycling effectively perform jobs that would often require constructing a programming loop in other programming languages, and in the chapters that follow, we will see that R also offers many other constructs that avoid explicit code repetition with the requirement of a declarative syntax that automatically performs it. For example, the following code is syntactically correct: # example of unnecessary loop usage a &lt;- 1: 5 b &lt;- 6:10 c &lt;- numeric() for (i in 1:length (a)) c[i] &lt;- a[i] + b[i] but probably works slower and is much less readable than: # R syntax a &lt;- 1:5 b &lt;- 6:10 c &lt;- a + b All of this does not mean that we should not use loops in R, but that their use should be accompanied by an additional consideration of whether the loop is really needed and whether there is an alternative syntax that completes the same task but can be written declaratively (and is potentially faster, since many routines in R are implemented in language C). Early adoption of the “R” mode of thinking will result in long-term benefits that will be reflected in a more compact, cleaner, and often more effective program code. Homework exercises Create a data frame with the following command: cities &lt;- data.frame(zipcode = c(10000, 51000, 21000, 31000, 2000), cityName = c(&quot;Zagreb&quot;, &quot;Rijeka&quot;, &quot;Split&quot;, &quot;Osijek&quot;, &quot;Dubrovnik&quot;), cityTax= c(18, 15, 10, 13, 10)) Add a column called “taxLevel” which will be the ordinal factor variable with the levels “small”, “medium” and “high” depending on whether the percentage of tax is strictly smaller than 12, between 12 and 15 or strictly greater than 15. Use the ifelse command. Replace the loops in the next block with equivalent vectorized operations (for the second loop, review the sum function documentation). a &lt;- numeric() i &lt;- 1 while (i &lt;= 100) { a &lt;- c(a, i) i &lt;- i + 1 } total &lt;- 0 for (i in a) { if (i %% 2 == 0) total &lt;- total + i * i } print (total) Program in Ru &lt;/ span&gt; by Damir Pintar is licensed under Creative Commons Attribution-NonCommercial-NoDerivative 4.0 International License Based on a work at https://ratnip.github.io/FER_OPJR/ "],["packages.html", "6 Packages, built-in functions and environments 6.1 Working with packages 6.2 Built-in functions 6.3 Environments Homework exercises", " 6 Packages, built-in functions and environments 6.1 Working with packages The standard R distribution comes with two collections of packages (called r-base and r-recommended) that contain a language R “core” of sorts - a set of building blocks sufficient for conducting the most typical data analysis tasks using the programming language R. In addition, CRAN (Comprehensive R Archive Network) provides a rich repository of additional packages for a wide variety of applications, from “quality-of-life” upgrades to basic elements to highly specialized packages aiming for extremely specific purposes. In line with common programming language practices, R uses a notion of “packages” or “libraries” to logically organize already “pre-programmed” collections of data, functions, and compiled code. Upon starting the R environment, certain packages are automatically loaded, making their content immediately available for use. The list of loaded packages can be easily obtained using the search function, which will then show us the so-called “search path”. Exercise 6.1 - the search path # call the `search` function (no parameters) # to see which packages are loaded into the environment # call the `search` function (no parameters) # to see which packages are loaded into the environment search() ## [1] &quot;.GlobalEnv&quot; &quot;package:corrplot&quot; &quot;package:broom&quot; ## [4] &quot;package:caret&quot; &quot;package:e1071&quot; &quot;package:car&quot; ## [7] &quot;package:carData&quot; &quot;package:Hmisc&quot; &quot;package:Formula&quot; ## [10] &quot;package:survival&quot; &quot;package:lattice&quot; &quot;package:sn&quot; ## [13] &quot;package:stats4&quot; &quot;package:gridExtra&quot; &quot;package:hflights&quot; ## [16] &quot;package:lubridate&quot; &quot;package:GGally&quot; &quot;package:forcats&quot; ## [19] &quot;package:stringr&quot; &quot;package:dplyr&quot; &quot;package:purrr&quot; ## [22] &quot;package:readr&quot; &quot;package:tidyr&quot; &quot;package:tibble&quot; ## [25] &quot;package:ggplot2&quot; &quot;package:tidyverse&quot; &quot;package:MASS&quot; ## [28] &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; ## [31] &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; ## [34] &quot;Autoloads&quot; &quot;package:base&quot; We see that most packages are listed as package::package_name. The positioning of the package name in the list also represents their “priority” in the namespace traversal path, which will be discussed later. If we want to load a new package into our environment, we can do this by using the library function with the package name provided as a parameter (without quotes). Exercise 6.2 - loading new packages into the working environment # load the `lubridate` package (or other package of your choice) # load the `lubridate` package library(lubridate) The command from the previous example commonly results in one of these two outcomes: if the package exists locally (in a folder designated for storing additional packages), it will be loaded into the working environment. Package loading can be accompanied by messages about objects that are “masked” after loading. This in particular means that the new package has temporarily denied access to certain elements from previously loaded packages because their names match. This often does not pose any problems, but if the user has the need to access the masked elements they will have to use their “full name” - e.g. they will also have to specify the name of the package where they are located. For example, if the filter function of the stats package is masked after loading the new package, it is still available through the full name stats::filter, but not directly through the name filter, as this will call the function from the latest loaded package. More details about how R resolves the names of variables and functions will be given below. if we do not have the above package on the local computer, we receive an error message that this package does not exist. In this case, supposing the package name is correct and it does exist on the CRAN repository, it is necessary to retrieve it from there using the install.packages function, which gives the name of one or more packages (with quotation marks!) as parameters. This function assumes that the R environment has a predefined CRAN mirror i.e. the specific address of the CRAN repository from where the package will be downloaded (a large number of countries have their own “copy” of the CRAN repository). If we are working in the RStudio interface, it has probably automatically turned on the setting which finds the closes CRAN repository, but in the very rare chance there are problems (which aren’t related to internet connectivity) it might be useful to check the documentation and see how the access to another CRAN repository can be established. Exercise 6.3 - installing a package from a CRAN repository # install the `hflights` package from the CRAN repository # (You can do this even if you already have the specified package) # load the package again into the work environment # print the search path # install the `dplyr` package from the CRAN repository # (You can do this even if you already have the specified package) install.packages(&quot;hflights&quot;) # load the package again into the work environment library(hflights) # print the search path search() ## [1] &quot;.GlobalEnv&quot; &quot;package:corrplot&quot; &quot;package:broom&quot; ## [4] &quot;package:caret&quot; &quot;package:e1071&quot; &quot;package:car&quot; ## [7] &quot;package:carData&quot; &quot;package:Hmisc&quot; &quot;package:Formula&quot; ## [10] &quot;package:survival&quot; &quot;package:lattice&quot; &quot;package:sn&quot; ## [13] &quot;package:stats4&quot; &quot;package:gridExtra&quot; &quot;package:hflights&quot; ## [16] &quot;package:lubridate&quot; &quot;package:GGally&quot; &quot;package:forcats&quot; ## [19] &quot;package:stringr&quot; &quot;package:dplyr&quot; &quot;package:purrr&quot; ## [22] &quot;package:readr&quot; &quot;package:tidyr&quot; &quot;package:tibble&quot; ## [25] &quot;package:ggplot2&quot; &quot;package:tidyverse&quot; &quot;package:MASS&quot; ## [28] &quot;package:stats&quot; &quot;package:graphics&quot; &quot;package:grDevices&quot; ## [31] &quot;package:utils&quot; &quot;package:datasets&quot; &quot;package:methods&quot; ## [34] &quot;Autoloads&quot; &quot;package:base&quot; The install.packages function has many additional parameters which might be a good idea to check, especially if certain problems arise while using the package. For example, setting the dependencies parameter to TRUE will explicitly state that all packages that the new package being installed is depending on need to be installed, too, if for some reason they didn’t get installed during the first installation try. Note: as a rule, we install packages only once, through the console so that there is never a need to install the package installations in R Markdown documents; also, for easier organization of reports, loading all required packages is usually done by at the beginning of the document, in the code snippet called setup. If we want to update packages, one of the easiest way to do this is to simply install them again (caution: if there is a huge discrepancy between package versions, this can cause issues with the code). If we want to find out more information about a package, one way to achieve this would be to do this would be calling the library function with the parameter called help set to the package name. library(help = dplyr) # recommendation: try directly in the console However, usually it’s more convenient to simply search for the package name (i.e. “CRAN dplyr”) in the web browser and then read the documentation PDF. Another quite popular way of perusing documentation of the package is with the help of the so-called “vignettes”. Vignettes are actually “mini-tutorials” of a package done in HTML which present the functionality of the package in an accessible way with the help of detailed explanations and the associated program code. We can look at which vignettes are installed on the system by calling the browseVignettes() function without parameters (or optionally adding as the parameter the package name if we only care about its vignettes). If the package has only one vignette (for example, stringr), we can also open the vignette immediately with the help of the vignette option. vignette(&quot;stringr&quot;) # recommendation: try directly in the console 6.2 Built-in functions In previous chapters, we have already introduced some of the functions that we get bundled in with our R distribution. These are, for example, numeric functions (log,abs, sqrt,round, etc.), vector creation functions (rep,seq, etc.), package management functions (install.packages, library, etc.) and so on. R rarely uses the term “built-in” functions since - as it was already shown - the R environment automatically loads some commonly used packages whose elements are immediately available for use, without necessarily indicating the name of the package they are in. Let’s take the stats package for example. This package contains a rich set of functions related to statistical processing. One of these functions is rnorm, which returns a numerical vector of the desired length whose elements are randomly selected from the normal distribution with arithmetic mean 0 and standard deviation 1 (these values can also be changed using the mean and sd parameters). If we want, we can easily invoke this function by using its full name, i.e. the package_name::function_name(parameters) syntax. Exercise 6.4 - calling a function with its full name # create a vector `x` that will have 10 random elements # drawn from standard normal distribution # use the full name of the `rnorm` function of the`stats` package # round the elements of vector `x` to two decimal places # use the full name of the `round` function from the `base` package # print vector `x` # create a vector `x` that will have 10 random elements # drawn from standard normal distribution # use the full name of the `rnorm` function of the `stats` package x &lt;- stats::rnorm(10) # round the elements of vector `x` to two decimal places # use the full name of the `round` function from the `base` package x &lt;- base::round(x, 2) # print vector `x` x ## [1] 1.43 0.98 -0.62 -0.73 -0.52 -1.75 0.88 1.37 -1.69 -0.63 Although this is a syntactically correct way of calling a function, it’s usually more common to see a call which excludes package names and simply names the function directly. Exercise 6.5 - calling a function without the package name # create vector `y` by the same principle as vector x # use only one line of code # use the function names without the name of the package # print vector `y` # create vector `y` by the same principle as vector x # use only one line of code # use the function names without the name of the package y &lt;- round(rnorm(10), 2) # print vector `y` y ## [1] 0.02 0.71 -0.65 0.87 0.38 0.31 0.01 -0.04 0.72 -0.50 We can ask ourselves - how does R know where to find the function’s implementation, if we did not explicitly specify the package where it’s contained? There’s a very easy answer to this, which will be outlined in the next subchapter dealing with environments.Before we delve into this, let’s just remind ourselves that R allows us to quickly get help on the function by simply calling ?function_name or help(function_name), and that we can get examples of using the function through example(function_name). We should use these calls very often even if we think that we are well acquainted with the function we are calling - it is possible that there is an additional parameter (or a related function that is also often listed in the documentation), which will further help us in carrying out the task related to the function we want to use. 6.3 Environments As already mentioned, working in R often boils down to writing instructions on how to manipulate various objects. To do this, we need mechanisms which help us easily refer to the objects concerned. In R (and other programming languages), this is called “binding”. When we created a variable called x of the numeric type and assigned a number 5 to it, what we actually did was asking R to reserve a place a memory for a new data container (for numerical data) and to document a new character string reference which was then “bound” to this container, allowing us to retrieve or change its contents. Let’s now try to think what happens when our code refers to x. When R sees this name, it must search its internal “records” to determine which variables currently exist, of what types they are and how to access them. In order to find the variable which corresponds to stated name, R uses a mechanism called “lexical scoping”, based on the concept of “environments”. An “environment” is often referred to as a “bag of names”. It helps us to logically group the names of the objects we use and to help R find the name in other environments if the variable does not exist in the current environment. The latter is enabled with the system that dictates that every environment needs to have its own “parent environment” (except the one on the very top, the so-called “empty environment”). This system of parent environment links then creates a kind of “environment hierarchy”, often referred to as “search path”; R, looking for the default variable name, searches the environments “upwards” until it finds the first appearance of the requested name or encounters an the empty environment. An interesting fact is that the environment itself is also an object - if we want, we can create a reference to it, send it to functions as a parameter, and so on. The “default” environment in which we work and in which we create new variables is the so-called “global environment”, or .GlobalEnv (watch out for the dot!). It is at the bottom of the environment hierarchy. We can get a reference to it via the mentioned variable name, or by using the globalenv() function. Exercise 6.6 - global environment # create a variable `e` which refers to the global environment # print `e` # create a variable `x` and assign the number `5` to it # execute the `ls` function, without parameters # execute the `ls` function with `e` as a parameter # print `x` # print `e$x` (notice the list syntax!) # create a variable `e` which refers to the global environment e &lt;- .GlobalEnv # or e &lt;- globalenv() # print `e` e # create a variable `x` and assign the number `5` to it x &lt;- 5 # execute the `ls` function, without parameters #ls() # try directly on console! # execute the `ls` function with `e` as a parameter #ls(e) # try directly on console! # print `x` x # print `e$x` (notice the list syntax!) e$x ## &lt;environment: R_GlobalEnv&gt; ## [1] 5 ## [1] 5 —–TU JE PROBLEM —– From the last example, we can see that the environment also references itself, so this is a completely correct (although unnecessarily complicated) syntax for printing the x variable: e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$e$x The environments are somewhat similar to lists, meaning that they basically “encapsulate” a certain number of objects in a unique structure which then allows for refering to them byh names. The most important differences between environments and lists are: the order of elements in the environment is irrelevant the environment (as a rule) has a link to its parent environment If we have a reference to an environment, we can easily get a reference to its parent environment using the parent.env function. Exercise 6.7 - parent environments # print out the parent environment of the global environment and explain the result # print out the parent environment of the global environment and explain the result parent.env(e) ## &lt;environment: package:corrplot&gt; ## attr(,&quot;name&quot;) ## [1] &quot;package:corrplot&quot; ## attr(,&quot;path&quot;) ## [1] &quot;C:/R/R-4.2.0/library/corrplot&quot; As we can see, the parent of the global environment is the last loaded package. This should not really be too unusual - the global environment has the largest “priority” when referencing the variables, but immediately below it are the objects and functions that we last loaded into the work environment (supporting the assumption that the “most recent” package is the one that we intend to use immediately). In other words, by loading a package, the new package is always “slotted in” between the global environment and the package that was previously loaded last. When we called the search function, what we actually got was the hierarchy of environments that represented all loaded packages in the order of their loading. In other words, the “search path” and “environment hierarchy” are basically the very same thing. If we want, we can make easily create our own environments and place them in their own hierarchy. We can even go one step further and create new variables in these environments (whose references will be tied to that environment instead of the global one). With these environments we can: put new variables in using the assign function or the combo of &lt;- and $ operators (like adding an element to a list) retrieve variables using the get function or the $ operator (like retreiving element from a list) list all variables using the ls function and the reference to the environment check if a variable exists using the exists function Look at the example below to get the feeling how custom environments work. # example - a small hierarchy of custom environments e &lt;- emptyenv() # the empty environment e2 &lt;- new.env() e3 &lt;- new.env() # hierarchy `e3` --&gt; `e2` --&gt; `e` (empty) parent.env(e2) &lt;- e parent.env(e3) &lt;- e2 # creating variable `x` in `e2` assign(&quot;x&quot;, 5, e2) # or e2$x &lt;- 5 # checking if there is an `x` in `e2` exists(&quot;x&quot;, e2) # listing all variables from `e2` ls(e2) # printing `x` from `e3` (even though it doesn&#39;t exist there) get(&quot;x&quot;, e3) # or e3$x ## [1] TRUE ## [1] &quot;x&quot; ## [1] 5 Why use the environment in practice? The environment is a convenient way of “wrapping” a set of variables that we can then send together in a function - for example variables which refer to large datasets. This was especially important in earlier versions of R that always defaulted to the so-called copy-on-modify mechanism, which goes: the function will use the reference to the original object sent to it as a parameter up to the point a command is met that will try to change said object; at that moment a copy of that object is created and all the changes pertain to the copy. This meant that sending lists containing very large elements could severely impact the performance since any changes on those elements would trigger copying them first, and then doing the change. An easy way to avoid this was simply wrapping those elements in environments instead. Since R 3.1.0 lists sent to a function will keep using a reference instead of making a deep copy, so the need to leverage the “environment workaround” is diminished. Finally, let’s demonstrate the attach function that certain data analysts tended to use to speed up the analysis process and write more readable code, but which can cause serious problems if used improperly. This function basically insert the data frame’s reference directly into the search path, allowing us to refer to its columns as if they were variable names in the global environment, without the need to reference the data frame itself. This sounds convenient, but has some very unfortunate side effects, best shown through an exercise. Exercise 6.8 - attach function cities &lt;- data.frame( zipcode = c(10000, 51000, 21000, 31000, 2000), cityName = c(&quot;Zagreb&quot;, &quot;Rijeka&quot;, &quot;Split&quot;, &quot;Osijek&quot;, &quot;Dubrovnik&quot;), avgSalary = c(6359., 5418., 5170., 4892., 5348.), population = c(790017, 128384, 167121, 84104, 28434), tax = c(18, 15, 10, 13, 10)) # execute function `attach` with `cities` as parameter # do this only once! # print the search path and comment on the result # print the `tax` &quot;variable&quot; # change the third element from the `tax` variable from 10 to 12 # print `cities`. Why is the tax in the 3rd row still 10? # execute the `ls` function # use the `detach` function to remove `cities` from the search path # execute function `attach` with `cities` as parameter # do this only once! attach(cities) ## The following object is masked from package:tidyr: ## ## population # print the search path and comment on the result search() cat(&quot;-------------------------\\n&quot;) # print the `zipcode` variable tax cat(&quot;-------------------------\\n&quot;) # change the third element from the `tax` variable to 12 tax[3] &lt;- 12 # print `cities` cities cat(&quot;-------------------------\\n&quot;) # execute the `ls` function #ls() # try it on console! # use the `detach` function to remove `cities` from the search path detach(cities) ## [1] &quot;.GlobalEnv&quot; &quot;cities&quot; &quot;package:corrplot&quot; ## [4] &quot;package:broom&quot; &quot;package:caret&quot; &quot;package:e1071&quot; ## [7] &quot;package:car&quot; &quot;package:carData&quot; &quot;package:Hmisc&quot; ## [10] &quot;package:Formula&quot; &quot;package:survival&quot; &quot;package:lattice&quot; ## [13] &quot;package:sn&quot; &quot;package:stats4&quot; &quot;package:gridExtra&quot; ## [16] &quot;package:hflights&quot; &quot;package:lubridate&quot; &quot;package:GGally&quot; ## [19] &quot;package:forcats&quot; &quot;package:stringr&quot; &quot;package:dplyr&quot; ## [22] &quot;package:purrr&quot; &quot;package:readr&quot; &quot;package:tidyr&quot; ## [25] &quot;package:tibble&quot; &quot;package:ggplot2&quot; &quot;package:tidyverse&quot; ## [28] &quot;package:MASS&quot; &quot;package:stats&quot; &quot;package:graphics&quot; ## [31] &quot;package:grDevices&quot; &quot;package:utils&quot; &quot;package:datasets&quot; ## [34] &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; ## ------------------------- ## [1] 18 15 10 13 10 ## ------------------------- ## zipcode cityName avgSalary population tax ## 1 10000 Zagreb 6359 790017 18 ## 2 51000 Rijeka 5418 128384 15 ## 3 21000 Split 5170 167121 10 ## 4 31000 Osijek 4892 84104 13 ## 5 2000 Dubrovnik 5348 28434 10 ## ------------------------- What happened up there? With the attach function, the cities data frame became a “mini-environment”, i.e. its columns became available within the search path. The obvious benefit of this is that we can refer to the columns directly, without referencing the original data frame and operator $. But this seemingly practical trick has hidden traps - first, if the column names match variable names in the global environment, then these variables will take precedence (we will get a warning when attaching, but not if we create new variables with identical names later on). Second - and much more problematic - if we try to change the column of the data frame by directly referencing just its name, R will silently apply the copy-on-modify principle by creating a new variable in a global environment that will be a copy of the referenced column. An inattentive analyst can completely miss the fact that the changes are not reflected at the data frame itself, and then proceed with the analysis as if these changes were made, which can have far-reaching consequences. These potential problems are very widespread among R beginners, so in the R literature it is commonly suggested that the attach function is not used unless it is deemed very necessary. For example. Google’s R-style guide says “the error potentials for using the attach function are numerous, so avoid it”. If we really dislike having to continual refer to the name of the data frame when referencing its columns, there are new packages that allow exactly this without the unfortunate side-effects of the attach function. We will be diving deep into these packages in one of the latter chapters - until then, the safest way to refer to data frame columns is to always use the $ operator coupled with the name of the data frame itself. Homework exercises Load the following packages in the working environment: magrittr, dplyr, tidyr,ggplot2. Print the search path and check where the loaded packages are. The following commands will create a vector of 20 randomly selected natural numbers from 1 to 100. # 20 random natural numbers from 1 to 100, with repetition set.seed(1234) a &lt;- sample(1:100, 20, replace = T) Use the cheat sheets and/or official documentation to find built-in functions that perform the following tasks. Print vector a the values of the vector a arranged in reverse order unique values from the vector a the values of the vector a sorted in ascending order We mentioned that loaded packages are actually “environments”. If we want to get a direct reference to them, we need to use as.environment and the package name. Try to get a reference to the package::magrittr package in the form of an environment, and use ls to check which names are contained within it. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],["user.html", "7 User Defined Functions 7.1 User defined functions 7.2 The apply family Homework exercises", " 7 User Defined Functions 7.1 User defined functions Although R is an object-oriented programming language, it leans heavily towards the realm of functional programming. This functional paradigm is not new (it dates back to the 1950s), but has recently gained considerable popularity as a kind of complement to the object-oriented programming, which might be said was the dominant programming paradigm in the last few decades. In order not to delve too deep into the subject of functional programming and its relationship to object-oriented principles, we will list only a few indicative features and guidelines related to these two leading paradigms. Object-oriented programming in principle, sees the program as a system of nouns where the components are realized in the form of objects that encapsulate the attributes of the mentioned noun and the methods that perform certain tasks related to the given noun. Also, object-oriented systems focus on the controlled change of the components’ state, commonly as a result of the exchange of messages between them. Functional programming sees the program as a system of verbs where the functions, i.e. the tasks we want to execute, have priority over the components over which these tasks are executed. Functional programming models the information system through components that generally do not change their own state, so that the result of the program is strictly dependent on the inputs, which facilitates testing and maintenance. In short, the distinction between object-oriented programming and function programming can be oversimplified as follows: “For object-oriented programming, we create data that contains functions, and in functional programming we create functions that contain data”. We do not have to be too bothered with the features of functional programming to learn R, nor should it be necessary to adopt a completely new programming paradigm. But for successful mastering of R, adapting some of the functional programming concepts can prove to be extremely useful since it will allow us to write cleaner and more efficient code that will be in accordance with the way in which R is designed as a language. In R, the following is true: functions are “first-order” objects, we can reference them with a variable of the selected environment, send them to functions as arguments, receive as return values function and store them in data structures, such as a list. The function in R is simply an “executable” object. A large number of functions - especially those that replace the program loop construct - work on the principle of functional languages where we perform the work in a manner that declaratively specifies which function we want to apply on which data structure, and let the programming language itself perform low-level tasks such as iteration by structure and preparation of the result. Examples of this will be learned soon, but now let’s first learn how to create our own functions in R. 7.1.1 Defining a new function In a general case, the definition of a new function looks like this: function_name &lt;- function(input arguments) { function_body return_statement } We can notice that the function definition leverages the operator &lt;-. This is not a coincidence - the definition of a function is merely creating a “callable” variable in a certain environment - the name of that variable is the name of the function we are creating. In R we do not define the types of input and output arguments. Input arguments have a name and an optional default value. The functions formally return one single value, which is not necessarily restrictive as it sounds - returning multiple variables simply means we wrap them first in the form of a vector or a list. The return keyword (whose syntax is like a function, i.e. return(x)) can be used to explicitly state what is being returned, but is actually optional - the function automatically returns the result of the last expression in the function, so we can simply put the name of the returning variable as the last line in the function, or even just leave the expression which calculates it, even though that can slightly compromise the readability. For example, we can write a simple function that doubles the input parameter like this: doubleUp &lt;- function(x) { y &lt;- 2 * x return(y) } or, shorter: doubleUp &lt;- function(x) { y &lt;- 2 * x y } or even like this: doubleUp &lt;- function(x) { 2 * x } Let’s try to create another simple function. For starters, let’s make our own implementation of the abs function, called myAbs. Exercise 7.1 - first user defined function # create a function called `myAbs` which # will mimic R&#39;s `abs` function (but will not call it directly) # create a function called `myAbs` which # will mimic R&#39;s `abs` function (but will not call it directly) # BAD SOLUTION! myAbs &lt;- function(x) { if (x &lt; 0) -x else x } # try myAbs(c(1, -1)). Why doesn&#39;t this work correctly? # create a function called `myAbs` which # will mimic R&#39;s `abs` function (but will not call it directly) # GOOD SOLUTION myAbs &lt;- function(x) { ifelse(x &lt; 0, -x, x) } We usually want to write functions which follow R’s philosophy of “everything is a vector”, so they should work properly on vectors, not just scalar, if it makes sense concerning the planned functionality. Finally, if we want to increase the robustness of the function in such a way that we reject the execution of the logic within a function if certain conditions are not satisfied, we can use the stopifnot function. This function calculates the default logical expression and terminates the function if the specified condition is not true. Exercise 7.2 - using stopifnot inside a function definition # write the function `parallelMax` which requires two numeric vectors as input # and returns a vector of the same size containing the larger # between two corresponding elements of the original vectors # if one or both vectors aren&#39;t numeric or aren&#39;t the same size # the function must throw an error # do not use loops! # execute this new function over the following vector pairs # c(T, F, T) i c(1, 2, 3) # c(1, 2, 3, 4) i c(5, 6, 7) # c(1, 2, 3) i c(0, 4, 2) # (second part of the exercise should be tested inside the console!) # write the function `parallelMax` which requires two numeric vectors as input # and returns a vector of the same size containing the larger # between two corresponding elements of the original vectors # if one or both vectors aren&#39;t numeric or aren&#39;t the same size # the function must throw an error # do not use loops! parallelMax &lt;- function(a, b) { stopifnot(is.numeric(a) &amp;&amp; is.numeric(b) &amp;&amp; length(a) == length(b)); ifelse(a &gt; b, a, b) } When calling a function, we can optionally specify the parameter names, and R will even allow the mixing of named and unnamed parameters (although this is not something we should often use in practice). When R connects the sent values with formal parameters, the named parameters will have priority and will be resolved first, after which the unnamed parameters will be resolved in te order they were provided. We can see this in the next exercise, in which we will use the opportunity to showcase a very useful function - paste. This function concatenates character strings with the addition of a space separator (there is an alternative function paste0 for joining without spaces). Exercise 7.3 - function parameters printABC &lt;- function(a, b, c) { print(paste(&quot;A:&quot;, a, &quot;B:&quot;, b, &quot;C:&quot;, c)) } # think before executing - what will be printed with the following command? #printABC(1, a = 2, 3) printABC &lt;- function(a, b, c) { print(paste(&quot;A:&quot;, a, &quot;B:&quot;, b, &quot;C:&quot;, c)) } # think before executing - what will be printed with the following command? printABC(1, a = 2, 3) ## [1] &quot;A: 2 B: 1 C: 3&quot; In practice, we should stick to conventions which dictate first using unnamed parameters in the order they appear in the function definition, and then named. It is customary to set only those named parameters whose default value does not suit us, whereas strict ordering does not matter (although using the order provided in the function definition will increase the legibility of our code). If we want to write a function that receives an arbitrary number of arguments, we can use the element ..., i.e. the ellipsis. An example of this feature is the built-in paste function which can receive an arbitrary number of character strings. If we use the ellipsis in our functions, we should place them at the end of the list of input arguments, and within the function itself we simply convert it into a list, and then access its parameters in the way that suits us. Exercise 7.4 - function with an arbitrary number of parameters printParams &lt;- function(...) { params &lt;- list(...) for (p in params) print(p) } # call the above function with any random parameters printParams(c(1, 2, 3), 5, T, data.frame(x = 1:2, y = c(T, F))) ## [1] 1 2 3 ## [1] 5 ## [1] TRUE ## x y ## 1 1 TRUE ## 2 2 FALSE 7.1.2 The “copy-on-modify” principle One of the more common questions raised when learning a new programming language is whether the functions work in “call-by-value” or “call-by-reference” mode. The difference is basically whether the function may change the content of the variables sent at the place of the formal argument or not; call-by-value principle forward only copies of original arguments. On the other hand, the call-by-reference principle makes it so the function receives “references” of the original variables, i.e. it behaves as if the original variables were passed to the function and all changes to them would be reflected in the calling function or program. The language R uses a hybrid principle known as copy-on-modify. With this principle, references are forwarded to the function, which allows us to transmit “large” variables without the fear of them getting unnecessarily copied. But this is only valid if the function does not change the value of the resulting variables - at the moment when the function attempts to make any changes, copying the variable is carried out and the function continues to work on the copy. Let’s check the above statements in a following examples. 7.1.3 Example 7.1 - “copy-on-modify” # creating a data frame df &lt;- data.frame(id = 1:5, name = LETTERS[1:5]) # function changes a column and returns nothing # (assignment operator does not have a return value) f &lt;- function(x) { x$name[1] &lt;- &quot;change!&quot; } # the original data frame is unchanged f(df) df ## id name ## 1 1 A ## 2 2 B ## 3 3 C ## 4 4 D ## 5 5 E The function actually creates a new temporary environment within which it stores “local” variables. In the above example, a new copy of the column in question is created and changed - but the original data frame column is unchanged. It is important to note that the function could access an external variable even without sending it to the function, since by referencing the x variable that does not exist in the local environment the R search function will continue in the parent’s environment, which in this case would be the global environment. Attempting to change this variable would still fail - R would detect an attempt to change the variable and create a local copy of the same name. 7.1.4 Example 7.2 - “copy-on-modify (2)” # creating a data frame df &lt;- data.frame(id = 1:5, name = LETTERS[1:5]) # function changes a column of the external data frame and returns nothing f &lt;- function() { df$name[1] &lt;- &quot;change!&quot; } # the original data frame is unchanged f() df ## id name ## 1 1 A ## 2 2 B ## 3 3 C ## 4 4 D ## 5 5 E One way to work around is is to wrap the data frame in its own environment. This way we can “force” the function to use call-by-reference, and at the same time ensure that no deep copies will be unnecessarily created. 7.1.5 Example 7.3 - “copy-on-modify” and environments # creating a data frame in a new environment e &lt;- new.env() parent.env(e) &lt;- emptyenv() e$df &lt;- data.frame(id = 1:5, name = LETTERS[1:5]) # function changes a data frame using a reference from the environment f &lt;- function(e) { e$df$name[1] &lt;- &quot;change!&quot; } f(e) e$df ## id name ## 1 1 change! ## 2 2 B ## 3 3 C ## 4 4 D ## 5 5 E A simpler way of solving the above task would be using the &lt;&lt;- operator. This operator’s function is to change the variable of the given name that is located somewhere in the search path. R will follow the search path, and change the first occurrence of the specified variable. If the variable of this name does not exist anywhere in the search path, R will create a new variable in the first environment above the environment of the function. 7.1.6 Example 7.4 - the &lt;&lt;- operator # operator `&lt;&lt;-` f &lt;- function(x) { x &lt;&lt;- 7 x &lt;- 6 } x &lt;- 5 f() x ## [1] 7 Let’s see how this would work with our data frame. 7.1.7 Example 7.5 - changing a data frame column with the &lt;&lt;- operator # creating a data frame df &lt;- data.frame(id = 1:5, name = LETTERS[1:5]) f &lt;- function() { df$name[1] &lt;&lt;- &quot;change!&quot; } f() df ## id name ## 1 1 change! ## 2 2 B ## 3 3 C ## 4 4 D ## 5 5 E However this operator should be used very cautiously, and we will achieve greater robustness using the assign or$operator with reference to the environment where the variable we want to change is. Finally, we must mention one feature of functions in R - the so-called. “lazy evaluation”. This simply means that R will not evaluate the received parameter until it is explicitly used. Up to that moment, this object is so-called. “promise” - R “knows” how to evaluate that object but it will not do so until it really needs it. This increases the efficiency of the language; if a parameter is used only in a conditional branch, then in scenarios when it is not needed it will not consume memory. But equally, we need to be careful because a lazy evaluation can lead to unexpected problems if we do not take into account its existence. 7.1.8 Function as an object We have already said that R has good support for the so-called “functional programming” which represents a programming paradigm that puts emphasis on designing functions without reliance on objects with interchangeable states. One of the characteristics of such languages are “first class functions”, which means that the language supports the definition of functions in such a way that they are equal objects to all other types of objects - they can be stored in a variable, used as an input argument of another function or as a return value, stored in other data structures, etc. Let us show this on a trivial example. We know that R offers the function sum within the base package, and this function calculates the arithmetic sum of the vector elements we send to it as an input parameter. But sum is actually the name of the variable that references the code that implements this function. If we want, we can very easily bind this function to some other variable by which we have effectively changed its name, or - better said - provided an alternative way of calling from a completely different environment. sum2 &lt;- sum sum2(1:10) # same as sum(1:10) This is easiest to understand in a way that the function is simply an “callable variable”, whereby the “call” refers to the use of a syntax that includes a reference to the function and input arguments framed in parentheses, which will return some value after execution in the R environment. The function can also be a return value from another function. funcCreator &lt;- function() { f &lt;- function(x) x + 1 f } newFunc &lt;- funcCreator() # we get the &quot;add one&quot; function newFunc(5) ## [1] 6 The function simply creates a new function and returns it to the calling program as it would have done with any other object. The return value is stored in the variable that is now “callable” - if we add brackets and parameters it will be executed in the way it is defined within the function that created it. Note that we could use the fact that the function returns the result of the last expression and define the function even shorter: # shorter definition funcCreator &lt;- function() { function(x) x + 1 } These functions are often referred to as “factories” or “generators” of functions, and in contrast to the above example, in practice, the function generator often receives some parameters that determine how the returned function will behave. Try to create a function factory that returns the multiplication functions using a pre-set parameter. Exercise 7.5 - function factory # create the `multiplicationFactory` function that creates # multiplication functions by the pre-set constant # use the above function to create the `times2` function # which doubles the received number # call the `times2` function with parameter 3 and print out the result # create the `multiplicationFactory` function that creates # multiplication functions by the pre-set constant multiplicationFactory &lt;- function(x) { function(a) a*x } # use the above function to create the `times2` function # which doubles the received number times2 &lt;- multiplicationFactory(2) # call the `times2` function with parameter 3 and print out the result times2(3) ## [1] 6 The multiplicationFactory function actually creates a “family” of functions that all provide the multiplication option with the selected number - i.e. parameter selected by the programmer itself. This way of managing functions may be initially confusing, but by using it in practice (which we will show in the next chapter), it is easy to notice the added flexibility and effectiveness of such an approach. If we define a function, and do not bind it to some variable, then we created the so-called “anonymous function”. # anonymous function function(x) x * x We can notice that each function is initially “anonymous”. If we return to the function definition syntax, we see that it is actually a combination of creating an anonymous function and binding it to a new variable. Of course, leaving the function anonymous as we did in the example above does not make much sense, just as it does not make sense to define a vector or list without creating a reference to that object - in that case the created object is not in any way usable because there are no links to it and will be quickly deleted by R within the “garbage collection” routine. We can ask ourselves - is there a scenario where the anonymous function is meaningful and useful? Explicit anonymous functions are used when a “disposable” function is needed, for example, as an argument for some other function. If the function we want to send as an argument is easy to define in a single line, and we do not plan to use it afterwards in the program, then it makes no sense to define it separately and assign it its own reference. An example of this will be seen in the apply family of functions. At the end of this section, we repeat the most important things - in R, the function is an object like any other, the only specificity is that it is an object that is “executable”, ie, which, using the function call syntax, does some work and returns some value. Even anonymous function can be executed (although only once, since we do not have a reference for future calls). # calling the anonymous function (function(x) x + 1)(2) ## [1] 3 7.2 The apply family Very often, knowledge of the basics of the language R is reflected by the skill of using the so-called apply family of functions, available in thebase package. These functions are specifically designed to perform repetitive tasks over various data structures, and as such they replace the program logic that would usually be realized in a through program loops. Additionally, these functions typically receive other functions as input arguments and, to a certain extent, encourage the functional programming paradigm. The family name comes from the fact that these functions commonly have a suffix “apply”. Some of the functions from this family are: apply lapply sapply vapply tapply,mapply, rapply … All of these functions work in a similar way - they receive a data set, a function that we want to apply to elements of that set, and optional additional parameters, and as the output give a set of function results, most often “packaged” in an appropriate format. The difference is mainly in the types of input and output arguments, as well as specific details about the implementation of the function itself and/or the way results are prepared. This family of functions is best learned through examples. We will begin with the “basic” function - apply. 7.2.1 The apply function The apply function is the only one that literally shares the name with the family of these functions. It is intended to work with matrices (actually with arrays, but since it is relatively rare to use data structures that have more than two dimensions, here we will focus only on matrices). The command syntax is as follows: result &lt;- apply( &lt;matrix&gt;, &lt;rows (1) or columns (2)&gt;, &lt;function_name&gt; ) Or, described in words, to implement the apply function, we: choose a matrix decide whether to “cut it” by rows or columns declare which function we want applied to each row (or column) Depending on how function works, as a result we get a matrix or (which is a more frequent case) a vector. Let’s try to use this function in a concrete example. Exercise 7.6 - the apply function m &lt;- matrix(1:9, nrow = 3, ncol = 3, byrow = TRUE) # print matrix `m` # use the `apply` function to calculate # and print the column sums of the `m` matrix # use the `apply` function to calculate # and print the multiplicaton of row elements # from the `m` matrix # print matrix `m` m cat(&quot;------------\\n&quot;) # use the `apply` function to calculate # and print the column sums of the `m` matrix apply(m, 2, sum) cat(&quot;------------\\n&quot;) # use the `apply` function to calculate # and print the multiplicaton of row elements # from the `m` matrix apply(m, 1, prod) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 4 5 6 ## [3,] 7 8 9 ## ------------ ## [1] 12 15 18 ## ------------ ## [1] 6 120 504 If we want to perform a custom task to the rows/columns, we often use an anonymous function, for example: apply(m, 1, function(x) x[1]) # return the first element of each row Exercise 7.7 - the apply function and anonymous functions # for each row of `m` calculate the natural logarithm # of the sum of row elements # rounded to 2 decimals # use the `apply` function # for each row of `m` calculate the natural logarithm # of the sum of row elements # rounded to 2 decimals # use the `apply` function apply(m, 1, function(x) round(log(sum(x)),2)) ## [1] 1.79 2.71 3.18 Let’s repeat - apply (and related functions) implicitly disassembles the input data structure into elements. In the examples above, these elements - rows or columns - are actually numeric vectors. The argument x received by an anonymous function is exactly that vector, or, better said each of these vectors that are sent one by one. The results of the function are “remembered” and “packed” into the final result. Let’s try to program the last example without using the apply function. Exercise 7.8 - loop as the alternative to the apply function # for each row of `m` calculate the natural logarithm # of the sum of row elements # rounded to 2 decimals # use the for program loop # for each row of `m` calculate the natural logarithm # of the sum of row elements # rounded to 2 decimals # use the for program loop rez &lt;- numeric(nrow(m)) for (i in 1:nrow(m)) rez[i] &lt;- round(log(sum(m[i,])), 2) rez ## [1] 1.79 2.71 3.18 If we compare the syntax of the examples with and without using the apply function, we can see how much the syntax which uses apply s actually “cleaner”. If we use loops, we must explicitly specify the logic of passing through the structure, which draws attention away from the job description that we actually want to do. What if we want to send to the apply function a function which needs several parameters? For example, let’s say that instead of the upper function that extracts the first line element we want a function with two parameters - the first a vector (matrix row or column), and the second an integer that indicates the index of the element to extract. The answer is simple - just add additional parameters at the end of the function call. # apply function and input function with multiple parameters apply(m, 1, function(x,y) x[y], 2) # second element of each row Finally, it should be noted that for similar processing of data in the matrix form, we do not necessarily need to use apply - many popular operations such as adding row or column elements, calculating the average of the elements of the rows and columns, and the like. This has already been implemented through functions such as rowSums,colSums, rowMeans,colMeans and the like. They are easier to use, but specialized - for more flexibility, the most common option is apply. 7.2.2 The lapply, sapply and vapply functions The name of the lapply function comes from list apply - i.e. apply the function to the elements of lists. To put simply - it is a function that will receive the list and a function as the input arguments, apply the functions to each individual list element and return again a new list as a result. Exercise 7.9 - the lapply function l &lt;- list(a = 1:3, b = rep(c(T, F), 10), c = LETTERS) # use the `lapply` function to calculate the length (number of elements) # of each element of the `l` list # use the `lapply` function to calculate the length (number of elements) # of each element of the `l` list lapply(l, length) ## $a ## [1] 3 ## ## $b ## [1] 20 ## ## $c ## [1] 26 Just like with the apply function, for thelapply function, we often use anonymous functions as a parameter. The following task has no special practical use, but it will help us understand the functionality of the lapply function in combination with a slightly more complex anonymous function. Exercise 7.10 - the lapply function and anonymous functions # process the elements of the `l &#39;list as follows: # - Calculate the mean value if it is a numerical vector # - count the values of TRUE if it is a logical vector # - calculate the length of the vector for all other cases # use the `lapply` function and an anonymous function # do not forget that anonymous function can also use blocks! # process the elements of the `l &#39;list as follows: # - Calculate the mean value if it is a numerical vector # - count the values of TRUE if it is a logical vector # - calculate the length of the vector for all other cases # use the `lapply` function and an anonymous function # do not forget that anonymous function can also use blocks! lapply(l, function(x) { if (is.numeric(x)) { mean(x) } else if (is.logical(x)) { sum(x) } else length(x) }) ## $a ## [1] 2 ## ## $b ## [1] 10 ## ## $c ## [1] 26 The lapply function is essentially quite simple to use and is very popular due to this fact. But once we use it for a while, we can find it irritating that t it always returns the list as a result, although some other data structure would be more suitable for us - for example a vector, especially if the resulting list has just simple numbers as elements. For this reason, R offers the unlist function to simplify the list to a vector. Exercise 7.11 - the unlist function l &lt;- list(a = 1:10, b = 10:20, c = 100:200) # calculate the mean value of the elements of the `l` list # print the results as a numeric vector # use `lapply` and `unlist` # calculate the mean value of the elements of the `l` list # print the results as a numeric vector # use `lapply` and `unlist` unlist(lapply(l, mean)) ## a b c ## 5.5 15.0 150.0 The displayed combination of lapply andunlist will give us as a result a one-dimensional vector, which in many cases is what we want. But sometimes some other data structure would suit us - for example, a matrix. In this case we need an additional step in transforming a one-dimensional vector into a matrix using the matrix function, with the number of rows and columns being explicitly assigned. The question may arise - why is lapply not able to check the structure of the result it has created and determine the optimal data structure for formatting it (vector, matrix, or list)? That’s exactly the idea behind the sapply function, or simplified list apply. This function first performs lapply internally, and then simplifies the result to a vector, matrix or array, depending on the characteristics of the results obtained. Exercise 7.12 - the sapply function l &lt;- list(a = 1:10, b = 10:20, c = 100:200) # calculate the median of elements of the `l` list # and collect the results in a numeric vector # use the `sapply` function # extract the first and last element of each of the elements of the `l` list # use `sapply` and anonymous function # calculate the median of elements of the `l` list # and collect the results in a numeric vector # use the `sapply` function sapply(l, median) cat(&quot;------------\\n&quot;) # extract the first and last element of each of the elements of the `l` list # use `sapply` and anonymous function sapply(l, function(x) c(x[1], x[length(x)])) ## a b c ## 5.5 15.0 150.0 ## ------------ ## a b c ## [1,] 1 10 100 ## [2,] 10 20 200 Note that as a result in the last example, we received a matrix, but that R formed it “by columns”. If we wanted a matrix with elements arranged in rows, we can not use sapply for this directly, because the matrix is formed internally, without the possibility of forwarding thebyrow = T parameter. To obtain such a matrix, one option is already mentioned with the combination of lapply,unlist and matrix, or - more simply - transposing thesapply results using t function (from transpose). The sapply function is quite popular due to its simplicity and efficiency, so it is relatively often used in interactive analysis. On the other hand, the use of this function in program scripts is not recommended since its result is unpredictable in the general case - e.g. the script can expect a matrix in the continuation of the code, and the sapply function, due to the specificity of the input data, returns the vector, which can cause unforeseen results, which is not easy to spot later and diagnose where the error occurred. If we are developing our own programs in R and want to use sapply, then the better choice will be thevapply function, which works identically to sapply, but uses an additional parameter called FUN.VALUE with which we explicitly define what kind of “simplification” we expect. For example. numeric(3) means that the result of applying the function to each element of the original list should be a numeric vector of three elements. If the result for any list item differs from the expected one, the function will raise an error. Exercise 7.13 - the vapply function myList &lt;- list(numbers &lt;- c(1:5), names &lt;- c(&quot;Ivo&quot;, &quot;Pero&quot;, &quot;Ana&quot;), alphabet &lt;- LETTERS) # think which of the following calls will be successful, # and which will throw out the error # check the results on the console vapply(myList, length, FUN.VALUE = numeric(1)) vapply(myList, function(x) as.character(c(x[1], x[2])), FUN.VALUE = character(2)) vapply(myList, function(x) as.logical(x), FUN.VALUE = character(1)) Finally, let’s return briefly to lapply and consider one important fact - it is intended for use on lists, and data frames are actually lists. In other words, the lapply function is very handy for processing tabular datasets when we want to apply a particular function to the columns of the data frame. One of the more frequent operations performed in data analysis is the so-called. “standardization” of the numeric columns of the data frame - i.e. reducing all numerical values to “normal” distribution with the arithmetic mean of 0 and standard deviation of 1. This can be done by reducing each individual value by the arithmetic mean of the column (the mean function) and dividing with standard deviation of the column (function sd). This is a great way to demonstrate the use of lapply with data frames. Exercise 7.14 - the lapply function and data frames df &lt;- data.frame( a = 1:10, b = seq(100, 550, 50), c = LETTERS[1:10], d = rep(c(T,F), 5), e = -10:-1) # standardize numerical columns using `lapply` # do not change the remaining columns # round the normalized values to three decimal places # save the result in the df variable # print df # standardize numerical columns using `lapply` # do not change the remaining columns # round the normalized values to three decimal places # save the result in the df variable df &lt;- lapply(df, function(x) { if (is.numeric(x)) { round((x - mean(x))/sd(x), 3) } else x }) # print df df ## $a ## [1] -1.486 -1.156 -0.826 -0.495 -0.165 0.165 0.495 0.826 1.156 1.486 ## ## $b ## [1] -1.486 -1.156 -0.826 -0.495 -0.165 0.165 0.495 0.826 1.156 1.486 ## ## $c ## [1] &quot;A&quot; &quot;B&quot; &quot;C&quot; &quot;D&quot; &quot;E&quot; &quot;F&quot; &quot;G&quot; &quot;H&quot; &quot;I&quot; &quot;J&quot; ## ## $d ## [1] TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE TRUE FALSE ## ## $e ## [1] -1.486 -1.156 -0.826 -0.495 -0.165 0.165 0.495 0.826 1.156 1.486 We see that after using lapply we get a list and that if we want the result in the form of a data frame we need to add another step using theas.data.frame function. If we are looking for a simpler way that immediately gives the data frame as a result, there is one convenient “trick” that we will explain below. Let’s look at the solution of the previous problem, and put the following little change in the assignment of the result of the lapply function: df[] &lt;- lapply(...) In this way, R will not create a “new” variable named df, but rather thelapply result will be entered in the ’all rows and columns of the df data frame. This made it possible for us to get the result in the form of a data frame, which we actually wanted. For this very reason, in R scripts, we will often see a similar syntax (df [] &lt;- lapply ...). Try to modify the above example in the above manner and make sure that the result will be a data frame. Another commonly used trick in working with data frames is the following command: sapply(df, class) This command actually gives us the answer to the question - which types are the columns of the given data frame? Although there are other ways to get this information, this method is popular both because of the compactness of the results and the independence of additional packages. 7.2.3 Other functions from the apply family and the available alternatives In the previous chapters, we have probably listed the most popular members of the apply family. This family has more members, including some who do not have a suffix -apply: mapply, which works in parallel over multiple data structures rapply, which recursively applies functions within the structure tapply, which applies functions over sub-groups within a structure defined by factors Map, themapply version, which does not simplify the result by, thetapply version for data frames etc. The reason why these functions will not be explained in detail is twofold: firstly, as already mentioned, these functions are in practice applied much less often than the functions we have shown in the previous chapters. Secondly, with the increase in popularity of the language R, a large number of packages are oriented to improve the existing functions of the language R in the sense of easier and more efficient programming, especially when working with data frames. If we are looking for convenient alternative functions to those from the apply family, it’s recommended to look at some of the following packages plyr - an extremely popular package that, among other things, offers a number of functions very similar to apply functions, but derived in a way that they have a consistent signature and explicitly defined input and output formats that are easily read from the function name itself (in particular, the first letters ); so the llply function uses a list as both the input and the output, whilemdply needs a matrix as input and outputs a data frame purrr - a package that replaces the functions of theapply family with functions corresponding to similar functions from other programming languages; since the application of the same function to a number of elements of a data structure in functional languages is often called “mapping”, the set of functions of this package carries the prefix maps_, and the function names often correspond to the expected results (for examplemap2_lgl means that as a result we expect a logical vector , and the map2_df a data frame) dplyr - a relatively new package, which in a certain sense represents the successor of the plyr package but oriented almost exclusively toward data frames; the functions of this package are not so much oriented to replace the apply family functions as providing a specific platform for working with data frames in a manner similar to languages oriented precisely for this purpose, such as, for example, the SQL language In future lectures we will introduce the dplyr package precisely because this package greatly facilitates and accelerates the data analysis process and is extremely well accepted in the R community. Homework exercises R has a which function which converts a logical vector into a numeric one containing indexes where the original vector has a TRUE value (so c(T, F, F, F, F, T, F, T) becomes c(1, 6, 8)). Create a function which replicates this behaviour. Take the numerical vector x of length n. In statistics, the standardized moment of the k-th order is calculated like this: \\[\\frac{1}{n}\\sum_{i=1}^n{(x_i - \\bar{x})}^{k+1}\\] Create a factory of moment functions (moment(k)) for calculating the standardized central moment of the k-th order. Create the functions zero_moment(x) and first_moment(x) with parameter values k set to 0 and 1 accordingly. Test the functions on vector 1:1000. Compare the results given by the sd (standard deviation) function over the vector 1:1000 and root of the first moment you have calculated. Take the m matrix created by the following command: m &lt;- rbind(1:5, seq(2, 10, 2), rep(3, 5), 3:7, seq(100, 500, 100)) With the apply function and the new anonymous function, create a vector that will contain the first even element of each row, or zero if the corresponding row does not have even elements. The following commands will create a list of 100 elements where each element will be a numeric vector of a random length of 1 to 10. set.seed(1234) myList &lt;- replicate(100, sample(1:10, sample(1:10, 1))) With the help of lapply / sapply (and additional commands if necessary), create: the numerical vector v with the lengths of the list elements list l with normalized numerical vectors of the original list numerical vector ind4 with indexes of all list elements containing number 4 the df5 data frame containing columns which have all the elements of the length of 5 from the original list Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],["objects.html", "8 Object oriented systems in R 8.1 Object-oriented systems in R 8.2 Overview of the S3 object model 8.3 Generic functions 8.4 Adding generic functions to an object 8.5 Short overview of S4 objects Homework exercises", " 8 Object oriented systems in R 8.1 Object-oriented systems in R R is designed as an object-oriented language, together with the most commonly listed mechanisms that the object-oriented paradigm dictates: encapsulation (enveloping various attributes into a common entity), polymorphism (different classes of objects leveraging the same interface) and inheritance (new objects can be created from the existing ones by extending them with additional elements). R has inherited its initial object modelling system from the S language, which resulted in these objects being known as “S3 objects” (S3 being the language version from which they were originally taken). This object model, as will be showcased very soon, is actually rather simple and unconventional, yet fairly suitable for using in R itself, it being primarily a domain-oriented language focused on data science-related tasks. However, with the continual influx of professional programmers into the R community, the pressure increased to bring support for “proper” objects that will be more similar to the way they work in the other programming languages, mostly in order to bring better robustness in design and management of objects. At the time of this writing, we formally have four types of objects in the programming language R : “base” classes - denoting basic R elements (functions, vectors, data frames) S3 objects - OOP system inherited from S language (version 3) S4 objects - a more formal and rigorous OOP system approaching standard object-oriented mechanisms from other languages RC objects (reference classes) - the most recent way of creating objects that replicates the “classical” object-oriented principles based on the exchange of messages; a subsystem of RC is called R6, offered by the package of the same name The existence of roughly three separate models of defining objects (we ignore the basic object model since we are interested in those that support the OOP principle) can seem disheartening - is it necessary to learn all three models? How to distinguish them? Which one to choose? However, despite the fact that the story about the object nature of R during its development is (unnecessarily) complicated, the good news is that for most needs, it’s quite enough to learn how the S3 model works, which is also the simplest. A large number of popular R packages use only S3 classes and it is possible to work in language R for a very long time without meeting the need for learning S4 or RC models. For this reason, we will mostly focus on the S3 objects, with a very brief overview of S4 (readers who want more information on other models, especially the RC one which is outside the scope of this coursebook, are strongly advised to check the book called Advanced R, by the author Hadley Wickham, which has multiple chapters devoted to object oriented programming in R). 8.2 Overview of the S3 object model As already mentioned, S3 objects are actually inherited from the S programming language and represent a relatively primitive implementation of the concept of “object”, as far as expectations go regarding standard mechanism of object creation management. This is most easily communicated with the following statement: An S3 object is merely a list which has an additional class attribute. This means that an S3 object can be created in the following way: Example 8.1 - creating a new S3 object # we are creating a new object of a class `Person` pero &lt;- list(id = &quot;12345678&quot;, surname = &quot;Peric&quot;, weight = 78) class(pero) &lt;- &quot;Person&quot; Notice that we do not have a formally defined “class template” as is the established practice in other programming languages, it is only implied by the object’s structure. In practice however we would probably not expect from our users to construct objects “manually”, but rather to use a special constructor function whose parameters will ultimately define the object’s internal structure. The function would internally use the same code as above, possibly including some additional checks whether the parameters are defined correctly. Let us create such a constructor function. For simplicity’s sake, let’s forgo for now the usual insistence on vectorization and assume users will always create a single “Person” object at a time (even though realistically you should aim for making functions which allow for creating of multiple objects at once, if possible and suitable for their planned use). Exercise 8.1 - constructor function # create a function called `Person` # input parameters: id, surname, weight # returns: object of class &quot;Person&quot; # before creating the object function should check that: # - id is a character vector, a string of 8 characters # - surname is a character vector # - weight is a numeric vector, larger then 0 # create `john`, a `Person` with the following characteristics: # id: 13571313, surname: Watson, weight: 76 # print `john` # create a function called `Person` # input parameters: id, surname, weight # returns: object of class &quot;Person&quot; # function should check that: # - id is a character vector, each character string 8 characters (`nchar` function!) # - surname is a character vector # - weight is a numeric vector, larger then 0 Person &lt;- function(id, surname, weight) { stopifnot(is.character(id) &amp;&amp; nchar(id) == 8) stopifnot(is.character(surname)) stopifnot(is.numeric(weight) &amp;&amp; weight &gt; 0) p &lt;- list(id = id, surname = surname, weight = weight) class(p) &lt;- &quot;Person&quot; p } # create `john`, a `Person` with the following characertistics: # id: 13571313, surname: Watson, weight: 76 # print `john` john &lt;- Person(&quot;13571313&quot;, &quot;Watson&quot;, 76) john ## $id ## [1] &quot;13571313&quot; ## ## $surname ## [1] &quot;Watson&quot; ## ## $weight ## [1] 76 ## ## attr(,&quot;class&quot;) ## [1] &quot;Person&quot; As we can see, the obvious advantage of making constructor functions for our S3 objects is additional robustness in the form of object’s structure being enforced by the way the constructor function is called, as well as the capabilities of embedding additional checks and requirements inside the constructor code. Let’s now talk about inheritance, i.e. a mechanism which allows for creating new objects (or classes) out of the existing ones, inheriting their current features while adding new (or modifying the inherited) ones. The S3 OOP system enables inheritance, but again in a very informal and relatively trivial way. Similar to how we created an object of a certain class by simply taking a list and declaring it is a member of a certain class through the class attribute, we can also use this very same attribute to list the “parents” of the class, i.e. classes that this object is inheriting features from. The names of parent classes should be placed in the same character vector as the class name, so they follow immediately after, sorted by ‘importance’. For example, the following code creates a new mate object of class Employee which inherits the class Person: Example 8.2 - inheriting an S3 object mate &lt;- list(id = &quot;12345678&quot;, lastname = &quot;Peric&quot;, weight = 78, yearEmpl = 2001) class(mate) &lt;- c (&quot;Employee&quot;, &quot;Person&quot;) Again, a better solution would of course be creating a new constructor function which would take care to properly inherit the parent structure, and quite possibly integrate the constructor function of the parent object themselves in the construction process. 8.3 Generic functions Looking at the above way of designing objects, we can notice that we completely avoided discussion of an important feature of any OOP system - the methods, i.e. functions that are being encapsulated inside an object. And here we come to one of the more striking differences between the S3 object model and the “standard” OOP systems we might be familiar with from the other programming languages - in the S3 object model the methods are defined outside of the objects themselves, in the form of so-called generic functions. This should not necessarily be so unusual as it initially seems. If we go back to the original description of polymorphism, we can see the basic idea should be that the user is able to ask for executing similar functions (eg “print”, “draw”, “summary description”) over objects of different types. The “generic” expected functionality of a chosen function should be the same, but the specifics would differ depending on the object itself - hence the name generic function. For example, we expect that the print function will always results in some kind of printout on screen, but how this printout ultimately looks will depend on the specifics of the object we are printing. This way of designing an object can seem rather unconventional, but the fact stands that the code that uses generic functions instead of encapsulated methods tends to look far more intuitive, especially for users who do not have an extensive background in programming. Specifically, let’s compare a hypothetical command: start(car, speed = 20) with the command: car.start(speed = 20) By reading the first command, the car is perceived as an “object” (in grammatical terms), that is, an entity we ask for something to be performed on. The second command sets the car as a “subject”, which is a common practice of the object-oriented languages, but is not in line with the general understanding of how we perceive performing tasks, especially in data analysis - we manipulate and transform datasets and we draw plots, as opposed to asking data to manipulate themselves or a plot to get itself drawn. So to sum up, R is designed in such a fashion that enforces thinking about what we want to do instead of asking ourselves where the function we want to call is. If we want to print an object, it is logical to just simply forward it to the print function, and if we want to get a brief summary about it, we try calling the summary function. The big question is of course - how can one single function “know” what to do with an object? Does it need to have the implementation code for every single class it could conceivably be called on? The answer is actually rather simple - the generic function is just an “interface” to the function we “actually” want to call, and the programmer of the object (or class) is responsible for creating a separate code with the implementation logic. The trick is merely in taking care to follow the naming convention - if the generic function is called genFun, and the name of the object class is className, the function storing the implementation code needs to be called genFun.className, and should be available in the search path when the generic function is called. If no such function exists, than the function genFun.default will be called instead. We can see the proof of this in the next exercise. Exercise 8.2 - generic functions # print the source code `summary` function (just use the function name!) # print the source code of a function actually being called when you call # the `summary` function of the `factor` class object # print the `summary` function (only the function name!) summary # print the source code of a function actually being called when you call # the `summary` function of the `factor` class object summary.factor ## standardGeneric for &quot;summary&quot; defined from package &quot;base&quot; ## ## function (object, ...) ## standardGeneric(&quot;summary&quot;) ## &lt;environment: 0x000002b492941588&gt; ## Methods may be defined for arguments: object ## Use showMethods(summary) for currently available ones. ## function (object, maxsum = 100L, ...) ## { ## nas &lt;- is.na(object) ## ll &lt;- levels(object) ## if (ana &lt;- any(nas)) ## maxsum &lt;- maxsum - 1L ## tbl &lt;- table(object) ## tt &lt;- c(tbl) ## names(tt) &lt;- dimnames(tbl)[[1L]] ## if (length(ll) &gt; maxsum) { ## drop &lt;- maxsum:length(ll) ## o &lt;- sort.list(tt, decreasing = TRUE) ## tt &lt;- c(tt[o[-drop]], `(Other)` = sum(tt[o[drop]])) ## } ## if (ana) ## c(tt, `NA&#39;s` = sum(nas)) ## else tt ## } ## &lt;bytecode: 0x000002b49fbe87b0&gt; ## &lt;environment: namespace:base&gt; 8.4 Adding generic functions to an object The properties of the generic function are as follows: the function has an intuitive, clearly defined purpose expected mode of operation is similar for multiple object types (e.g. drawing will always result in an image of sorts) each type of object requires its own implementation depending on the characteristics of the object (e.g. the way the method of drawing the circle differs from drawing a square) The method of implementing generic functions (for S3 objects!) is actually extremely simple, which is probably the reason for their wide acceptance and great popularity in the R community. The process is reduced to three simple steps: choose the name of a generic function and declare that it is a generic function with the help of a function called UseMethod alternatively, ignore this step and choose one of the existing generic functions (often a much better choice!) create an object and declare its class implement the function called &lt;gen_function_name&gt;.&lt;class_name&gt; (optional) if you created a completely new generic function, consider implementing &lt;gen_function_name&gt;.default function and as many other implementation functions as you want, if you think the function should work on other types of objects, too And that’s all! R does not require any additional steps, the above is quite sufficient for R to recognize the new generic function and apply it to all objects for whose class this generic function is implemented in the form gen_function_name.class_name (or gen_function_name.default for all classes for which there is no special implementation). In the next exercise, let’s try to implement the generic infoAbout method for the Person class. Exercise 8.3 - new generic function peter &lt;- Person(id = &quot;12345678&quot;, surname = &quot;Parker&quot;, weight = 78) # create a new generic `infoAbout` function using the `UseMethod` function infoAbout &lt;- function(x) UseMethod(&quot;infoAbout&quot;) # implement a `infoAbout.Person` function which takes a `Person` # and writes the following on screen # ID: &lt;id&gt;, surname: &lt;surname&gt;, weight: &lt;weight&gt; # use the `paste` function to prepare the printout # and `cat` to put it on screen # implement a `infoAbout.default` function # which simply fowards the input parameter to the `cat` function # call `infoAbout` with `peter` as parameter # call `infoAbout` with `1:5` as parameter # implement a `infoAbout.Person` function which takes a `Person` # and writes the following on screen # ID: &lt;id&gt;, surname: &lt;surname&gt;, weight: &lt;weight&gt; # use the `paste` function to prepare the printout # and `cat` to put it on screen infoAbout.Person &lt;- function(p) { rez &lt;- paste0(&quot;ID:&quot;, p$id, &quot;, surname:&quot;, p$surname, &quot;, weight:&quot;, p$weight, &quot;\\n&quot;) cat(rez) } # implement a `infoAbout.default` function # which simply fowards the input parameter to the `cat` function infoAbout.default &lt;- function(x) cat(x) # call `infoAbout` with `peter` as parameter infoAbout(peter) # call `infoAbout` with `1:5` as parameter infoAbout(1:5) ## ID:12345678, surname:Parker, weight:78 ## 1 2 3 4 5 Of course, it doesn’t really make sense to have our own function for printing out stuff on screen when there are already perfectly good generic function for that purpose such as print or cat. With that in mind, let’s augment the print function so it works with our Person objects. Since we have already created our infoAbout function, and we are already armed with the knowledge that functions are first-class objects, it should be trivial to allow for Person objects to be nicely printed on screen with print. Exercise 8.4 - augmenting the existing generic functions # make sure `print` is a generic function # (print out its source code by referencing its name) # augment the `print` function so it allows pretty printing # of the `Person` class # (you may use the already created `infoAbout.Person` function) # call `print` with `peter` as parameter (or try the autoprint!) # make sure `print` is a generic function # (print out its source code by referencing its name) print # augmnet the `print` function so it allows pretty printing # of the `Person` class # (you may use the already created `infoAbout.Person` class) print.Person &lt;- infoAbout.Person # call `print` with `peter` as parameter (or try the autoprint!) print(peter) ## function (x, ...) ## UseMethod(&quot;print&quot;) ## &lt;bytecode: 0x000002b48e0f8538&gt; ## &lt;environment: namespace:base&gt; ## ID:12345678, surname:Parker, weight:78 ***. Finally, we demonstrate the ability of R to list all the currently known implementations of a generic method. To do this we simply use the methods function to which we pass the name of the method concerned. With the same function we can also check which generic functions implementations exist for a particular class. For this we use the class parameter to which we are passing the class name for which we are interested in finding generic functions implemented for it. Exercise 8.5 - methods function # list all implementations of the `summary` function # check with generic function implementations exist for the `factor` class # list all implementations of the `summary` function #methods(summary) # try on console! cat(&quot;-----------------------\\n&quot;) # check with generic function implementations exist for the `factor` class methods(class = &quot;factor&quot;) ## ----------------------- ## [1] - / [ [[ [[&lt;- ## [6] [&lt;- + all.equal Arith as.character ## [11] as.data.frame as.Date as.duration as.interval as.list ## [16] as.logical as.period as.POSIXlt as.vector as_date ## [21] as_datetime as_factor brief c cbind2 ## [26] coerce Compare corresp droplevels fixed ## [31] format histogram initialize is.na&lt;- length&lt;- ## [36] levels&lt;- Logic Math Ops plot ## [41] print rbind2 recode relevel relist ## [46] rep scale_type show slotsFromS3 summary ## [51] Summary type_sum xtfrm ## see &#39;?methods&#39; for accessing help and source code 8.4.1 Conclusion on S3 objects In short, the conclusions about S3 objects can be as follows: S3 objects function in in a simple, informal way - they are simply lists with the arbitrary value of class attributes the S3 object methods are not encapsulated within the objects, but are designed “out of” objects in the form of generic functions much of this is left to the responsibility of the programmer who should enforce the object structure through *constructor functions and the way to handle objects through generic function implementations** S3 objects are not suitable for complex object models due to heavy model maintenance and large potential of errors 8.5 Short overview of S4 objects We will not be dealing with S4 objects in detail, but will provide a very brief overview how they look and what are some of the most significant changes from the S3 model. S4 objects offer a slight “upgrade” on S3 objects in the sense that they dictate more strictness and formality when defining and using classes, while still retaining most of the simplicity of the S3 model. The most notable (and arguably welcome) change is allowing for a formal definition of a class template. An S4 class has three main properties: name, which identifies it representation, which describes its attributes (called “slots”) (optionally) a vector of parent classes (that it “contains”) The S4 equivalent of our S3 Person class would therefore look like this: Example 8.3 - creating a new S4 class # template definition setClass(&quot;Person&quot;, representation(id = &quot;character&quot;, surname = &quot;character&quot;, weight = &quot;numeric&quot;)) # class instantiation pero &lt;- new(&quot;Person&quot;, id = &quot;12345678&quot;, surname = &quot;peric&quot;, weight = 76) Another difference is that instead of $ you use the @ operator to access the attributes (slots) of a class. Also, the function getSlots will return all the slots of the class in question. Example 8.4 - accessing S4 class slots # list class `Person` slots getSlots(&quot;Person&quot;) # retrieve slot values from `pero` instance paste0(pero@id, &quot;: &quot;, pero@surname) ## id surname weight ## &quot;character&quot; &quot;character&quot; &quot;numeric&quot; ## [1] &quot;12345678: peric&quot; S4 also leverages generic functions, but with certain differences when it comes to the syntax of creating them. We will not delve into specifics and will just briefly show one way of assigning existing generics to a new S4 object through the usage of a setMethod function. Example 8.5 - S4 objects and generics setMethod(&quot;print&quot;, signature(x = &quot;Person&quot;), function(x) { rez &lt;- paste0(&quot;ID:&quot;, x@id, &quot;, surname:&quot;, x@surname, &quot;, weight:&quot;, x@weight, &quot;\\n&quot;) cat(rez) }) print(pero) ## ID:12345678, surname:peric, weight:76 And this is where we finish our overview of R’s OOP systems. As stated, the S3 objects are still the most prevalent, and S4 are worth knowing if you come upon them in certain packages (even though most of the time it’s enough to remember to use the @ operator instead of $ to extract their attributes). As for other types of objects (including the relatively popular RC or R6 classes), they are outside the scope of this course and will be left as optional concepts to discover on your own. In general, R is probably not the best choice for implementing software solutions that heavily rely on intricate object-oriented architectures - but regardless of that, there are packages and models that allow this, with ever rising support and new features, so there is always an option of going down this road if the need arises. Homework exercises The following exercises all assume usage of S3 classes. Create a class object Block with the attributes height, width and depth equal to10, 20 and 30 respectively. Implement a constructor for the class Employee which inherits the Person class defined by the following constructor and print implemenation: Person &lt;- function(id, surname, weight) { p &lt;- list(id = id, surname = surname, weight = weight) class(p) &lt;- &quot;Person&quot; p } print.Person &lt;- function(p) { rez &lt;- paste(&quot;ID:&quot;, p$id, &quot;, surname:&quot;, p$surname, &quot;, weight:&quot;, p$weight, &quot;\\n&quot;) cat(rez) } Employee has all the attributes of the Person class as well as the superior attribute which represents a reference to a employee who is his/her superior (if such exists, otherwise it should be NA). Create two objects of the Employee class (one superior to another) and print them with the print function. Then implement your own version of the generic function print for the Employee class that prints employee data and his/her superior employee data (if it exists, otherwise it prints a message that there is no superior employee). Reprint both employees with the print function. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],["pipe.html", "9 Pipeline operator and tidy data 9.1 Pipeline operator 9.2 Tidy data Homework exercises", " 9 Pipeline operator and tidy data 9.1 Pipeline operator Let’s look at the following example: imagine that in R we want to create 100 random real variables from the range of [0,100], round them to two decimals, select a sample of 10 variables from this set, calculate the arithmetic mean of the sample and print it on the screen. One possible solution could be: set.seed(1234) # (for repeatability) res &lt;- runif(100, 0, 100) # 100 random variables from uniform distribution from 0 to 100 res &lt;- round(res, 2) res &lt;- sample(res, 10) res &lt;- mean(res) res ## [1] 46.651 This kind of code has a lot of unnecessary repetition - in every line we use the res variable as well as the assignment operator res variable so we can store the in-between results. Alternatively, we could do everything in one row: Exercise 9.1 - Nested functions set.seed(1234) # repeat the above example, but with only one line of code mean(sample(round(runif(100, 0, 100), 2), 10)) ## [1] 46.651 Here we see a typical example of a “code sandwich” which is not only often seen in R, but also appears in most programming languages. When we use the result of one function as an input to another, we have to “nest” it in the subsequent function call ending up with an error-prone code which is equally difficult to both write and read. A natural way of “solving” a series of instructions from the previous example would be to resolve them sequentially, i.e. from “left to right”; when we finish one task, the result becomes the input for the following task and so on until the process is complete. It would be benefitial if we apply this approach through programming code. This was precisely the motivation for the development of the so-called pipeline operator, provided by the package magrittr (Bache and Wickham 2014). The weird name package is actually inspired by the name of abstract painter Rene Magritte, more specifically his famous painting La trahison des images depicting a pipe under which are the words Ceci n’est pas une pipe. In the same way, the magrittr package delivers a pipeline or pipe operator %&gt;%which is “not really a pipe”. Whatever you may think about this play on words or the painting itself which inspired this package, what is undeniable is the fact that the pipeline operator makes the code much more readable and as such has quickly become a strong favorite in the R community, especially when programming heavily relies on chaining function calls. How does the %&gt;% operator work? It’s rather simple - we place it between function calls to designate that the output from the function on the left should be the input for the function on the right. The place where the result from the previous function should go is denoted by the . symbol. We can do this as many times as we want, that is, depending on how many functions calls we “chain”. h(g(f(x), y), z, w) # code without the %&gt;% operator f(x) %&gt;% g(., y) %&gt;% h(., z, w) # code with the %&gt;% operator If the result of the previous function the first argument of the next function, then the . symbol (or in effect the whole argument) can be thrown out, so the syntax is even shorter: f(x) %&gt;% g(y) %&gt;% h(z, w) # code without dots Notice that these function calls are actually formally incorrect because they have an “invisible” first argument. Nevertheless, many R programmers prefer this syntax because it requires less typing and is somewhat more transparent, while the mentioned irregularity does not really matter in practice as long as the developer is aware of the existence of an “invisible” argument. Now let’s try to reformat our first example with the help of the `%&gt;% ’operator. Exercise 9.2 - operator %&gt;% set.seed(1234) # solve the first example again using the %&gt;% operator set.seed(1234) runif(100, 0, 100) %&gt;% round(2) %&gt;% sample(10) %&gt;% mean() %&gt;% print() ## [1] 46.651 Note that by reading the above program code, we very easily interpret the meaning of that line of program code, especially when compared to the same command written in the form of a “sandwich”. We can store the end result of our “chain” of functions in the usual way: sum1to10 &lt;- 1:10 %&gt;% sum # result is stored in variable `sum1to10` but it may be more visually consistent to use the “inverted” assignment operator: -&gt;. 1:10 %&gt;% sum -&gt; sum1to10 # works the same as the example above Also note that in situations where the result of the previous function is the only parameter of the following function, we can completely leave out the parentheses (so in the examples above, sum,sum()orsum(.) would all work equally). Now let’s try to combine the %&gt;% operator and lapply with the example already seen in the section on functions in theapply family. Exercise 9.3 - lapply function and %&gt;% operator l &lt;- list(a = 1:10, b = 10:20, c = 100:200) # create a matrix which contains the first and last element of each list element # these elements must be in their own rows # use functions lapply, unlist and matrix as well as the %&gt;% operator # save the result in the `res` variable # print &#39;res&#39; l &lt;- list(a = 1:10, b = 10:20, c = 100:200) l %&gt;% lapply(function(x) c(x [1], x [length(x)])) %&gt;% unlist %&gt;% matrix(ncol = 2, byrow = T) -&gt; res res ## [,1] [,2] ## [1,] 1 10 ## [2,] 10 20 ## [3,] 100 200 The pipeline operator is very convenient in conjunction with “classic” functions, but we may encounter a problem when we want to combine it with other operators. The cause of the problem is the syntax - the pipeline operator achieves its efficiency by imposing a new, “sequential” syntax, which is not compatible with the syntax imposed by other operators, such as +, %% or [. If it is really important for us to have a “continuous” function call chain in our code that will contain not only functions but also other operators, then one solution is to use operators as “ordinary” functions. Namely, each operator is actually a function that shares a name with the operator (using the backtick quotation mark which allows the use of symbols as variable names), so the following pairs of expressions are actually equivalent: Example 9.1 - operators as functions # each pair of instructions is equivalent 2 + 3 `+`(2, 3) 1:5 `:`(1, 5) x &lt;- c(1, 2, 3) `&lt;-`(&quot;x&quot;, c(1,2,3)) x[1] `[`(x, 1) ## [1] 5 ## [1] 5 ## [1] 1 2 3 4 5 ## [1] 1 2 3 4 5 ## [1] 1 ## [1] 1 Let’s try to use this principle in the next exercise. Exercise 9.4 - combining pipe operator with other operators set.seed(1234) # &quot;clean up &quot; the following command with the help of the pipeline operator matrix(table(sample(round(sqrt(sample(1:10000, 10000, replace = T))), 100))[1:9], 3, 3) ## [,1] [,2] [,3] ## [1,] 2 2 2 ## [2,] 1 1 1 ## [3,] 2 2 2 set.seed(1234) # &quot;clean up &quot; the following command with the help of the pipeline operator 1:10000 %&gt;% sample(10000, replace = T) %&gt;% sqrt %&gt;% round %&gt;% sample(100) %&gt;% table %&gt;% `[`(1:9) %&gt;% matrix(3, 3) ## [,1] [,2] [,3] ## [1,] 2 2 2 ## [2,] 1 1 1 ## [3,] 2 2 2 The %&gt;% operator is particularly well suited when dealing with data frames, especially in scenarios when we have a defined data transformation procedure (e.g. filtering some rows, then selecting columns, then grouping the data according to a categorical variable etc.). With the help of this operator, the code itself acts as a readily interpretable representation of our data transformation process which we can easily customize and extend later. Future examples and exercises will often extensively rely on this operator, so we recommend that you master it well before proceeding with the lessons that follow. 9.2 Tidy data It is often stated in various literature that in data analysis, data preparation is often the most time consuming segment of the process. The book “Exploratory Data Mining and Data Cleaning” mentions that preparation often requires from 50% to 80% of the total time devoted to analysis. Also, as Hadley Wickham states in his article “Tidy Data”, data preparation is often not just the first step of the analysis, but rather a process that gets repeated as new knowledge is discovered or new data is collected. Hadley Wickham also introduced the term “tidy data” to refer to the organization of a data in such a way as to facilitate its further processing and analysis. In practice, datasets we get are often not originally intended for the purposes of analysis and as such are not organized in a way that would allow them to be easily used in the analytical process. “Tidy data” is actually a set of principles which may guides us how to “rearrange” data so that its structure matches the standard, expected metadata. Tidy data principles have similarities with the relational data model, but are defined in a way that is more appropriate for statisticians and developers. These principles can be summarized as follows: the data is organized into a table each line represents an observation each column represents a property or variable of that observation Since this may sound too trivial, let’s take a look at what properties Hadley lists as typical of “messy” data: column names are not variable names but rather their values multiple different variables are stored in the same column variables are saved in rows multiple types of different observations are stored in the same table one type of observation is stored in multiple tables Below, we will give some examples of tables that do not fully conform to the definition of tidy data, and show how to easily “tidy them up”. For this, we will leverage functions provided by the tidyr package. 9.2.1 The pivot_longer andpivot_wider functions The workbook which corresponds to this leeson should have a file called students.csv. Let’s load it into our work environment. Since the file is stored using UTF-8 encoding (since it contains Croatian letters), you can also add a fileEncoding =\"UTF-8\" parameter to the read.csv command to get the special characters correctly printed. Exercise 9.5 - students data set # load data from `students.csv` file into `students` variable # familiarize yourself with the data using standard functions for this purpose # (names, sapply/class, str, head, summary ...) # in the following examples for this process, we will use the phrase &quot;briefly explore the data frame&quot; # as a shortcut for the above process students &lt;- read.csv(&quot;students.csv&quot;, fileEncoding =&quot;UTF-8&quot;, stringsAsFactors = F) str(students) head(students) ## &#39;data.frame&#39;: 27 obs. of 10 variables: ## $ JMBAG : int 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 ... ## $ Surname : chr &quot;Anić&quot; &quot;Babić&quot; &quot;Crnoja&quot; &quot;Črnjac&quot; ... ## $ Name : chr &quot;Iva&quot; &quot;Josip&quot; &quot;Petra&quot; &quot;Lucija&quot; ... ## $ Math.1 : chr &quot;2&quot; &quot;5&quot; &quot;4&quot; &quot;2&quot; ... ## $ Physics.1 : chr &quot;2&quot; &quot;3&quot; &quot;3&quot; &quot;5&quot; ... ## $ Programming : chr &quot;NULL&quot; &quot;4&quot; &quot;4&quot; &quot;2&quot; ... ## $ Electronics : chr &quot;NULL&quot; &quot;3&quot; &quot;2&quot; &quot;2&quot; ... ## $ Digital.Logic: chr &quot;4&quot; &quot;NULL&quot; &quot;3&quot; &quot;3&quot; ... ## $ Math.2 : chr &quot;2&quot; &quot;5&quot; &quot;4&quot; &quot;3&quot; ... ## $ Algorithms.1 : chr &quot;2&quot; &quot;5&quot; &quot;3&quot; &quot;4&quot; ... ## JMBAG Surname Name Math.1 Physics.1 Programming Electronics Digital.Logic ## 1 1341 Anić Iva 2 2 NULL NULL 4 ## 2 1342 Babić Josip 5 3 4 3 NULL ## 3 1343 Crnoja Petra 4 3 4 2 3 ## 4 1344 Črnjac Lucija 2 5 2 2 3 ## 5 1345 Dizla Stipe NULL 4 3 5 2 ## 6 1346 Ermić Igor NULL 3 NULL 5 5 ## Math.2 Algorithms.1 ## 1 2 2 ## 2 5 5 ## 3 4 3 ## 4 3 4 ## 5 2 2 ## 6 5 5 Note that this dataset has a lot of missing values that are written as NULL. Because R does not recognize this as a missing value, it loaded the data as character strings (or as factors if we had the stringsAsFactors parameter set to TRUE). Because the columns that relate to the ratings are obviously numeric, we can easily convert them to such using the as.numeric () (or as.numeric(as.character()) command if they are factors!). But there is a simpler way - if we know how the missing value is represented in the dataset, we can directly embed it in the read.csv command using the na.strings parameter. Exercise 9.6 - using the na.strings parameter # reload the data from the `students.csv` file into the `students` variable # add the `na.strings` parameter to the `read.csv` command with a character string representing NA # briefly explore the `students` data frame students &lt;- read.csv (&quot;students.csv&quot;, fileEncoding = &quot;UTF-8&quot;, stringsAsFactors = F, na.strings = &quot;NULL&quot;) str(students) head(students) ## &#39;data.frame&#39;: 27 obs. of 10 variables: ## $ JMBAG : int 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 ... ## $ Surname : chr &quot;Anić&quot; &quot;Babić&quot; &quot;Crnoja&quot; &quot;Črnjac&quot; ... ## $ Name : chr &quot;Iva&quot; &quot;Josip&quot; &quot;Petra&quot; &quot;Lucija&quot; ... ## $ Math.1 : int 2 5 4 2 NA NA NA 3 3 4 ... ## $ Physics.1 : int 2 3 3 5 4 3 3 4 2 2 ... ## $ Programming : int NA 4 4 2 3 NA 3 3 3 4 ... ## $ Electronics : int NA 3 2 2 5 5 3 4 2 2 ... ## $ Digital.Logic: int 4 NA 3 3 2 5 2 4 3 5 ... ## $ Math.2 : int 2 5 4 3 2 5 2 5 NA 4 ... ## $ Algorithms.1 : int 2 5 3 4 2 5 5 4 3 2 ... ## JMBAG Surname Name Math.1 Physics.1 Programming Electronics Digital.Logic ## 1 1341 Anić Iva 2 2 NA NA 4 ## 2 1342 Babić Josip 5 3 4 3 NA ## 3 1343 Crnoja Petra 4 3 4 2 3 ## 4 1344 Črnjac Lucija 2 5 2 2 3 ## 5 1345 Dizla Stipe NA 4 3 5 2 ## 6 1346 Ermić Igor NA 3 NA 5 5 ## Math.2 Algorithms.1 ## 1 2 2 ## 2 5 5 ## 3 4 3 ## 4 3 4 ## 5 2 2 ## 6 5 5 We see that the columns are now of the appropriate type - but still the data frame does not fully fit the definition of “tidy data”. The names of the columns are actually the categories of the variable course and the ‘observation’ in this table is represented by the student, even though a more logical granularity would be to have each grade as a specific observation. Now, adding a new grade from a course is only possible by adding a new column, making sure to add grades for all students, which would inevitably mean immediately entering a lot of NA values for all student/course combinations where no grade yet exists (and maybe never will, if the student never takes that course at all). Let’s try to reorganize the table so each row represents “a grade student received in a particular course”. Consider what steps should be taken to create such a table. We need to: create a categorical variable Course which would have the names of all the courses (currently spread over column names) create all student/course combinations that make sense (i.e. have a grade assigned to them) fill in the combinations with the corresponding grade value This process is not impossible, but it does require a lot of jumping through hoops to redesign the data frame. To simplify this process, we can use the pivot_longer function from thetidyr package, which performs the exact procedure we described above: it “pivots” columns into a single variable and then populates the values of that variable with the existing column/row combinations, converting a table into a “longer” shape. The function signature looks like this (not all parameters are shown): pivot_longer(data, cols, names_to, values_to, values_drop_na) You can get a detailed description of the function as well as the parameters not shown above by calling ?pilot_longer, but here we will just briefly explain some of the parameters: data represents our data frame cols represents the set of columns we “pivot”; we can specify column names separated by commas (quotes are also not required), use the syntax first_column:last_column, or even just name the columns we do NOT want to gather by prefacing them with the - sign names_to represents the name of the new column - the categorical variable we want to create (in our case \"Course\"); value_to represents the name of the new column (variable) which will hold values with values (in our case Grade\") values_drop_na describes whether we want to omit observations with NA values Let’s try out this function with our untidy data frame. Exercise 9.7 - pivot_longer function # create a `grades` data frame by using the `pivot_longer` function on the `students` data frame # new columns should be called &quot;Course&quot; and &quot;Grade&quot; # briefly explore the `grades` data frame # create a `grades` data frame by using the `pivot_longer` function on the `students` data frame # new columns should be called &quot;Course&quot; and &quot;Grade&quot; #library(tidyr) # if necessary grades &lt;- pivot_longer(students, cols = Math.1:Algorithms.1, names_to = &quot;Course&quot;, values_to = &quot;Grade&quot;, values_drop_na = T) str(grades) head(grades) ## tibble [168 × 5] (S3: tbl_df/tbl/data.frame) ## $ JMBAG : int [1:168] 1341 1341 1341 1341 1341 1342 1342 1342 1342 1342 ... ## $ Surname: chr [1:168] &quot;Anić&quot; &quot;Anić&quot; &quot;Anić&quot; &quot;Anić&quot; ... ## $ Name : chr [1:168] &quot;Iva&quot; &quot;Iva&quot; &quot;Iva&quot; &quot;Iva&quot; ... ## $ Course : chr [1:168] &quot;Math.1&quot; &quot;Physics.1&quot; &quot;Digital.Logic&quot; &quot;Math.2&quot; ... ## $ Grade : int [1:168] 2 2 4 2 2 5 3 4 3 5 ... ## # A tibble: 6 × 5 ## JMBAG Surname Name Course Grade ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 1341 Anić Iva Math.1 2 ## 2 1341 Anić Iva Physics.1 2 ## 3 1341 Anić Iva Digital.Logic 4 ## 4 1341 Anić Iva Math.2 2 ## 5 1341 Anić Iva Algorithms.1 2 ## 6 1342 Babić Josip Math.1 5 The pivot_wider function does the inverse job from the pivot_longer function. It will “expand” data from a combination of a categorical column and a corresponding “value” column so the categories become column names, and the values from the value column get appropriately “spread” over the corresponding columns. The (abridged) function signature looks like this: pivot_wider(data, names_from, names_prefix = &quot;&quot;, values_from, values_fill = NULL) Complete documentation of this function is easily retrieved by using ?pivot_wider. The shortened list of parameters outlined above do the following: data is again out data frame names_from represents the name of the column whose values we will be pivoting into new columns (no need for quotes) names_prefix is an optional prefix we can add to new columns (useful if values of new columns are numbers, again no need for quotes) values_from represents the name of the column with values we will be filling the cells of new columns with values_fill is an optional value we can put when the values are missing; if left at NULL the values will be filled with NA Let’s try to “reconstruct” the original students data frame with this command. Exercise 9.8 - pivot_wider function # &quot;pivot&quot; the `grades` data frame into a wide format to reconstruct the # original data frame # store the results in a `students2` data frame # compare `students` and` students2` data frames # &quot;pivot&quot; the `grades` data frame into a wide format to reconstruct the # original data frame # store the results in a `students2` data frame students2 &lt;- pivot_wider(grades, names_from = Course, values_from = Grade) # compare `students` and `students2` data frames head(students) head(students2) str(students) str(students2) ## JMBAG Surname Name Math.1 Physics.1 Programming Electronics Digital.Logic ## 1 1341 Anić Iva 2 2 NA NA 4 ## 2 1342 Babić Josip 5 3 4 3 NA ## 3 1343 Crnoja Petra 4 3 4 2 3 ## 4 1344 Črnjac Lucija 2 5 2 2 3 ## 5 1345 Dizla Stipe NA 4 3 5 2 ## 6 1346 Ermić Igor NA 3 NA 5 5 ## Math.2 Algorithms.1 ## 1 2 2 ## 2 5 5 ## 3 4 3 ## 4 3 4 ## 5 2 2 ## 6 5 5 ## # A tibble: 6 × 10 ## JMBAG Surname Name Math.1 Physics.1 Digital.Logic Math.2 Algorithms.1 ## &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 1341 Anić Iva 2 2 4 2 2 ## 2 1342 Babić Josip 5 3 NA 5 5 ## 3 1343 Crnoja Petra 4 3 3 4 3 ## 4 1344 Črnjac Lucija 2 5 3 3 4 ## 5 1345 Dizla Stipe NA 4 2 2 2 ## 6 1346 Ermić Igor NA 3 5 5 5 ## # … with 2 more variables: Programming &lt;int&gt;, Electronics &lt;int&gt; ## &#39;data.frame&#39;: 27 obs. of 10 variables: ## $ JMBAG : int 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 ... ## $ Surname : chr &quot;Anić&quot; &quot;Babić&quot; &quot;Crnoja&quot; &quot;Črnjac&quot; ... ## $ Name : chr &quot;Iva&quot; &quot;Josip&quot; &quot;Petra&quot; &quot;Lucija&quot; ... ## $ Math.1 : int 2 5 4 2 NA NA NA 3 3 4 ... ## $ Physics.1 : int 2 3 3 5 4 3 3 4 2 2 ... ## $ Programming : int NA 4 4 2 3 NA 3 3 3 4 ... ## $ Electronics : int NA 3 2 2 5 5 3 4 2 2 ... ## $ Digital.Logic: int 4 NA 3 3 2 5 2 4 3 5 ... ## $ Math.2 : int 2 5 4 3 2 5 2 5 NA 4 ... ## $ Algorithms.1 : int 2 5 3 4 2 5 5 4 3 2 ... ## tibble [27 × 10] (S3: tbl_df/tbl/data.frame) ## $ JMBAG : int [1:27] 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 ... ## $ Surname : chr [1:27] &quot;Anić&quot; &quot;Babić&quot; &quot;Crnoja&quot; &quot;Črnjac&quot; ... ## $ Name : chr [1:27] &quot;Iva&quot; &quot;Josip&quot; &quot;Petra&quot; &quot;Lucija&quot; ... ## $ Math.1 : int [1:27] 2 5 4 2 NA NA NA 3 3 4 ... ## $ Physics.1 : int [1:27] 2 3 3 5 4 3 3 4 2 2 ... ## $ Digital.Logic: int [1:27] 4 NA 3 3 2 5 2 4 3 5 ... ## $ Math.2 : int [1:27] 2 5 4 3 2 5 2 5 NA 4 ... ## $ Algorithms.1 : int [1:27] 2 5 3 4 2 5 5 4 3 2 ... ## $ Programming : int [1:27] NA 4 4 2 3 NA 3 3 3 4 ... ## $ Electronics : int [1:27] NA 3 2 2 5 5 3 4 2 2 ... In the previous example, we demonstrated the inverse functionality of the pivot_longer and pivot_wider functions, but our usage pivot_wider did not serve to tidy up the data, it just enabled us to revert to the original data frame. Let us now look at an example where pivot_wider is sed to actually make the data tidier. Let’s load data from the cars.csv file that stores the technical characteristics of specific cars. Exercise 9.9 - cars data set #load the `cars.csv` file into a data frame called `cars` #briefly explore the `cars` data frame cars &lt;- read.csv(&quot;cars.csv&quot;, fileEncoding = &quot;UTF-8&quot;, stringsAsFactors = F) str(cars) head(cars) ## &#39;data.frame&#39;: 18 obs. of 3 variables: ## $ Car.model : chr &quot;Opel Astra&quot; &quot;Opel Astra&quot; &quot;Opel Astra&quot; &quot;Opel Astra&quot; ... ## $ Technical.Characteristic: chr &quot;Cylinders&quot; &quot;HP&quot; &quot;Length m&quot; &quot;Mass kg&quot; ... ## $ Value : num 4 125 4.27 1285 4 ... ## Car.model Technical.Characteristic Value ## 1 Opel Astra Cylinders 4.000 ## 2 Opel Astra HP 125.000 ## 3 Opel Astra Length m 4.267 ## 4 Opel Astra Mass kg 1285.000 ## 5 Audi A4 Cylinders 4.000 ## 6 Audi A4 HP 136.000 This table clearly violates the principles of tidy data which dictate that only one type of variable should be stored in a column - the technical characteristics of the car are placed in a unique column called Technical.Characteristic and in the value column holds very heterogeneous values (we have mass in kilograms, length in meters, etc.). Try tidying up this data frame with the pivot_wider function. Exercise 9.10 - pivot_wider function (2) # create an `cars2` data frame which will have # the tidied data from the `cars` data frame # briefly explore the `cars2` data frame # create an `cars2` data frame which will have # the tidied data from the `cars` data frame cars2 &lt;- pivot_wider(cars, names_from = Technical.Characteristic, values_from = Value) # briefly explore the `cars2` data frame str(cars2) head(cars2) ## tibble [5 × 6] (S3: tbl_df/tbl/data.frame) ## $ Car.model: chr [1:5] &quot;Opel Astra&quot; &quot;Audi A4&quot; &quot;Renault Grand Scenic&quot; &quot;Citroen C6&quot; ... ## $ Cylinders: num [1:5] 4 4 4 6 2 ## $ HP : num [1:5] 125 136 110 215 103 ## $ Length m : num [1:5] 4.27 4.7 4.56 NA NA ## $ Mass kg : num [1:5] 1285 1470 NA 1816 1260 ## $ Valves : num [1:5] NA NA 16 NA NA ## # A tibble: 5 × 6 ## Car.model Cylinders HP `Length m` `Mass kg` Valves ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Opel Astra 4 125 4.27 1285 NA ## 2 Audi A4 4 136 4.70 1470 NA ## 3 Renault Grand Scenic 4 110 4.56 NA 16 ## 4 Citroen C6 6 215 NA 1816 NA ## 5 Fiat 500L 2 103 NA 1260 NA The pivot_longer and pivot_wider commands are not only used for ‘messy’ data. Sometimes we will be pivoting our tables because one or other format is better suited for the analysis methods we wont to employ on the data. Let’s show this on the example of recommender systems Shopping cart data (or “consumer basket data”) is a record of items purchased by a customer during their visit to the store (be it a virtual store or a real retail outlet). If we write consumer basket information in a “wide” or “matrix” format, then we organize the information so that the columns represent individual items and the rows represent one successful “visit” to the story (or simply “invoice”). Here, a value of 1 means that the item ultimately ended up in the cart, while 0 means it was not present there (alternative way would be to use numbers over 1 to denote the exact quantity). This kind of representation is suitable for some recommender system implementations, because then the choice of which items to recommend can boil down to finding a “row vector” which is the closest to the one representing the current shopping cart. On the other hand, this type of representation is very uneconomical - if the store has a large number of items on offer, data will often have long rows mostly filled with “zeros”. Alternatively, the “long” format simply puts a combination of a shopping cart identifier (or account number) and the name (or code) of the item purchased in each row. This dataset will naturally have significantly more rows, but will still be much more suitable for storage purposes in cases where the number of items in the range is far greater than the number of items in the average basket. Exercise 9.11 - Consumer basket data # Load data from the `ConsumersBasket.csv` file into the `invoices` data frame # briefly explore the `invoices` data frame invoices &lt;- read.csv(&quot;ConsumerBasket.csv&quot;, stringsAsFactors = F, encoding = &quot;UTF-8&quot;) str(invoices) head(invoices) ## &#39;data.frame&#39;: 104 obs. of 21 variables: ## $ invoiceID : int 15671 15672 15673 15674 15675 15676 15677 15678 15679 15680 ... ## $ Coca.cola.2l : int 0 1 1 0 1 0 0 0 0 0 ... ## $ Chips.150g : int 0 0 1 0 0 0 0 1 0 0 ... ## $ Nutella.400.g : int 0 1 0 0 1 0 0 0 0 0 ... ## $ Light.Beer : int 0 1 0 0 0 0 0 0 0 1 ... ## $ Dark.Beer : int 1 0 0 1 0 0 0 0 0 1 ... ## $ Fabric.Softener.1.5l: int 0 0 0 0 0 0 0 1 0 1 ... ## $ Water.2l : int 0 0 1 0 0 0 0 0 1 1 ... ## $ Oranges : int 0 0 0 1 1 0 0 0 1 1 ... ## $ Apples : int 1 0 0 1 0 0 0 0 0 0 ... ## $ Pineapples : int 0 0 1 0 0 0 0 1 1 1 ... ## $ Napkins : int 0 0 0 0 1 1 0 0 0 0 ... ## $ Patte : int 0 0 1 1 0 0 0 0 0 1 ... ## $ Ketchup : int 1 0 0 1 0 0 0 0 0 1 ... ## $ Mustard : int 1 0 0 0 0 0 0 1 0 0 ... ## $ Milk.0.5l : int 0 1 0 1 1 0 0 0 0 0 ... ## $ Sour.Cream : int 0 1 0 0 0 0 1 0 0 1 ... ## $ Feta.cheese : int 0 0 0 1 0 0 0 0 0 0 ... ## $ Sardines : int 0 0 0 0 0 0 0 0 0 1 ... ## $ Tuna.patte : int 1 1 1 0 0 1 0 0 0 0 ... ## $ Nescaffe : int 0 0 1 0 0 0 0 0 1 0 ... ## invoiceID Coca.cola.2l Chips.150g Nutella.400.g Light.Beer Dark.Beer ## 1 15671 0 0 0 0 1 ## 2 15672 1 0 1 1 0 ## 3 15673 1 1 0 0 0 ## 4 15674 0 0 0 0 1 ## 5 15675 1 0 1 0 0 ## 6 15676 0 0 0 0 0 ## Fabric.Softener.1.5l Water.2l Oranges Apples Pineapples Napkins Patte Ketchup ## 1 0 0 0 1 0 0 0 1 ## 2 0 0 0 0 0 0 0 0 ## 3 0 1 0 0 1 0 1 0 ## 4 0 0 1 1 0 0 1 1 ## 5 0 0 1 0 0 1 0 0 ## 6 0 0 0 0 0 1 0 0 ## Mustard Milk.0.5l Sour.Cream Feta.cheese Sardines Tuna.patte Nescaffe ## 1 1 0 0 0 0 1 0 ## 2 0 1 1 0 0 1 0 ## 3 0 0 0 0 0 1 1 ## 4 0 1 0 1 0 0 0 ## 5 0 1 0 0 0 0 0 ## 6 0 0 0 0 0 1 0 Exercise 9.12 - converting data frame to a ‘long’ format # convert `invoices` data frame to a &quot;long&quot; format # each row needs to have invoiceID and itemName # only bought items need to be present # name the new data frame `invoicesLong` # store the new data frame in a new CSV file # called &quot;ConsumerBasketLong.csv&quot; # convert `invoices` data frame to a &quot;long&quot; format # each row needs to have invoiceID and itemName # only bought items need to be present # name the new data frame `invoicesLong` invoicesLong &lt;- pivot_longer(invoices, cols = -invoiceID, names_to = &quot;itemName&quot;, values_to = &quot;value&quot;) invoicesLong &lt;- invoicesLong[invoicesLong$value != 0, 1:2] invoicesLong &lt;- invoicesLong[order(invoicesLong$invoiceID),] head(invoicesLong) # store the new data frame in a new CSV file # called &quot;ConsumerBasketLong.csv&quot; write.csv(invoicesLong, file = &#39;ConsumerBasketLong.csv&#39;, row.names = F) ## # A tibble: 6 × 2 ## invoiceID itemName ## &lt;int&gt; &lt;chr&gt; ## 1 15671 Dark.Beer ## 2 15671 Apples ## 3 15671 Ketchup ## 4 15671 Mustard ## 5 15671 Tuna.patte ## 6 15672 Coca.cola.2l Exercise 9.13 - converting data frame to a ‘wide’ format # try formatting the &quot;long&quot; format back to &quot;wide&quot; # store the new data frame in a new CSV file # called &quot;ConsumerBasketWide.csv&quot; invoicesWide &lt;- invoicesLong invoicesWide$value &lt;- 1 invoicesWide &lt;- pivot_wider(invoicesWide, names_from = itemName, values_from = value, values_fill = 0) head(invoicesWide) write.csv(invoicesWide, file = &#39;ConsumerBasketWide.csv&#39;, row.names = F) ## # A tibble: 6 × 21 ## invoiceID Dark.Beer Apples Ketchup Mustard Tuna.patte Coca.cola.2l ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15671 1 1 1 1 1 0 ## 2 15672 0 0 0 0 1 1 ## 3 15673 0 0 0 0 1 1 ## 4 15674 1 1 1 0 0 0 ## 5 15675 0 0 0 0 0 1 ## 6 15676 0 0 0 0 1 0 ## # … with 14 more variables: Nutella.400.g &lt;dbl&gt;, Light.Beer &lt;dbl&gt;, ## # Milk.0.5l &lt;dbl&gt;, Sour.Cream &lt;dbl&gt;, Chips.150g &lt;dbl&gt;, Water.2l &lt;dbl&gt;, ## # Pineapples &lt;dbl&gt;, Patte &lt;dbl&gt;, Nescaffe &lt;dbl&gt;, Oranges &lt;dbl&gt;, ## # Feta.cheese &lt;dbl&gt;, Napkins &lt;dbl&gt;, Fabric.Softener.1.5l &lt;dbl&gt;, ## # Sardines &lt;dbl&gt; 9.2.2 The separate and unite functions The tidyr package has a number of other useful features for efficient transformation and clean up of our data frames, and here we will address two more commonly used onesm called separate and unite. The separate function is useful when a column has “complex” values that we want to separate into two or more columns. Exercise 9.14 - Deparments data set # read data from the `departments.csv` file into the` departments` variable # briefly explore the `departments` data frame departments &lt;- read.csv(&quot;departments.csv&quot;, stringsAsFactors = F) str(departments) head(departments) ## &#39;data.frame&#39;: 28 obs. of 4 variables: ## $ Department: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Quarter : chr &quot;Q1-2015&quot; &quot;Q2-2015&quot; &quot;Q3-2015&quot; &quot;Q4-2015&quot; ... ## $ RevenuesKn: num 12416 224290 10644 191229 258697 ... ## $ ExpensesKn: num 23101 63886 35468 12249 61515 ... ## Department Quarter RevenuesKn ExpensesKn ## 1 A Q1-2015 12416.2 23100.5 ## 2 A Q2-2015 224290.1 63886.1 ## 3 A Q3-2015 10643.7 35467.8 ## 4 A Q4-2015 191229.3 12249.1 ## 5 A Q1-2016 258697.4 61514.6 ## 6 A Q2-2016 121865.3 46092.6 This table shows the revenues and expenditures of a company departments by quarter. Quarters are currently stored in a complex variable called Quarter consisting of identifiers of the annual quarter (Q1, Q2, Q3 or Q4) and year. For analysis purposes, it would probably be more convenient to break this down into two columns - Quarter (which would store only the quarter identifier) and Year. The tidyr package for this purpose offers a separate function with the following signature: separate(data, col, into, sep = &quot;[^[:alnum:]] +&quot;, remove = TRUE, convert = FALSE, extra = &quot;warn&quot;, fill = &quot;warn&quot;, ...) The complete documentation of the function can be viewed with the command ?separate while here we explain some important parameters: col - column to be separated (no quotes required) into - names of new columns (character vector is recommended) sep - value separator in original column, default value is actually a regular expression for “something that is not an alphanumeric character” remove - describes whether or not to remove the original column Let’s try to apply this function to the departments data frame. Exercise 9.15 - the separate function # Separate the `Quarter` column into the `Quarter` and `Year` columns while removing the original column # save the resulting data frame to the `departments2` variable # try to do everything within one command with the help of the `%&gt;%` operator # briefly explore the `departments2` data frame departments %&gt;% separate(Quarter, c(&quot;Quarter&quot;, &quot;Year&quot;), &quot;-&quot;) -&gt; departments2 str(departments2) head(departments2) ## &#39;data.frame&#39;: 28 obs. of 5 variables: ## $ Department: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Quarter : chr &quot;Q1&quot; &quot;Q2&quot; &quot;Q3&quot; &quot;Q4&quot; ... ## $ Year : chr &quot;2015&quot; &quot;2015&quot; &quot;2015&quot; &quot;2015&quot; ... ## $ RevenuesKn: num 12416 224290 10644 191229 258697 ... ## $ ExpensesKn: num 23101 63886 35468 12249 61515 ... ## Department Quarter Year RevenuesKn ExpensesKn ## 1 A Q1 2015 12416.2 23100.5 ## 2 A Q2 2015 224290.1 63886.1 ## 3 A Q3 2015 10643.7 35467.8 ## 4 A Q4 2015 191229.3 12249.1 ## 5 A Q1 2016 258697.4 61514.6 ## 6 A Q2 2016 121865.3 46092.6 Note that the Quarter andYear columns are actually categorical variables so it would probably be a good idea to factorize them if we are to use them in further analysis. The separate function is often used to disassemble dates (eg 2016-10-28 into the year, month and day), but in such situations it is recommended to use the lubridate package created precisely for easier date management. We will introduce this package in one of the following chapters. Finally, let’s learn the unite function, which is somewhat less commonly used and is actually an inverse of the separate function. The unite function signature is: unite(data, col, ..., sep = &quot;_&quot;, remove = TRUE) In this case, too, we can easily retrieve the documentation for unite, and will just briefly describe the parameters that potentially require additional explanation: col - new column name (quotes not required) ... - the names of the columns we merge - we don’t have to use quotation marks, and if there are many columns we can use the same syntax to select as with the gather function Let’s try using this function on the departments2 data frame. Exercise 9.16 - the unite function # merge the `Quarter` and `Year` columns from the `departments2` table into a unique` Quarter` column # remove the previous `Quarter` and `Year` columns # use `-` as a separator # save the result to the `departments3` variable # use the `%&gt;% &#39;operator to put everything in one line # compare `departments` and `departments3` data frames departments2 %&gt;% unite(Quarter, Quarter, Year, sep = &quot;-&quot;) -&gt; departments3 str(departments2) head(departments2) str(departments3) head(departments3) ## &#39;data.frame&#39;: 28 obs. of 5 variables: ## $ Department: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Quarter : chr &quot;Q1&quot; &quot;Q2&quot; &quot;Q3&quot; &quot;Q4&quot; ... ## $ Year : chr &quot;2015&quot; &quot;2015&quot; &quot;2015&quot; &quot;2015&quot; ... ## $ RevenuesKn: num 12416 224290 10644 191229 258697 ... ## $ ExpensesKn: num 23101 63886 35468 12249 61515 ... ## Department Quarter Year RevenuesKn ExpensesKn ## 1 A Q1 2015 12416.2 23100.5 ## 2 A Q2 2015 224290.1 63886.1 ## 3 A Q3 2015 10643.7 35467.8 ## 4 A Q4 2015 191229.3 12249.1 ## 5 A Q1 2016 258697.4 61514.6 ## 6 A Q2 2016 121865.3 46092.6 ## &#39;data.frame&#39;: 28 obs. of 4 variables: ## $ Department: chr &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; ... ## $ Quarter : chr &quot;Q1-2015&quot; &quot;Q2-2015&quot; &quot;Q3-2015&quot; &quot;Q4-2015&quot; ... ## $ RevenuesKn: num 12416 224290 10644 191229 258697 ... ## $ ExpensesKn: num 23101 63886 35468 12249 61515 ... ## Department Quarter RevenuesKn ExpensesKn ## 1 A Q1-2015 12416.2 23100.5 ## 2 A Q2-2015 224290.1 63886.1 ## 3 A Q3-2015 10643.7 35467.8 ## 4 A Q4-2015 191229.3 12249.1 ## 5 A Q1-2016 258697.4 61514.6 ## 6 A Q2-2016 121865.3 46092.6 Homework exercises Initialize the random number generator using the command set.seed(1234). Then, with the help of a single command and the `%&gt;% ’operator, perform the following: create 100,000 random numbers drawn from the normal distribution with arithmetic mean of 10000 and standard deviation of 1000 round the numbers to the first larger integer drop duplicates from the set sort the set in ascending order randomly select 100 elements from the set organize these 100 elements into a 10x10 matrix, arranged in rows calculate sums of rows of the matrix print the mean of line sums on the screen. The weather.csv file contains meteorological station meteorological data which measures the temperature, pressure, humidity and wind speed every hour (data are downloaded and adapted from the data set of theweatherData package available on CRAN). Do the following: load the file into the data frame and examine the loaded data (names, str, summary,head …) answer: is it a tidy dataset? Why? Take the appropriate steps to obtain a data frame that complies with the tidy data principle save the new data frame to the file called weatherClean.csv Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ References "],["dates.html", "10 Working with dates and character strings 10.1 Working with dates 10.2 The lubridate package 10.3 Working with character strings Homework exercises", " 10 Working with dates and character strings 10.1 Working with dates Managing dates and timestamps is always a challenge when working with real-life datasets, since we have to take into account things like: different date and time format specifications different internal representations different time zones difference between mathematical and calendar understanding of time periods One of the more commonly used standards is the so-called “unix time” (or “POSIX time”), which counts as the number of seconds elapsed since midnight on January 1, 1970 UTC (Coordinated Universal Time). This is not an universal standard for all operations systems or applications; for example, Microsoft Excel has its own format where it counts the days since 1.1.1900 and then the number of hours, minutes and seconds elapsed since midnight. The R programming language has offers three main classes for date/time management: Date for dates POSIXct for compact representation of timestamps POSIXlt for “long” representation of timestamps (in list format) 10.1.1 The Date class We use the Date class when we want to note the date but not the exact time of an observation or event. This class does not have a constructor, and we most commonly create Date objects using the following functions: Sys.Date() which returns today’s date as.Date(&lt;date_string&gt;) which accepts a character string as a parameter The function as.Date() by default accepts dates in the following format: %Y-%m-%d. Here, %Y represents a four-digit year, while %m and %d are two-digit months and days. If we want to interpret a date that is written in another format then we need to add an additional parameter called format which will parametrically describe the format we are using (e.g. for 28/10/1978 the format parameter value should be %d/%m/%Y). Other format specifications can easily be viewed using the ?strptime command, although a much simpler approach entails using functions of thelubridate package (to be introduced later in this chapter). Exercise 10.1 - class Date # print out today&#39;s date # convert the following character strings to a `Date` type object and print the result on the screen: # &#39;1986-12-27&#39;, &#39;2016-31-05&#39;, &#39;17.10.2015 &#39;, &#39;01#01#2001&#39; # print out today&#39;s date Sys.Date() # convert the following character strings to a Date type object and print the result on the screen: # &#39;1986-12-27&#39;, &#39;2016-31-05&#39;, &#39;17.10.2015 &#39;, &#39;01#01#2001&#39; as.Date(&quot;27/12/1986&quot;) as.Date(&#39;2016-31-05&#39;, format = &#39;%Y-%d-%m&#39;) as.Date(&#39;17.10.2015&#39;, format = &#39;%d.%m.%Y&#39;) as.Date(&#39;01#01#2001&#39;, format = &#39;%d#%m#%Y&#39;) ## [1] &quot;2023-01-16&quot; ## [1] &quot;0027-12-19&quot; ## [1] &quot;2016-05-31&quot; ## [1] &quot;2015-10-17&quot; ## [1] &quot;2001-01-01&quot; Dates allow using simple arithmetic operations. We can add or substract days from a date by using operators + and - with integers, or we can calculate the difference between two dates with the operator -. *** Exercise 10.2 - date arithmetics # print out what date was 1000 days before today # add one day to 2/28/2015. and 2/28/2016 and print the result # print out how many days have passed since 1.1.2000. until today # print out what date was 1000 days before today Sys.Date() - 1000 # add one day to 2/28/2015. and 2/28/2016 and print the result as.Date(&#39;2015-02-28&#39;) + 1 as.Date(&#39;2016-02-28&#39;) + 1 # print out how many days have passed since 1.1.2000. until today Sys.Date() - as.Date(&#39;2000-01-01&#39;) ## [1] &quot;2020-04-21&quot; ## [1] &quot;2015-03-01&quot; ## [1] &quot;2016-02-29&quot; ## Time difference of 8416 days The last expression will actually result in an object of class difftime which is an object representation of a time interval. Printing this object uses so-called “automatic” unit selection (specifically, the units parameter is by default set to auto) that will attempt to select the most appropriate time unit for printing. If we want to explicitly choose which time unit we want (seconds, minutes, hours, days or weeks), then it’s easier to forgo using the - operator and leverage the difftime() function directly. Specific time unit can then be chosen by setting the units parameter to seconds, minutes etc. Exercise 10.3 - function difftime # How many weeks passed between 1.3.2016. i 1.3.2015.? # use the `difftime` function # NOTE: You do not need to explicitly call the `as.Date` function, the `difftime()` function # will do it by itself if you submit the date in the default format # how many hours have passed between 1.3.2015. and today? # How many weeks passed between 1.3.2016. i 1.3.2015.? # use the `difftime` function # NOTE: You do not need to explicitly call the `as.Date` function, the `difftime()` function # will do it by itself if you submit the date in the default format difftime(&#39;2016-03-01&#39;, &#39;2015-03-01&#39;, units = &quot;weeks&quot;) # how many hours have passed between 1.3.2015. and today? difftime(Sys.Date(), &#39;2015-03-01&#39;, units = &quot;hours&quot;) ## Time difference of 52.28571 weeks ## Time difference of 69073 hours The difftime function actually works with timestamps as well, i.e. we don’t necessarily have to work at the date level, we can get information on finer granularity when it comes to time units. We will show this in more detail after we learn the class POSIXct. Also, if we only require a number (of seconds, hours, days, etc.), we can easily transform the difftime output into an integer using the as.numeric function. The R language also implements a special variant of the seq function for working with dates, which has the following signature: seq(from, to, by, length.out = NULL, along.with = NULL, ...) The parameters of this function are as follows: from - start date (required parameter) to - final date by - step of the sequence in days, or a character string which denotes the time interval such as \"7 days\", \"2 weeks\" etc. (for all possibilities see the documentation!) length.out - length of sequence along.with - vector whose length we take for reference (along.with(x) is the same as length.out = length(x)) Let’s try this function. Exercise 10.4 - function seq and dates # print the date sequence from 1/1/2010. to 1/1/2030. in 6-month increments # make a schedule for one round of cleaning the common areas of an apartment building # common areas must be cleaned every 3 weeks # each apartment must get their own date # apartments are described by the following data frame apartments &lt;- data.frame(appId = 1:10, surname = c(&quot;Ebert&quot;, &quot;Ladovac&quot;, &quot;Cerić&quot;, &quot;Dikla&quot;, &quot;Anic&quot;, &quot;Perić&quot;, &quot;Žužić&quot;, &quot;Babić&quot;, &quot;Ibiza&quot;, &quot;Radler&quot;)) # Add a `cleaningDate` column with one date for each apartment # in order of apartment numbers, starting today # print out the &#39;apartments&#39; data frame # print the date sequence from 1/1/2010. to 1/1/2030. in 6-month increments seq(as.Date(&#39;2010-01-01&#39;), as.Date(&#39;2030-01-01&#39;), by = &quot;6 months&quot;) # Add a `cleaningDate` column with one date for each apartment # in order of apartment numbers, starting today seq(Sys.Date(), by = &quot;3 weeks&quot;, along.with = apartments$appId) -&gt; apartments$cleaningDate # print out the &#39;apartments&#39; data frame apartments ## [1] &quot;2010-01-01&quot; &quot;2010-07-01&quot; &quot;2011-01-01&quot; &quot;2011-07-01&quot; &quot;2012-01-01&quot; ## [6] &quot;2012-07-01&quot; &quot;2013-01-01&quot; &quot;2013-07-01&quot; &quot;2014-01-01&quot; &quot;2014-07-01&quot; ## [11] &quot;2015-01-01&quot; &quot;2015-07-01&quot; &quot;2016-01-01&quot; &quot;2016-07-01&quot; &quot;2017-01-01&quot; ## [16] &quot;2017-07-01&quot; &quot;2018-01-01&quot; &quot;2018-07-01&quot; &quot;2019-01-01&quot; &quot;2019-07-01&quot; ## [21] &quot;2020-01-01&quot; &quot;2020-07-01&quot; &quot;2021-01-01&quot; &quot;2021-07-01&quot; &quot;2022-01-01&quot; ## [26] &quot;2022-07-01&quot; &quot;2023-01-01&quot; &quot;2023-07-01&quot; &quot;2024-01-01&quot; &quot;2024-07-01&quot; ## [31] &quot;2025-01-01&quot; &quot;2025-07-01&quot; &quot;2026-01-01&quot; &quot;2026-07-01&quot; &quot;2027-01-01&quot; ## [36] &quot;2027-07-01&quot; &quot;2028-01-01&quot; &quot;2028-07-01&quot; &quot;2029-01-01&quot; &quot;2029-07-01&quot; ## [41] &quot;2030-01-01&quot; ## appId surname cleaningDate ## 1 1 Ebert 2023-01-16 ## 2 2 Ladovac 2023-02-06 ## 3 3 Cerić 2023-02-27 ## 4 4 Dikla 2023-03-20 ## 5 5 Anic 2023-04-10 ## 6 6 Perić 2023-05-01 ## 7 7 Žužić 2023-05-22 ## 8 8 Babić 2023-06-12 ## 9 9 Ibiza 2023-07-03 ## 10 10 Radler 2023-07-24 10.1.2 ThePOSIXct andPOSIXlt classes The class POSIXct is used when in addition to the date we also need to know the exact time for some observation or event. The usual way of creating an object of this class is using the following functions: Sys.time() which returns the current timestamp (using the timezone set by the operating system) as.POSIXct(&lt;timestamp_string&gt;) which uses a character string representing the date and time The function as.POSIXct() expects a timestamp character string that uses the following format: %Y-%m-%d %H:%M:%S. First three format specifications are identical to the date format specification, while %H, %M and %S represent two-digit hours, minutes and seconds (24-hour time format is assumed unless am or pm is noted). The as.POSIXct function can also accept an optional tz parameter which explicitly sets the timezone. To parse other timestamp formats, it is necessary - as with the class Date - to add a format parameter containing a specification on how to interpret the given character string. Again, for the list of all parameters we can refer to the the ?strptime command, although as with the Date class we will later learn it’s often easier to use one of the functions of the lubridate package. Exercise 10.5 - class POSIXct # print the current date and time # convert the following character strings to timestamps and print them on the screen: # &quot;2015-10-28 15:30:42&quot; # &quot;01-12-2001 14:30&quot; &lt;- timestamp read in New York, USA, EST time zone # print the current date and time Sys.time() # convert the following character strings to timestamps and print them on the screen: # &quot;2015-10-28 15:30:42&quot; # &quot;01-12-2001 14:30&quot; &lt;- timestamp read in New York, USA, EST time zone as.POSIXct(&quot;2015-10-28 15:30:42&quot;) as.POSIXct(&quot;01-12-2001 14:30&quot;, tz = &quot;EST&quot;, format = &quot;%d-%m-%Y %H:%M&quot;) ## [1] &quot;2023-01-16 13:33:45 CET&quot; ## [1] &quot;2015-10-28 15:30:42 CET&quot; ## [1] &quot;2001-12-01 14:30:00 EST&quot; Time zone names are standardized (so-called “Olson time zones”) and are retrieved from the underlying operating system. We can explicitly retrieve them using the OlsonNames() function. Also, we can easily print the current platform’s time zone by using the Sys.timezone() function. Exercise 10.6 - time zones # print the current time zone # print out 10 randomly selected time zone names installed on the current platform # print the current time zone Sys.timezone() # print out 10 randomly selected time zone names installed on the current platform sample(OlsonNames(), 10) ## [1] &quot;Europe/Warsaw&quot; ## [1] &quot;Europe/Uzhgorod&quot; &quot;Etc/GMT+11&quot; &quot;US/Hawaii&quot; ## [4] &quot;America/Regina&quot; &quot;GMT&quot; &quot;America/Araguaina&quot; ## [7] &quot;America/Phoenix&quot; &quot;Europe/Tiraspol&quot; &quot;Asia/Macao&quot; ## [10] &quot;America/Matamoros&quot; Timestamps can also use + and - operators (with integers as the second operand) to add or substract seconds from the timestamp. Additionally, we can subtract two timestamps to get the difference in seconds, or use the difftime function with the selected time unit value. Exercise 10.7 - timestamp arithmetics # print out what the time will be 1000 seconds from now # print out how many hours have passed since midnight 1/1/2015 # print out what the time will be 1000 seconds from now Sys.time() + 1000 # print out how many hours have passed since midnight 1/1/2015. by far difftime(Sys.time(), &quot;2015-01-01 00:00:00&quot;, units = &quot;hours&quot;) ## [1] &quot;2023-01-16 13:50:25 CET&quot; ## Time difference of 70501.56 hours The class POSIXlt is very similar to its compact relative POSIXct. We use the similarly fashioned as.POSIXlt function to create one, however the difference is that we end up with a list which allows us to easily extract certain parameters from a timestamp, such as number of seconds, minutes, day of the week, etc. We can easily see all the elements of the list if we create a POSIXlt object and then call the unclass function on it, which will convert it to an “ordinary” list. We can even go a step further - if we put this list through the unlist function, we get a simple, easily interpretable character vector as a result. Exercise 10.8 - class POSIXlt # convert the following character string to a timestamp of type `POSIXlt` # store the result in the variable `t_long` # &quot;01/05/2013 13:35&quot; # print just the number of hours of the timestamp `t_long` # then just the number of minutes # you can find this information in subelements called `hour` and `minute` # remove the class and list property of the variable `t_long` # and print it on the screen # convert the following character string to a timestamp of type `POSIXlt` # store the result in the variable `t_long` # &quot;01/05/2013 13:35&quot; t_long &lt;- as.POSIXlt(&quot;01/05/2013 13:35&quot;, format = &quot;%d/%m/%Y %H:%M&quot;) # print just the number of hours of the timestamp `t_long` # then just the number of minutes # you can find this information in subelements called `hour` and `minute` t_long$hour t_long$min # remove the class and list property of the variable `t_long` # and print it on the screen t_long %&gt;% unclass() %&gt;% unlist() ## [1] 13 ## [1] 35 ## sec min hour mday mon year wday yday isdst zone gmtoff ## &quot;0&quot; &quot;35&quot; &quot;13&quot; &quot;1&quot; &quot;4&quot; &quot;113&quot; &quot;3&quot; &quot;120&quot; &quot;1&quot; &quot;CEST&quot; NA 10.2 The lubridate package Although the R language has relatively good support for working with dates and timestamps, we can make managing them significantly more efficient by using a package called lubridate package. If we analyze data where the time component is very important, or manage data sets that use exotic ways of noting date and time, then we can greatly simplify and accelerate the analysis process by using the features provided by this package. One of the things that may be most helpful to developers who don’t like to write formatted date parsing specifications is the family of date parsing functions whose names match the overall “structure” of the record we want to parse. For example, the function called ymd can parse character strings in which the date is written in the order year-month-day. The function is “smart” enough to interpret the details of the record itself, such as delimiters, character fields, etc. If the record has a different day, month, and year layout, it is only necessary to arrange the letters appropriately in the function name. Exercise 10.9 - date parsing functions of the lubridate package # library(lubridate) # load if needed! # using the functions in the `lubridate` package # parse the following character strings into dates and print them on the screen # &quot;2016-07-31&quot; # &quot;28/2/1983&quot; # &quot;07#31#1996&quot; # &quot;20010830&quot; # using the functions in the `lubridate` package # parse the following character strings into dates and print them on the screen # &quot;2016-07-31&quot; # &quot;28/2/1983&quot; # &quot;07#31#1996&quot; # &quot;20010830&quot; ymd(&quot;2016-07-31&quot;) dmy(&quot;28/2/1983&quot;) mdy(&quot;07#31#1996&quot;) ymd(&quot;20010830&quot;) ## [1] &quot;2016-07-31&quot; ## [1] &quot;1983-02-28&quot; ## [1] &quot;1996-07-31&quot; ## [1] &quot;2001-08-30&quot; The aforementioned approach can also be used for timestamps, we just add the underscore and then the “letters” for hours, minutes and/or seconds (e.g. ymd_hms). Exercise 10.10 - timestamp parsing functions of the lubridate package # using the functions in the `lubridate` package # parse the following character strings to timestamps and print them on the screen # &quot;17/05/1977 10:15 pm&quot; # &quot;20160429 05:10:17&quot; # using the functions in the `lubridate` package # parse the following character strings to timestamps and print them on the screen # &quot;17/05/1977 10:15 pm&quot; # &quot;20160429 05/10/17&quot; dmy_hm(&quot;17/05/1977 10:15 pm&quot;) ymd_hms(&quot;20160429 05:10:17&quot;) ## [1] &quot;1977-05-17 22:15:00 UTC&quot; ## [1] &quot;2016-04-29 05:10:17 UTC&quot; Note that these functions always sets UTC for the time zone. This was intentionally designed to motivate the use of a consistent time zone in the data set we are analyzing. If desired, we can set the time zone with the tz parameter during parsing. Similarly, with timestamps already initialized, we can manage time zones using the following functions force_tz - “enforces” the change to a new time zone while leaving the timestamp values the same with_tz - converts a timestamp into one that matches the requested time zone t &lt;- ymd_hms(&quot;20161129 05:10:17&quot;, tz = &quot;EST&quot;) t force_tz(t, tz = &quot;CET&quot;) with_tz(t, tz = &quot;CET&quot;) ## [1] &quot;2016-11-29 05:10:17 EST&quot; ## [1] &quot;2016-11-29 05:10:17 CET&quot; ## [1] &quot;2016-11-29 11:10:17 CET&quot; The lubridate package also makes it easy to extract date and time segments from timestamps with functions such as year, week, month, etc. The same functions can also be used to change any of the timestamp components. Exercise 10.11 - extracting timestamp elements x &lt;- dmy_hms(&quot;7/19/1996 4:15:27 PM&quot;) ## Warning: All formats failed to parse. No formats found. # extract and print hours from the timestamp above, followed by minutes # set the year of the above timestamp to 2011, and the month to June # print timestamp `x` on the screen x &lt;- dmy_hms(&quot;7/19/1996 4:15:27 PM&quot;) ## Warning: All formats failed to parse. No formats found. # extract and print hours from the timestamp above, followed by minutes hour(x) minute(x) # set the year of the above timestamp to 2011, and the month to June year(x) &lt;- 2011 month(x) &lt;- 6 # print timestamp `x` on the screen x ## [1] NA ## [1] NA ## [1] NA See the lubridate documentation for a complete list of features. For the current date and time, lubridate offers alternatives to the Sys.Date() and Sys.time() functions, which are simply called today() and now(). Exercise 10.12 - functions today and now # print out tomorrow&#39;s date # print the timestamp which happened exactly an hour ago # print out tomorrow&#39;s date today() + 1 # print the timestamp which happened exactly an hour ago now() - 60 * 60 ## [1] &quot;2023-01-17&quot; ## [1] &quot;2023-01-16 12:33:46 CET&quot; We already mentioned that managing time-related data can become very complex, especially considering that time intervals can be expressed generically (e.g., “2 years”) or specifically (the span of two dates), and that mathematical and calendar ways of expressing time intervals do not necessarily match (e.g., “period of one year” can mathematically mean “the exact number of seconds in 365 days” but also the contextually dependent “period until the same date next year”). The lubridate package defines four options for defining time-related objects: instant - timestamp rounded to the second duration - “generically” defined interval in seconds period - similar to duration, but allows you to define durations that do not always last mathematically (e.g. “3 months”) interval - the time interval between two specific moments We have already met the “istants”, these are the timestamps we have already shown. To create durations and periods, we have intuitively defined functions that are named by English names for time units, with durations having the letter d as a prefix (from duration). Hence, we have the functions called minutes/dminutes, hours/dhours, weeks/dweeks etc. (note that there is no dmonths function, since we cannot unambiguously convert one month into seconds). Exercise 10.13 - durations and periods # print out objects which represent the duration and period of 3 weeks # use the variable `v` to store a period of 5 years, 3 months and 2 days # add the above period to today&#39;s date # print out objects which represent the duration and period of 3 weeks weeks(3) dweeks(3) # use the variable `v` to store a period of 5 years, 3 months and 2 days v &lt;- years(5) + months(3) + days(2) # add the above period to today&#39;s date today() + v ## [1] &quot;21d 0H 0M 0S&quot; ## [1] &quot;1814400s (~3 weeks)&quot; ## [1] &quot;2028-04-18&quot; Notice that the above expression would not be consistent if obtained “mathematically”. Finally, we can create an interval by using the interval function and providing the start and end timestamps, or by using the as.interval function and giving the duration/period and the start timestamp. We can also use the operator %--% with two timestamps as operands. Exercise 10.14 - intervals # create a variable `interval1` to store the interval # between 6 months before today and 6 months after today # create a variable `interval2` and store the interval from today # until the date that will happen in 4 months, 3 weeks and 2 days # create a variable `interval3` that will store the interval # between 1.5.2002. and 1.7.2002. # print out all three intervals # create a variable `interval1` to store the interval # between 6 months before today and 6 months after today interval1 &lt;- interval(today() - months(6), today() + months(6)) # create a variable `interval2` and store the interval from today # until the date that will happen in 4 months, 3 weeks and 2 days interval2 &lt;- as.interval(months(4) + weeks(3) + days(2), today()) # create a variable `interval3` that will store the interval # between 1.5.2002. and 1.7.2002. interval3 &lt;- dmy(&quot;1.5.2002&quot;) %--% dmy(&quot;1.7.2002&quot;) # print out all three intervals interval1 interval2 interval3 ## [1] 2022-07-16 UTC--2023-07-16 UTC ## [1] 2023-01-16 UTC--2023-06-08 UTC ## [1] 2002-05-01 UTC--2002-07-01 UTC When we have intervals defined we can then: check if a timestamp is within an interval with the help of the operator %within% check whether the intervals overlap by using the function int_overlaps() easily retrieve the start and end of intervals using the functions int_start() and int_end() “merge” two intervals with the help of the union function or find the intersection between them with the help of the intersect function other stuff we can learn by looking at the documentation Exercise 10.15 - working with intervals # check whether today is within the interval defined by the variable `interval1` # if `interval1` and `interval2` overlap # print out their intersection # check whether today is within the interval defined by the variable `interval1` today() %within% interval1 # if `interval1` and `interval2` overlap # print out their intersection if(int_overlaps(interval1, interval1)) intersect(interval1, interval2) ## [1] TRUE ## [1] 2023-01-16 UTC--2023-06-08 UTC In this section, we are introduced to some of the functionality offered by the base R’s date and time classes as well as some of the functionalities offered by the lubridate package. For more information, see the official documentation of the R language and the lubridate package, or the article called “Dates and Times Made Easy with lubridate” written by the author of the package Hadley Wickham, available at this link . 10.3 Working with character strings R has very good support for character strings. However, the functions offered by base R are a bit unintuitive and inconsistent when compared to similar functions offered by other programming languages commonly used for text analysis (such as Perl or Python). It is for these specific reasons that the stringr package emerged, offering a very popular alternative to existing character string functions. However, before getting acquainted with the features offered by this package, it is necessary to briefly address the general issues of managing character strings in data analysis as well as introduce a technology without which the implementation of character string analysis is almost unthinkable - “regular expressions”. 10.3.1 Text analysis and regular expressions Text analysis is an inevitable element of data analysis. Whether it’s simple category identification, searching for specific patterns, or something performing more complex tasks commonly known as text mining, it’s hard to imagine any meaningful data analysis that doesn’t at some point require knowledge of at least the basic methods of managing sets of character strings. Regardless of the complexity of character string analysis we want to perform, one technology is ubiquitous and universally applicable - regular expressions. This is a special “language” that we use to define “patterns” which we use to search or process textual information in various ways. A thorough review of regular expression technology is beyond the scope of this coursebook. Below, we will provide only a brief overview. If you have never encountered this technology before, we strongly recommend that you make the effort and master at least the basic concepts, preferably using one of many excellent web resources. One very short yet effective regular expression mini-courses can be found here . A regular expression is simply a string of characters that represents the pattern we are looking for within a text. Eg. the regular expression gram is contained in the character string Programming language R but is not in the character string Text analysis. We say we found a “match” with the first string but not with the second. This kind of regular expression is not too flexible - the true power of regular expressions lies in using special symbols which enable describing patterns in a more “generic” way. Let’s demonsrate this on a simple example. One common scenarios of using regular expressions is checking whether the user has entered an address that corresponds to the “general” form of the email address. One possibility is to simply use the expression @ as a regular expression we want to find a “match” with the email adress we are checking for validity. This can help with filtering out a certain number of invalid email adresses, but will also allow “addresses” such as @@@ and @23456. With a little “work” on the expression, you could come up with a slightly better solution, which may look something like this: \\w+@\\w+\\.\\w+ Although it looks like a series of random characters, with basic knowledge of regular expressions (and, in this case, using the “Perl” standard) we can interpret the above expression relatively easily. The character \\w stands for “letter, number or underscore”, the sign + means “1 or more”, etc. If you wanted to “transcribe” the above regular expression in a spoken language, it would be “one or more letters, digits or underscores’, followed by the sign @, then again one or more letters, digits or underscores, then a dot and then finally one or more letters, digits or underscores.” While this is not an overly sophisticated regular expression, it is still better than the first attempt. Further refinement is certainly possible, and although subsequent additions are increasingly making it harder to be easily interpreted by the human, we are also gaining more and more control over the formal definitions of what an email address should look like (for such specific uses, it is often worthwhile to check publicly available regular expression repositories where we can find complex but of high quality and carefully tested expressions that are easy enough to copy into our program code). We already stated that the above expression is written in so-called “Perl standard”. Unfortunately, today there isn’t a single standard for regular expressions. Most often used are so-called “POSIX standard” in two versions - BRE and ERE (Basic Regular Expressions and Extended Regular Expressions) which are pretty similar, except for the fact that BRE relies a bit on the \\ character and doesn’t recognize some of the “newer” symbols that ERE uses. Another popular standard is the already mentioned “Perl standard” which is a version of regular expressions implemented in the Perl programming language. Because Perl is one of the leading languages for text processing, this standard has become one of the most widely accepted methods of using regular expressions. In general, almost all popular programming languages have support for regular expressions, either embedded in the language or with the help of additional packages. R is one of the languages that already includes support for regular expressions in its base package. What’s more, R has built-in parallel support for the three most widely used standards mentioned above - POSIX ERE, POSIX BRE and Perl. POSIX ERE is the default setting, and with certain parameters we can easily “switch” to BRE (extended = FALSE) or Perl (perl = TRUE). In the following paragraphs, we will stick with the ERE standard, but it is also important to know that the above settings may be used if we already have previously constructed regular expressions that have been developed in another standard (and we do not want to be bothered by switching from one standard to another). The following table gives a brief overview of some of the more commonly used regular expression elements in the language R: Element Meaning abcd literal string “abcd” 1234 literal string “1234” \\\\d or [:digit:] or [0-9] any digit \\\\D or [:alpha:] or [A-Za-z] any letter [:alnum:] any alphanumeric character . any character \\\\. dot (full stop) [abc] only the characters listed [^abc] all characters except those listed * zero or more repetitions + one or more repetitions {n} exactly n repetitions {m, n} at least m, at most n repetitions ? optional character [:space:] or \\\\ s any blank (space, tab, new line) [:punct:] punctuation marks ^ ... $ a start and end markers (ab|cd) string “ab” or string “cd” Note that when using the special character \\ as a part of a regular expression we actually have to use the double character \\\\ (the first time to indicate to R that a special character follows, the second time to literally use it as part of the regular expression). The basic functions of the R language to work with character strings (and thus regular expressions) are, among other things, grep,grepl, regexrp,gregexrp, regmatches, sub, gsub etc. But since the stringr package offers a set of alternative functions with almost the same functionality but with far more intuitive names and more consistent signatures, we will focus on functions from that package, leaving the base functions to readers as an optional exercise to be learned from the official documentation. 10.3.2 The stringr package We have already stated that the stringr package actually reimplements to some extent the already existing functions of the language R, but in a more intuitive and consistent way. To be more precise, the functions of the stringr package have slightly reduced functionality, which is actually by design - one of the imperatives when designing this package was to identify the most commonly used functionalities for text analysis and focus on that primarily. Functionality that is “thrown out” concerns specific cases for which the developer will need to look for alternative solutions (often in the form of basic functions), but the benefit is in simpler, more intuitive features that are easier to learn and use effectively in the long-term effectively. Additional benefits of using the stringr package are: consistent treatment of factors as character strings consistent ordering of parameters, which is especially useful when used in conjunction with the operator %&gt;% We can start with some simpler functions for which we do not need regular expressions (we list simplified function signatures, for more precise definitions consult the package documentation): str_c(string1, string2, ...) - merge character strings, alternative to paste0 str_length(string) - returns the length of the character string str_sub(string, start, end) - returns a substring using start and end as letter indexes (negative index means “counting from the back”) str_sub(string, start, end) &lt;- string2 - modifies string by exchanging the defined substring with string2 (which does need not be the same length as the dropped substring!) str_trim(string) - returns a string with removed blanks from the beginning and end of a string Exercise 10.16 - basic functions for working with character strings string1 &lt;- &quot; This is an example&quot; string2 &lt;- &quot;of string concatenation! &quot; # using one line of instructions concatenate the above strings, # remove the blanks at the beginning and end of the string from the result, # then select the substring from 35th to 55th character # and print the final result on the screen string &lt;- &quot;R is overly complicated and not an easy language!&quot; # in the upper character string, replace all the characters # from 6th place (counted from the start) # to 14th (counting from the end) # with an empty string # print the string string1 &lt;- &quot; This is an example&quot; string2 &lt;- &quot; of string concatenation! &quot; # using one line of instructions concatenate the above strings, # remove the blanks at the beginning and end of the string from the result, # then select the substring from 22nd to 41st character # and print the final result on the screen str_c(string1, string2)%&gt;% str_trim()%&gt;% str_sub(22, 41) string &lt;- &quot;R is overly complicated and not an easy language!&quot; # in the upper character string, replace all the characters # from 6th place (counted from the start) # to 14th (counting from the end) # with an empty string str_sub(string, 6, -14) &lt;- &quot;&quot; # print the string string ## [1] &quot; string concatenatio&quot; ## [1] &quot;R is asy language!&quot; The str_c function also has a sep parameter if we want to paste the strings with a specific separator, and the collapse parameter which is NULL by default, but which can be used to merge elements of a character vector into a single string (with the value of the parameter used as a separator). Exercise 10.17 - merging character strings string1 &lt;- &quot;To merge&quot; string2 &lt;- &quot;these strings&quot; string3 &lt;- &quot;you need some space!&quot; # merge the above strings into a single string and print the result arrays &lt;- c(&quot;These&quot;, &quot;vector&quot;, &quot;elements&quot;, &quot;should&quot;, &quot;be&quot;, &quot;joined...&quot;) # merge the elements of the above vector into one string and print the result string1 &lt;- &quot;To merge&quot; string2 &lt;- &quot;these strings&quot; string3 &lt;- &quot;you need some space!&quot; # merge the above strings into a single string and print the result str_c(string1, string2, string3, sep = &quot; &quot;) strings &lt;- c(&quot;These&quot;, &quot;vector&quot;, &quot;elements&quot;, &quot;should&quot;, &quot;be&quot;, &quot;joined...&quot;) # merge the elements of the above vector into one string and print the result str_c(strings, collapse = &quot; &quot;) ## [1] &quot;To merge these strings you need some space!&quot; ## [1] &quot;These vector elements should be joined...&quot; Let’s look at some stringr functions that work with regular expressions: str_detect(string, pattern) - returns TRUE if string contains pattern, otherwiseFALSE str_extract(string, pattern) - returns a string of characters corresponding to the first occurrence of a pattern str_extract_all(string, pattern) - returns a list with all occurrences that match the pattern str_replace(string, pattern, replacement) - changes the first occurrence of a pattern with the replacement str_replace_all(string, pattern, replacement) - changes all occurrences of a pattern with the replacement All of these functions are vectorized, which means that they behave logically (ie, “parallelized”) when we use a character vector with multiple elements in place of a specific parameter. For example, if we give a vector of strings and a vector of replacements to the str_replace function, elements of the first vector and replacements will “pair up” in place of a given pattern. We can have other combinations, such as a vector of original elements and a vector of patterns, all three parameters as vectors with multiple elements etc. but every time the behavior of the function is consistent with the already learned principle of vectorization. Exercise 10.18 - functions and regular expressions addresses &lt;- c(&quot;pero.peric@fer.hr&quot;, &quot;iva.ivic@etfos.hr&quot;, &quot;ppetrovic@gmail.com&quot;, &quot;branko1987@yahoo.com&quot;, &quot;jaRULZ4EVR@gmail.nz&quot;, &quot;dperkovic@efzg.hr&quot;, &quot;lalaic1998@gmail.co.uk&quot;, &quot;perica.markic@fer.hr&quot;) # print the total number of mail addresses which belong to the `fer.hr` subdomain # print all addresses that contain at least one digit # list all addresses that have a vowel as the second character # print all unique email address subdomains # (subdomain part of the address is everything after behind the `@` character) # anonymize the addresses above: all character strings in front of &#39;@&#39; # should be replaced with random 6-digit numbers # print the total number of mail addresses which belong to the `fer.hr` subdomain str_detect(addresses, &quot;fer\\\\.hr&quot;)%&gt;% sum # print all addresses that contain at least one digit addresses[str_detect(addresses, &#39;[:digit:]&#39;)] # list all addresses that have a vowel as the second character str_detect(addresses, &quot;^.[aeiouAEIOU]&quot;) %&gt;% addresses[.] # print all unique email address subdomains # (subdomain part of the address is everything after behind the `@` character) str_extract(addresses, &#39;@(.*)&#39;) %&gt;% str_sub(2) %&gt;% unique # anonymize the addresses above: all character strings in front of &#39;@&#39; # should be replaced with random 6-digit numbers sample(100000: 999999, length(addresses)) %&gt;% as.character %&gt;% str_replace(addresses, &#39;^[^@]*&#39;, .) ## [1] 2 ## [1] &quot;branko1987@yahoo.com&quot; &quot;jaRULZ4EVR@gmail.nz&quot; &quot;lalaic1998@gmail.co.uk&quot; ## [1] &quot;pero.peric@fer.hr&quot; &quot;jaRULZ4EVR@gmail.nz&quot; &quot;lalaic1998@gmail.co.uk&quot; ## [4] &quot;perica.markic@fer.hr&quot; ## [1] &quot;fer.hr&quot; &quot;etfos.hr&quot; &quot;gmail.com&quot; &quot;yahoo.com&quot; &quot;gmail.nz&quot; ## [6] &quot;efzg.hr&quot; &quot;gmail.co.uk&quot; ## [1] &quot;715037@fer.hr&quot; &quot;206896@etfos.hr&quot; &quot;861759@gmail.com&quot; ## [4] &quot;278787@yahoo.com&quot; &quot;581691@gmail.nz&quot; &quot;258345@efzg.hr&quot; ## [7] &quot;618764@gmail.co.uk&quot; &quot;112453@fer.hr&quot; Finally, we learn one relatively useful function called str_split. This function splits the character string into a vector of character strings, depending on the given separator (which may be a space, some other chosen character(s), or even a regular expression), and is often used as a “more primitive” alternative to the read.csv andread.table functions when we want to parse the input “manually”, or - more commonly - to break a column of text into individual words for the purposes of text analysis. This function accepts a character string (or a collection of strings) as input, disassembles it according to the chosen separator and then returns the list of pieces as a result; if we disassemble only one string, we can easily translate the result into a vector using the unlist function. str_split(&quot;Example of str_split function&quot;, pattern = &quot;[:space:]&quot;) %&gt;% unlist ## [1] &quot;Example&quot; &quot;of&quot; &quot;str_split&quot; &quot;function&quot; We will now try to perform a very simple example of text analysis - figuring out which words happen most frequently in a chosen text. To do this we first need to read a text file into an R object. One of the simplest ways to achieve this is using the file function (which opens up the connection towards the textual file) and then the readLines function which will read a chosen number of lines and store them in a character vector. For smaller files we can also immediately read the entire file simply by omitting the number of lines we want to read. An example of using these two functions may look like this: con &lt;- file(&quot;textFile.txt&quot;, &quot;r&quot;) # r = &quot;read&quot; rows &lt;- readLines(con) # or readLines(con, n = 100) close(con) # closing the connection The next assignment will use two files HobbitChapterOne.txt - the text we want to analyse and stopwords.txt - file with frequent words which are “uninteresting” from the analysis point of view Exercise 10.19 - simple text analysis # store the text from `HobbitChapterOne.txt` into a variable called `hobbit` # and text from `stopwords.txt` into a variable called `stopwords` # perform the next steps: # - merge all text segments from `hobbit` into one long character strings # - remove all punctuation marks from the text # - change all text to &quot;lowercase&quot; (use `tolower`) # - split the text using spaces as a separator # - remove &quot;empty words&quot; (words of length 0) if such exist # - remove all words which are present in the `stopwords` variable too # - count the frequencies of the words # - print on screen 20 most frequent words # store the text from `HobbitChapterOne.txt` into a variable called `hobbit` # and text from `stopwords.txt` into a variable called `stopwords` con &lt;- file(&quot;HobbitChapterOne.txt&quot;) hobbit &lt;- readLines(con) close(con) con &lt;- file(&quot;stopwords.txt&quot;) stopwords &lt;- readLines(con) close(con) # perform the next steps: # - merge all text segments from `hobbit` into one long character strings # - remove all punctuation marks from the text # - change all text to &quot;lowercase&quot; (use `tolower`) # - split the text using spaces as a separator # - remove &quot;empty words&quot; (words of length 0) if such exist # - remove all words which are present in the `stopwords` variable too # - count the frequencies of the words # - print on screen 20 most frequent words hobbit %&gt;% str_c(collapse = &quot; &quot;) %&gt;% str_replace_all(&#39;[:punct:]&#39;, &#39;&#39;) %&gt;% tolower %&gt;% str_split(&#39;[:space:]&#39;) %&gt;% unlist -&gt; hobbit2 hobbit3 &lt;- hobbit2[!(hobbit2 %in% stopwords | nchar(hobbit2) == 0)] freq &lt;- table(hobbit3) %&gt;% sort(decreasing = T) freq[1:20] ## hobbit3 ## bilbo gandalf one thorin dwarves door baggins good hobbit little ## 39 36 36 34 31 27 25 25 25 24 ## long went know time away go old things come morning ## 24 23 20 19 17 17 17 17 16 16 Homework exercises The following tasks relate to the data set stored in the CSV file crimeSample.csv, which is a sample from the record of criminal incidents in the City of Philadelphia (the original data set can be found at this link ). The original set of columns was reduced and 1000 incidents were randomly sampled from the set of all observations. Before solving problems, load the data into the crimes data box and familiarize yourself with the data set (str,head, etc.) Convert the timestamp column from character type to POSIXct type. Add the following columns to the data frame: Year,Month, Hour. Fill in the columns with the appropriate information from the timestamp. Answer the question: in what month does the most crime occur? Which hour of the day is “most dangerous” according to the data? Answer the question: What is the percentage of incidents where the incident description contains the word burglary '' or robbery ’’? (tip: convert the entire crime description column to lowercase using the tolower() function. Print any unique four-digit numbers you can find on street names where a criminal incident is recorded. Let’s Program in Ru by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License . Based on work at https://ratnip.github.io/FER_OPJR/ "],["dplyr.html", "11 Data wrangling 11.1 Data wrangling with package dplyr 11.2 Dataset: Titanic 11.3 Creating a subset of observations with filter andslice 11.4 Creating a subset of columns with select 11.5 Creating new columns with mutate 11.6 Sample Dataset: Houston flights 11.7 Grouping and aggregation with group_by andsummarise 11.8 Merging data frames with join functions 11.9 Integrating dplyr with relational databases Homework exercises", " 11 Data wrangling 11.1 Data wrangling with package dplyr The dplyr package is one of the newer R language packages whose main function is to efficiently and easily manage data frames with a set of intuitively designed functions. At the time of writing this lesson, is at the top of the list of most popular packages despite strong “competition” in this area made up of extremely popular packages such as plyr, data.table and base, traditional methods for working with data frames. Either of these options is a good choice, and although the dplyr package is not the most efficient in terms of performance (by far the best performance is currently offered by thedata.table package), the advantage of this package is its intuitive, readable syntax that makes programming and working with data frames fast and easy, representing a great choice for developers who are familiar with R or simply want to focus on working with data through readable, easy-to-use program code. The specific advantages of the dplyr package are: simple syntax (similar to SQL but procedural) that uses the five main “verbs” to manipulate data frames and as such defines a standalone language-within-language of sorts higher efficiency compared to the methods offered by the basic package (initially dplyr was designed for greater programming efficiency, not necessarily for better performance, but in the meantime the implementation of certain routines in C also allowed for improved computational performance) integration with relational databases and big data sources (certain dplyr functions can be used directly on tables in the database by automatically translating those functions into SQL commands) The aforementioned basic “five verbs” offered by the dplyr package are as follows: filter - for filtering the dataset by rows select - for selecting individual columns arrange - for changing the order of the rows mutate - for creating new columns from existing ones summarise - for data aggregation In addition to these five basic verbs, we often use: group_by for grouping data within a dataset the join family of functions for merging data frames SQL language experts will easily notice the parallels between the SQL language and the stated dplyr functionality. The biggest difference is that SQL works “declaratively”, i.e. we have to follow the rules of building an SQL command that “does everything at once”, while in R using the functions of the dplyr package and the already familiar operator%&gt;%actions can be performed on datasets procedurally, with clear data processing flow going from left to right. Before we do a detailed look at the functionality of the dplyr package, let’s get acquainted with one of the two datasets we will be using during this lesson. 11.2 Dataset: Titanic We will choose one commonly used dataset - “Titanic Passenger Survival Dataset”. This dataset contains information about the passengers of the cruise shipTitanic which sank on 14 April 1912, whereupon only 706 out of 2223 passengers survived. This dataset contains, amongst other things, passengers’ names, gender, age, passenger class, etc. There is a version of this dataset that comes with R distribution itself, but we will use its extended version from a Kaggle competition “Titanic: Machine Learning From Disaster” which can be found more at this link. Let’s load this data set using the read_csv function, areadr package function that “upgrades” the read.csv function. Also, instead of the str function, let’s try to use its equivalent, the glimpse function, provided by the dplyr package. Exercise 11.1 - dataset Titanic # load the data set from the file `Titanic.csv` into the variable` # `titanic` using the `read_csv` function from the `readr` package # please read the documentation before using the function # view the structure of the `titanic` dataframe # using the `glimps` function # and check the first few rows with the `head` function # load the data set from the file `Titanic.csv` into the variable` # `titanic` using the `read_csv` function from the `readr` package # please read the documentation before using the function titanic &lt;- read_csv(&quot;Titanic.csv&quot;) # view the structure of the `titanic` dataframe # using the `glimpse` function # and check the first few rows with the `head` function glimpse(titanic) head(titanic) ## Rows: 891 Columns: 12 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (5): Name, Sex, Ticket, Cabin, Embarked ## dbl (7): PassengerId, Survived, Pclass, Age, SibSp, Parch, Fare ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Rows: 891 ## Columns: 12 ## $ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,… ## $ Survived &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1… ## $ Pclass &lt;dbl&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3… ## $ Name &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bradley (Fl… ## $ Sex &lt;chr&gt; &quot;male&quot;, &quot;female&quot;, &quot;female&quot;, &quot;female&quot;, &quot;male&quot;, &quot;male&quot;, &quot;mal… ## $ Age &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, … ## $ SibSp &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0… ## $ Parch &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0… ## $ Ticket &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;113803&quot;, &quot;37… ## $ Fare &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,… ## $ Cabin &lt;chr&gt; NA, &quot;C85&quot;, NA, &quot;C123&quot;, NA, NA, &quot;E46&quot;, NA, NA, NA, &quot;G6&quot;, &quot;C… ## $ Embarked &lt;chr&gt; &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;Q&quot;, &quot;S&quot;, &quot;S&quot;, &quot;S&quot;, &quot;C&quot;, &quot;S&quot;, &quot;S&quot;… ## # A tibble: 6 × 12 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 0 3 Braund… male 22 1 0 A/5 2… 7.25 &lt;NA&gt; ## 2 2 1 1 Cuming… fema… 38 1 0 PC 17… 71.3 C85 ## 3 3 1 3 Heikki… fema… 26 0 0 STON/… 7.92 &lt;NA&gt; ## 4 4 1 1 Futrel… fema… 35 1 0 113803 53.1 C123 ## 5 5 0 3 Allen,… male 35 0 0 373450 8.05 &lt;NA&gt; ## 6 6 0 3 Moran,… male NA 0 0 330877 8.46 &lt;NA&gt; ## # … with 1 more variable: Embarked &lt;chr&gt; Why use the function read_csv instead of its counterpart with a similar name, read.csv? There are several reasons: greater autonomy, i.e. better inference of column types faster processing speed provides status information for easier error detection no automatic categorization the loaded object is automatically converted into a “tibble” All these reasons are clear, except perhaps the last one. What is a “tibble”? This is a colloquial name for an object of class tbl_df, which is an upgrade of sorts of the data.frame class, i.e. the basic data frame. The biggest advantage of this type of object is that when you try to print the whole data frame to the screen (which usually happens by mistake and which can result in a “freezing” of the R console), a special function will be called that will print only part of the frame and be able to resume operation immediately . If we want to “upgrade” an existing dataframe to a tibble, we can do this with the as_tibble command (note that functions in the tidyverse collection often contain an underscore _, to help distinguish them as upgrades of basic R constructs which have similar names but use a dot .). We can fine tune how tibbles get printed by using the options function and the parameters tibble.print_max and `tibble.width, for example: # I want to see a maximum of 10 rows and always print all columns options(tibble.print_max = 10, tibble.width = Inf) An easier way for a one-off printing of more than 10 rows we can simply add a paramter n to the default print function: # `df` is of class `tbl_df`, I want to see 50 lines printed print(df, n = 50) Before proceeding, it would be a good idea to get acquainted with the data set we will be using, either through a more detailed research of the dataset, or by collecting documentation on the dataset. The brief description of the dataset itself, collected from the official Keggle page of the competition, is as follows: VARIABLE DESCRIPTIONS: survival Survival (0 = No; 1 = Yes) pclass Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd) name Name sex Sex age Age sibsp Number of Siblings/Spouses Aboard parch Number of Parents/Children Aboard ticket Ticket Number fare Passenger Fare cabin Cabin embarked Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton) SPECIAL NOTES: Pclass is a proxy for socio-economic status (SES) 1st ~ Upper; 2nd ~ Middle; 3rd ~ Lower Age is in Years; Fractional if Age less than One (1) If the Age is Estimated, it is in the form xx.5 With respect to the family relation variables (i.e. sibsp and parch) some relations were ignored. The following are the definitions used for sibsp and parch. Sibling: Brother, Sister, Stepbrother, or Stepsister of Passenger Aboard Titanic Spouse: Husband or Wife of Passenger Aboard Titanic (Mistresses and Fiances Ignored) Parent: Mother or Father of Passenger Aboard Titanic Child: Son, Daughter, Stepson, or Stepdaughter of Passenger Aboard Titanic Other family relatives excluded from this study include cousins, nephews/nieces, aunts/uncles, and in-laws. Some children travelled only with a nanny, therefore parch=0 for them. As well, some travelled with very close friends or neighbors in a village, however, the definitions do not support such relations. At this point, it should be considered whether the dataset includes some categorical data. As we have learned, unlike the read.csv function that automatically factorizes all character columns (which is not recommended), the read_csv function from the readr package does not factorize anything, but rather leaves that to the analyst. While this represents additional work for the analyst, the level of control and robustness that is obtained is more than a sufficient compromise. In the titanic dataset we notice the following categorical variables: Survival (survival - 2 categories: 0 and 1) Pclass (passenger class - 3 categories: 1, 2 and 3) Sex (gender - 2 categories: “M” and “F”) Embarked (port of embarkation - 3 categories: “C”, “Q” and “S”) Let’s factorize the columns listed. Exercise 11.2 - factorizing the columns of the Titanic dataset # convert the `Survival`,` Pclass`, `Sex` and` Embarked` columns # of the `titanic` data frame into factors # briefly explore the `titanic` data frame # using the &#39;glimpse&#39; function # convert the `Survival`,` Pclass`, `Sex` and` Embarked` columns # of the `titanic` data frame into factors titanic$Survived &lt;- as.factor(titanic$Survived) titanic$Pclass &lt;- as.factor(titanic$Pclass) titanic$Sex &lt;- as.factor(titanic$Sex) titanic$Embarked &lt;- as.factor(titanic$Embarked) # briefly explore the `titanic` data frame # using the &#39;glimpse&#39; function glimpse(titanic) ## Rows: 891 ## Columns: 12 ## $ PassengerId &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,… ## $ Survived &lt;fct&gt; 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1… ## $ Pclass &lt;fct&gt; 3, 1, 3, 1, 3, 3, 1, 3, 3, 2, 3, 1, 3, 3, 3, 2, 3, 2, 3, 3… ## $ Name &lt;chr&gt; &quot;Braund, Mr. Owen Harris&quot;, &quot;Cumings, Mrs. John Bradley (Fl… ## $ Sex &lt;fct&gt; male, female, female, female, male, male, male, male, fema… ## $ Age &lt;dbl&gt; 22, 38, 26, 35, 35, NA, 54, 2, 27, 14, 4, 58, 20, 39, 14, … ## $ SibSp &lt;dbl&gt; 1, 1, 0, 1, 0, 0, 0, 3, 0, 1, 1, 0, 0, 1, 0, 0, 4, 0, 1, 0… ## $ Parch &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 0, 0, 5, 0, 0, 1, 0, 0, 0… ## $ Ticket &lt;chr&gt; &quot;A/5 21171&quot;, &quot;PC 17599&quot;, &quot;STON/O2. 3101282&quot;, &quot;113803&quot;, &quot;37… ## $ Fare &lt;dbl&gt; 7.2500, 71.2833, 7.9250, 53.1000, 8.0500, 8.4583, 51.8625,… ## $ Cabin &lt;chr&gt; NA, &quot;C85&quot;, NA, &quot;C123&quot;, NA, NA, &quot;E46&quot;, NA, NA, NA, &quot;G6&quot;, &quot;C… ## $ Embarked &lt;fct&gt; S, C, S, S, S, Q, S, S, S, C, S, S, S, S, S, S, Q, S, S, C… There is also a more concise way to factorizing columns which leverages the lapply function: categories &lt;- c(&quot;Survived&quot;, &quot;Pclass&quot;, &quot;Sex&quot;, &quot;Embarked&quot;) titanic[categories] &lt;- lapply(titanic[categories], as.factor) Now that we have a good understanding of our data set and have made the initial preparations in terms of column categorization, we can begin with the introduction of dplyr package functions. 11.3 Creating a subset of observations with filter andslice In the chapter on data frames, we have already learned that “slicing” data frames can be done similar to slicing matrices - using index vectors to define which rows/columns are retained. We have also learned that index vectors can be numerical (location), logical, and character-typed. When defining a subset of rows, by far the most common type of index vector is logical - with the help of variables, i.e. data frame columns, we define a specific criterion that “filters” the rows. Unfortunately, the basic R syntax of using logical index vectors to determine a subset of rows is somewhat clumsy, as can be seen from this example: df[df$a &gt; 5 &amp; df$b != 3,] The first and obvious issue is the need to repeat the name of the data frame (which we can eliminate with the help of the attach function, which we said was not an ideal solution because it brings a number of new potential problems). The second issue is the readability problem - the above command is not easy to interpret visually, that is, it is not easy to see immediately afte that it is an instruction which aims to reduce the number of rows in a data frame. The alternative is to use the filter function from the package dplyr which uses explicitely indicates that it is filtering rows, and also allows the use of column names without having to reference the name of the data frame: filter(df, a &gt; 5 &amp; b != 3) Also, it is good to note that the first argument of the function is the data frame itself, which allows us to easily chain it. Most of the dplyr package functions are designed on this principle. The above function is the most common way to select a subset of rows (readers who are acquainted with the SQL language will notice a similarity to the WHERE segment of SQL queries). In addition to the filter function, to specify a subset of rows we want to keep we can also use some of the following functions, which also have very intuitive names (for easier interpretation rather than signature functions, we give examples of parameters): distinct(df) - for removing duplicates slice(df, 1:10) - for location indexing sample_frac(df, 0.2) - random selection of a certain percentage of rows sample_n(df, 50) - randomly select of a specific number of rows top_n(df, 10, a) - returns the first 10 rows, ordered by the values of attribute a We can use the following to rearrange the rows in the result: arrange(df, a, desc(b)) - sort by column a in ascending order and then by b in descending order Let’s try this in the following examples: Exercise 11.3 - row selection # print information about all first class passengers over 60 years of age # print information about all surviving male travelers who # have `George` or` Frank` in their name # check whether the dataset contains duplicate observations # randomly select and print information about five passengers who # have not survived the sinking # print order in descending order of ticket price # print information about the five oldest first class passengers # arrange the result in ascending order of age # print information about all first class passengers over 60 years of age filter(titanic, Pclass == 1 &amp; Age&gt; 60) ## # A tibble: 14 × 12 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 55 0 1 Ostby… male 65 0 1 113509 62.0 B30 ## 2 97 0 1 Golds… male 71 0 0 PC 17… 34.7 A5 ## 3 171 0 1 Van d… male 61 0 0 111240 33.5 B19 ## 4 253 0 1 Stead… male 62 0 0 113514 26.6 C87 ## 5 276 1 1 Andre… fema… 63 1 0 13502 78.0 D7 ## 6 439 0 1 Fortu… male 64 1 4 19950 263 C23 … ## 7 457 0 1 Mille… male 65 0 0 13509 26.6 E38 ## 8 494 0 1 Artag… male 71 0 0 PC 17… 49.5 &lt;NA&gt; ## 9 546 0 1 Nicho… male 64 0 0 693 26 &lt;NA&gt; ## 10 556 0 1 Wrigh… male 62 0 0 113807 26.6 &lt;NA&gt; ## 11 626 0 1 Sutto… male 61 0 0 36963 32.3 D50 ## 12 631 1 1 Barkw… male 80 0 0 27042 30 A23 ## 13 746 0 1 Crosb… male 70 1 1 WE/P … 71 B22 ## 14 830 1 1 Stone… fema… 62 0 0 113572 80 B28 ## # … with 1 more variable: Embarked &lt;fct&gt; # print information about all surviving male travelers who # have `George` or` Frank` in their name filter(titanic, str_detect(Name, &quot;(George|Frank)&quot;) &amp; Sex == &quot;male&quot; &amp; Survived == 1) ## # A tibble: 6 × 12 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 166 1 3 &quot;Golds… male 9 0 2 363291 20.5 &lt;NA&gt; ## 2 371 1 1 &quot;Harde… male 25 1 0 11765 55.4 E50 ## 3 508 1 1 &quot;Bradl… male NA 0 0 111427 26.6 &lt;NA&gt; ## 4 571 1 2 &quot;Harri… male 62 0 0 S.W./… 10.5 &lt;NA&gt; ## 5 710 1 3 &quot;Mouba… male NA 1 1 2661 15.2 &lt;NA&gt; ## 6 832 1 2 &quot;Richa… male 0.83 1 1 29106 18.8 &lt;NA&gt; ## # … with 1 more variable: Embarked &lt;fct&gt; # check whether the dataset contains duplicate observations nrow(titanic) == nrow(distinct(titanic)) ## [1] TRUE # randomly select and print information about five passengers who # have not survived the sinking # print order in descending order of ticket price filter(titanic, Survived == 0) %&gt;% sample_n(5) %&gt;% arrange(desc(Fare)) ## # A tibble: 5 × 12 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 51 0 3 Panula… male 7 4 1 31012… 39.7 &lt;NA&gt; ## 2 552 0 2 Sharp,… male 27 0 0 244358 26 &lt;NA&gt; ## 3 151 0 2 Batema… male 51 0 0 S.O.P… 12.5 &lt;NA&gt; ## 4 336 0 3 Denkof… male NA 0 0 349225 7.90 &lt;NA&gt; ## 5 768 0 3 Mangan… fema… 30.5 0 0 364850 7.75 &lt;NA&gt; ## # … with 1 more variable: Embarked &lt;fct&gt; # print information about the five oldest first class passengers # arrange the result in ascending order of age filter(titanic, Pclass == 1 &amp; Sex == &quot;female&quot;) %&gt;% top_n(5, Age) %&gt;% arrange(Age) ## # A tibble: 6 × 12 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 12 1 1 Bonnel… fema… 58 0 0 113783 26.6 C103 ## 2 196 1 1 Lurett… fema… 58 0 0 PC 17… 147. B80 ## 3 269 1 1 Graham… fema… 58 0 1 PC 17… 153. C125 ## 4 367 1 1 Warren… fema… 60 1 0 110813 75.2 D37 ## 5 830 1 1 Stone,… fema… 62 0 0 113572 80 B28 ## 6 276 1 1 Andrew… fema… 63 1 0 13502 78.0 D7 ## # … with 1 more variable: Embarked &lt;fct&gt; Notice that the top_n function returns the “first n” rows but does not necessarily arrange them in that order. If we want them arranged we also need to use the arrange function. 11.4 Creating a subset of columns with select Another method of slixing a data frame selecting a subset of columns. Unlike selecting a subset of rows, where we most often use logical indexing, columns or variables are most often referenced by their name. The syntax for selecting a subset of columns by name using the basic indexing method in R looks like this: df[, c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)] Here we also notice a certain clumsiness in the syntax and difficulty in interpretation. Column names must be wrapped in a c function, which reduces readability, and the command doesn’t explicitly state that it is selecting columns of the data frame, we have to conclude it from the position of the index vector. In addition, there is no easy way to select a range of columns by name, the existence of a substring or pattern within the name, etc. The select function allows us to explicitly select columns using syntax: select(df, a, b, c) So we simply list the data frame and the row of columns we want to select. It is also notice the similarity to SQL, specifically the SELECT part of SQL queries. But the above syntax is not all that this feature has to offer - select has a number of helper functions and operators that greatly extend its functionality, such as: select(df, a:c) - select columns from a toc select(df, -a, -b) - select all columns except a andb select(df, starts_with(\"PO\"))) - select columns beginning with the letters \"PO\" select(df, contains(\"col\")) - select columns containing the letters\"col\" select(df, matches(\"[123]{2,3}\")) - select columns that match the given regular expression Additional options are easy to find in the official documentation. Let’s try this command, also on our dataset. Exercise 11.4 - selecting the columns # for randomly selected 10 rows, print the passenger&#39;s name, age and # whether he survived the sinking or not # for the 10 oldest passengers, print all attributes from name to ticket price # print all attributes except for the identifier and cabin number # for a randomly selected 1% of rows # for rows from number 10 to number 20, print out all columns beginning with a vowel # for randomly selected 10 passengers whose age is unknown, print all # attributes from name to ticket price, then the passenger class # and finally whether or not the passenger survived # sort the rows alphabetically by name # for randomly selected 10 rows, print the passenger&#39;s name, age and # whether he survived the sinking or not sample_n(titanic, 10) %&gt;% select(Name, Age, Survived) ## # A tibble: 10 × 3 ## Name Age Survived ## &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 Millet, Mr. Francis Davis 65 0 ## 2 Wilhelms, Mr. Charles 31 1 ## 3 Davis, Miss. Mary 28 1 ## 4 Brocklebank, Mr. William Alfred 35 0 ## 5 Lindahl, Miss. Agda Thorilda Viktoria 25 0 ## 6 Vande Walle, Mr. Nestor Cyriel 28 0 ## 7 Nosworthy, Mr. Richard Cater 21 0 ## 8 Artagaveytia, Mr. Ramon 71 0 ## 9 Youseff, Mr. Gerious 45.5 0 ## 10 Bryhl, Mr. Kurt Arnold Gottfrid 25 0 # for the 10 oldest passengers, print all attributes from name to ticket price top_n(titanic, 10, Age) %&gt;% select(Name:Fare) ## # A tibble: 11 × 7 ## Name Sex Age SibSp Parch Ticket Fare ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Wheadon, Mr. Edward H male 66 0 0 C.A. 24579 10.5 ## 2 Ostby, Mr. Engelhart Cornelius male 65 0 1 113509 62.0 ## 3 Goldschmidt, Mr. George B male 71 0 0 PC 17754 34.7 ## 4 Connors, Mr. Patrick male 70.5 0 0 370369 7.75 ## 5 Duane, Mr. Frank male 65 0 0 336439 7.75 ## 6 Millet, Mr. Francis Davis male 65 0 0 13509 26.6 ## 7 Artagaveytia, Mr. Ramon male 71 0 0 PC 17609 49.5 ## 8 Barkworth, Mr. Algernon Henry Wilson male 80 0 0 27042 30 ## 9 Mitchell, Mr. Henry Michael male 70 0 0 C.A. 24580 10.5 ## 10 Crosby, Capt. Edward Gifford male 70 1 1 WE/P 5735 71 ## 11 Svensson, Mr. Johan male 74 0 0 347060 7.78 # print all attributes except for the identifier and cabin number # for a randomly selected 1% of rows sample_frac(titanic, 0.01) %&gt;% select(-PassengerId, -Cabin) ## # A tibble: 9 × 10 ## Survived Pclass Name Sex Age SibSp Parch Ticket Fare Embarked ## &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; ## 1 0 3 Cacic, Mr. Luka male 38 0 0 315089 8.66 S ## 2 1 2 Shelley, Mrs. W… fema… 25 0 1 230433 26 S ## 3 0 3 Lefebre, Miss. … fema… NA 3 1 4133 25.5 S ## 4 0 3 Tobin, Mr. Roger male NA 0 0 383121 7.75 Q ## 5 0 3 Dean, Mr. Bertr… male 26 1 2 C.A. … 20.6 S ## 6 0 3 Skoog, Miss. Ma… fema… 2 3 2 347088 27.9 S ## 7 0 2 Moraweck, Dr. E… male 54 0 0 29011 14 S ## 8 0 3 Nysveen, Mr. Jo… male 61 0 0 345364 6.24 S ## 9 0 3 Braund, Mr. Owe… male 22 1 0 A/5 2… 7.25 S # for rows from number 10 to number 20, print out all columns beginning with a vowel slice(titanic, 10:20) %&gt;% select(matches(&quot;^[AEIOUaeiou]&quot;)) ## # A tibble: 11 × 2 ## Age Embarked ## &lt;dbl&gt; &lt;fct&gt; ## 1 14 C ## 2 4 S ## 3 58 S ## 4 20 S ## 5 39 S ## 6 14 S ## 7 55 S ## 8 2 Q ## 9 NA S ## 10 31 S ## 11 NA C # for randomly selected 10 passengers whose age is unknown, print all # attributes from name to ticket price, then the passenger class # and finally whether or not the passenger survived # sort the rows alphabetically by name filter(titanic, is.na(Age)) %&gt;% sample_n(10) %&gt;% select(Name:Fare, Pclass, Survived) %&gt;% arrange(Name) ## # A tibble: 10 × 9 ## Name Sex Age SibSp Parch Ticket Fare Pclass Survived ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; ## 1 Farthing, Mr. John male NA 0 0 PC 17… 222. 1 0 ## 2 Garfirth, Mr. John male NA 0 0 358585 14.5 3 0 ## 3 Harrington, Mr. Charle… male NA 0 0 113796 42.4 1 0 ## 4 Mitkoff, Mr. Mito male NA 0 0 349221 7.90 3 0 ## 5 Moore, Mr. Leonard Cha… male NA 0 0 A4. 5… 8.05 3 0 ## 6 Moran, Mr. James male NA 0 0 330877 8.46 3 0 ## 7 Paulner, Mr. Uscher male NA 0 0 3411 8.71 3 0 ## 8 Peter, Miss. Anna fema… NA 1 1 2668 22.4 3 1 ## 9 Saalfeld, Mr. Adolphe male NA 0 0 19988 30.5 1 1 ## 10 Sadlier, Mr. Matthew male NA 0 0 367655 7.73 3 0 11.5 Creating new columns with mutate When working with datasets, there is often a need to create additional variables with the help of information stored in one or more existing variables. Most often, we create a new column using an expression that describes how we transform existing data; motivation can be the normalization of a numerical variable, the creation of an indicator or categorical variable, the summation of multiple variables into one single variable, or any other transformation in order to obtain a new variable that is in some way required for the further steps of the analysis process. Assuming that we want to create a new column that stores the sum of two numerical values of the existing columns, then the base R syntax might look as follows: df$c &lt;- df$a + df$b The dplyr package offers us an alternative in the form ofmutate and transmute functions: mutate(df, c = a + b) transmute(df, c = a + b) Difference between these two function is that mutate returns the entire original data frame with newly created columns, while transmute retains only the columns specified inside the function call. Therefore, we can use transmute as a shortened combination of mutate and select: transmute(df, a, c = a + b) # same as mutate(df, c = a + b) %&gt;% select(a, c) (NOTE: We don’t have to necessarily expect a degradation of peformance here as a result of copying data frames, since R does a “shallow copy” when adding a column, that is, a new data frame only references a new column while retaining references to the remaining columns. If we were creating a “deep copy”, i.e. if we were copying all columns into the new frame, which would make this operation “expensive” when it comes to processing) Note that mutate andtransmute are not alternatives to the UPDATE command from SQL, but rather correspond to the expressions used in the SELECT [segment of the command in scenarios when we do not select individual columns but combine them as part of an expression part of the SQL query. The mutate and transmute functions use common (vectorized) functions and operators, but we also have a number of additional so-called “window” functions that give us additional flexibility when creating new variables, such as: ntile,cut - for transforming a numerical column into a categorical; ntile will make categories of the same size, while cut will cut the range in equally sized intervals dense_rank,min_rank - for ranking of observations (the difference is only in handling observations with the same values) between - for creating an indicator column explaining whether a given variable is in the interval given by two other columns pmin,pmax - “parallel minimum and maximum”, ie the minimum or maximum values of the selected columns viewed by individual rows etc. A list of all available functions can be found in the documentation. Exercise 11.5 - creating new columns # add a `hadRelativesOnBoard` logical column to the` titanic` table which # describes whether the passenger had relatives on board # for randomly selected 10 passengers over the age of 20 who boarded in Southampton # print out passenger&#39;s name, travel class, and transformed ticket price # replace the ticket price with what it would cost today # (assume $1 of 1912 equals $23.85 today, calculating in the inflation) # call the new column `FareToday` # round the amount to two decimal places and add the prefix `$` # sort the result by passenger class in descending order # create a `FareCategory` column which puts the ticket prices # into five equally sized categories # then randomly select 20 passengers and print # passenger name, travel class, ticket price and category # sort the result by price category # add an `EmbarkationPort` column to the `titanic` table, which will contain # the full name of the port of embarkation (Southampton, Queenstown or Cherbourg) # use `mutate` and two `ifelse` functions # print the first 10 rows of the `titanic` table # add a `hadRelativesOnBoard` logical column to the` titanic` table which # describes whether the passenger had relatives on board titanic &lt;- mutate(titanic, hadRelativesOnBoard = SibSp&gt; 0 | Parch&gt; 0) # for randomly selected 10 passengers over the age of 20 who boarded in Southampton # print out passenger&#39;s name, travel class, and transformed ticket price # replace the ticket price with what it would cost today # (assume $1 of 1912 equals $23.85 today, calculating in the inflation) # call the new column `FareToday` # round the amount to two decimal places and add the prefix `$` # sort the result by passenger class in descending order filter(titanic, Sex == &#39;female&#39;, Age&gt; 20, Embarked == &#39;S&#39;) %&gt;% sample_n(10) %&gt;% transmute(Name, Pclass, FareToday = str_c(&quot;$&quot;, round(Fare * 23.85, 2))) %&gt;% arrange(desc(Pclass)) ## # A tibble: 10 × 3 ## Name Pclass FareToday ## &lt;chr&gt; &lt;fct&gt; &lt;chr&gt; ## 1 Asplund, Mrs. Carl Oscar (Selma Augusta Emilia Johansson) 3 $748.59 ## 2 Andersson, Mrs. Anders Johan (Alfrida Konstantia Brogren) 3 $745.91 ## 3 Lahtinen, Mrs. William (Anna Sylfven) 2 $620.1 ## 4 Ball, Mrs. (Ada E Hall) 2 $310.05 ## 5 Mack, Mrs. (Mary) 2 $250.43 ## 6 Swift, Mrs. Frederick Joel (Margaret Welles Barron) 1 $618.41 ## 7 Shutes, Miss. Elizabeth W 1 $3660.08 ## 8 Perreault, Miss. Anne 1 $2229.98 ## 9 Hogeboom, Mrs. John C (Anna Andrews) 1 $1859.31 ## 10 Robert, Mrs. Edward Scott (Elisabeth Walton McMillan) 1 $5040.4 # create a `FareCategory` column which puts the ticket prices # into five equally sized categories # then randomly select 20 passengers and print # passenger name, travel class, ticket price and category # sort the result by price category mutate(titanic, FareCategory = ntile(Fare, 5)) %&gt;% sample_n(20) %&gt;% select(Name, Pclass, Fare, FareCategory) %&gt;% arrange(FareCategory) ## # A tibble: 20 × 4 ## Name Pclass Fare FareCategory ## &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;int&gt; ## 1 &quot;Carlsson, Mr. August Sigfrid&quot; 3 7.80 1 ## 2 &quot;Coelho, Mr. Domingos Fernandeo&quot; 3 7.05 1 ## 3 &quot;Parkes, Mr. Francis \\&quot;Frank\\&quot;&quot; 2 0 1 ## 4 &quot;Attalah, Mr. Sleiman&quot; 3 7.22 1 ## 5 &quot;Rogers, Mr. William John&quot; 3 8.05 2 ## 6 &quot;Harris, Mr. George&quot; 2 10.5 2 ## 7 &quot;Kraeff, Mr. Theodor&quot; 3 7.90 2 ## 8 &quot;Moore, Mr. Leonard Charles&quot; 3 8.05 2 ## 9 &quot;Hamalainen, Master. Viljo&quot; 2 14.5 3 ## 10 &quot;Panula, Master. Eino Viljami&quot; 3 39.7 4 ## 11 &quot;Andersson, Mr. Anders Johan&quot; 3 31.3 4 ## 12 &quot;Hart, Miss. Eva Miriam&quot; 2 26.2 4 ## 13 &quot;Smith, Mr. James Clinch&quot; 1 30.7 4 ## 14 &quot;Blank, Mr. Henry&quot; 1 31 4 ## 15 &quot;Chibnall, Mrs. (Edith Martha Bowerman)&quot; 1 55 5 ## 16 &quot;Robbins, Mr. Victor&quot; 1 228. 5 ## 17 &quot;Maioni, Miss. Roberta&quot; 1 86.5 5 ## 18 &quot;Warren, Mrs. Frank Manley (Anna Sophia Atkinson)&quot; 1 75.2 5 ## 19 &quot;Chaffee, Mr. Herbert Fuller&quot; 1 61.2 5 ## 20 &quot;Fortune, Miss. Mabel Helen&quot; 1 263 5 # add an `EmbarkationPort` column to the `titanic` table, which will contain # the full name of the port of embarkation (Southampton, Queenstown or Cherbourg) # use `mutate` and two `ifelse` functions mutate(titanic, EmbarkationPort = ifelse(Embarked == &#39;S&#39;, &#39;Southampton&#39;, ifelse(Embarked == &#39;C&#39;, &#39;Cherbourg&#39;, &#39;Queenstown&#39;))) -&gt; titanic # print the first 10 rows of the `titanic` table head(titanic, 10) ## # A tibble: 10 × 14 ## PassengerId Survived Pclass Name Sex Age SibSp Parch Ticket Fare Cabin ## &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;chr&gt; &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1 0 3 Braun… male 22 1 0 A/5 2… 7.25 &lt;NA&gt; ## 2 2 1 1 Cumin… fema… 38 1 0 PC 17… 71.3 C85 ## 3 3 1 3 Heikk… fema… 26 0 0 STON/… 7.92 &lt;NA&gt; ## 4 4 1 1 Futre… fema… 35 1 0 113803 53.1 C123 ## 5 5 0 3 Allen… male 35 0 0 373450 8.05 &lt;NA&gt; ## 6 6 0 3 Moran… male NA 0 0 330877 8.46 &lt;NA&gt; ## 7 7 0 1 McCar… male 54 0 0 17463 51.9 E46 ## 8 8 0 3 Palss… male 2 3 1 349909 21.1 &lt;NA&gt; ## 9 9 1 3 Johns… fema… 27 0 2 347742 11.1 &lt;NA&gt; ## 10 10 1 2 Nasse… fema… 14 1 0 237736 30.1 &lt;NA&gt; ## # … with 3 more variables: Embarked &lt;fct&gt;, hadRelativesOnBoard &lt;lgl&gt;, ## # EmbarkationPort &lt;chr&gt; 11.6 Sample Dataset: Houston flights Let’s now load the hflights data frame, which is in a package of the same name and can be retrieved from the CRAN repository. After loading the package, we can move the data frame to the global environment with the help of the data function. Exercise 11.6 - dataset hflights # load the `hflights` package # if necessary install it from the CRAN repository # put the `hflights` data frame in the global environment using the `data` function # convert the frame to `hflights` into a &#39;tibble&#39; # briefly explore the `hflights` dataset # you can also check the documentation with the help of command `?hflights` # install.packages(&quot;hflights&quot;) #if necessary #library(dplyr) # if not already loaded #library(hflights) data(hflights) # convert the frame to `hflights` in &#39;tibble&#39; hflights &lt;- as_tibble(hflights) glimpse(hflights) head(hflights) ## Rows: 227,496 ## Columns: 21 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011… ## $ Month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ DayofMonth &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 1… ## $ DayOfWeek &lt;int&gt; 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 1, 2… ## $ DepTime &lt;int&gt; 1400, 1401, 1352, 1403, 1405, 1359, 1359, 1355, 1443… ## $ ArrTime &lt;int&gt; 1500, 1501, 1502, 1513, 1507, 1503, 1509, 1454, 1554… ## $ UniqueCarrier &lt;chr&gt; &quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;, &quot;AA&quot;… ## $ FlightNum &lt;int&gt; 428, 428, 428, 428, 428, 428, 428, 428, 428, 428, 42… ## $ TailNum &lt;chr&gt; &quot;N576AA&quot;, &quot;N557AA&quot;, &quot;N541AA&quot;, &quot;N403AA&quot;, &quot;N492AA&quot;, &quot;N… ## $ ActualElapsedTime &lt;int&gt; 60, 60, 70, 70, 62, 64, 70, 59, 71, 70, 70, 56, 63, … ## $ AirTime &lt;int&gt; 40, 45, 48, 39, 44, 45, 43, 40, 41, 45, 42, 41, 44, … ## $ ArrDelay &lt;int&gt; -10, -9, -8, 3, -3, -7, -1, -16, 44, 43, 29, 5, -9, … ## $ DepDelay &lt;int&gt; 0, 1, -8, 3, 5, -1, -1, -5, 43, 43, 29, 19, -2, -3, … ## $ Origin &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IA… ## $ Dest &lt;chr&gt; &quot;DFW&quot;, &quot;DFW&quot;, &quot;DFW&quot;, &quot;DFW&quot;, &quot;DFW&quot;, &quot;DFW&quot;, &quot;DFW&quot;, &quot;DF… ## $ Distance &lt;int&gt; 224, 224, 224, 224, 224, 224, 224, 224, 224, 224, 22… ## $ TaxiIn &lt;int&gt; 7, 6, 5, 9, 9, 6, 12, 7, 8, 6, 8, 4, 6, 5, 6, 12, 8,… ## $ TaxiOut &lt;int&gt; 13, 9, 17, 22, 9, 13, 15, 12, 22, 19, 20, 11, 13, 15… ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ CancellationCode &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, … ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## # A tibble: 6 × 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 2011 1 1 6 1400 1500 AA 428 ## 2 2011 1 2 7 1401 1501 AA 428 ## 3 2011 1 3 1 1352 1502 AA 428 ## 4 2011 1 4 2 1403 1513 AA 428 ## 5 2011 1 5 3 1405 1507 AA 428 ## 6 2011 1 6 4 1359 1503 AA 428 ## # … with 13 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;int&gt;, ## # AirTime &lt;int&gt;, ArrDelay &lt;int&gt;, DepDelay &lt;int&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, ## # Distance &lt;int&gt;, TaxiIn &lt;int&gt;, TaxiOut &lt;int&gt;, Cancelled &lt;int&gt;, ## # CancellationCode &lt;chr&gt;, Diverted &lt;int&gt; 11.7 Grouping and aggregation with group_by andsummarise In the data analysis literature, we will often come across a so-called SAC paradigm (Split-Apply-Combine). It’s a strategy that comes down to breaking down a big task into smaller parts, doing some work on each part, and finally combining all the results into a single whole. The need for this paradigm is found in different analysis scenarios - in exploratory data analysis we will want to calculate different statistics or create new variables separately for different subsets of data (eg, depending on a category variable); When processing extremely large amounts of data, we often want to speed up the processing process by breaking the data into smaller sets that will each be processed separately (the well-known Map-Reduce principle). Users of SQL will easily recognize this principle as grouping and aggregation, which are carried out through the GROUP BY segment of an SQL query with the accompanying elements in the SELECT part. The dplyr package offers very similar functionality (albeit in a procedural way) - we first perform “grouping”, i.e. create subsets of rows of a frame, and then carry out further processing in parallel over each subset, to collect all the results in a single dataframe. For grouping, dplyr offers thegroup_by function, which converts a table (data frame) into a grouped table (grouped_tbl`): group_by(df, a, b, c) Let’s try this feature on our hflights data frame. Exercise 11.7 - grouped table # create a `flight815` variable that will contain rows from the `hflights` data frame # related to flight number 815 # create a `grouped815` variable that will contain rows from `flight815` # grouped by month # check the class of variables `flight815` and `grouped815` # briefly explore the `flight815` and `grouped815` variable structures # with the help of the `glimpse` function # create a `flight815` variable that will contain rows from the `hflights` data frame # related to flight number 815 flight815 &lt;- filter(hflights, FlightNum == 815) # create a `grouped815` variable that will contain rows from `flight815` # grouped by month grouped815 &lt;- group_by(flight815, Month) # check the class of variables `flight815` and `grouped815` class(flight815) class(grouped815) ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; ## [1] &quot;grouped_df&quot; &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; # briefly explore the `flight815` and `grouped815` variable structures # with the help of the `glimpse` function glimpse(flight815) print(&quot;------------------------&quot;) # for more transparent printing on the console glimpse(grouped815) ## Rows: 95 ## Columns: 21 ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011… ## $ Month &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4… ## $ DayofMonth &lt;int&gt; 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, … ## $ DayOfWeek &lt;int&gt; 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3… ## $ DepTime &lt;int&gt; 2116, 2109, 2111, 2112, 2124, 2145, 2114, 2110, 2116… ## $ ArrTime &lt;int&gt; 2255, 2252, 2255, 2326, 2321, 2327, 2324, 2301, 2305… ## $ UniqueCarrier &lt;chr&gt; &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;… ## $ FlightNum &lt;int&gt; 815, 815, 815, 815, 815, 815, 815, 815, 815, 815, 81… ## $ TailNum &lt;chr&gt; &quot;N74856&quot;, &quot;N57852&quot;, &quot;N78866&quot;, &quot;N57863&quot;, &quot;N57868&quot;, &quot;N… ## $ ActualElapsedTime &lt;int&gt; 219, 223, 224, 254, 237, 222, 250, 231, 229, 217, 23… ## $ AirTime &lt;int&gt; 194, 191, 190, 197, 198, 200, 192, 187, 186, 192, 18… ## $ ArrDelay &lt;int&gt; 12, 6, 9, 40, 35, 41, 38, 18, 19, 10, 29, 33, 35, 4,… ## $ DepDelay &lt;int&gt; 6, -1, 1, 2, 14, 35, 4, 0, 6, 9, 6, 17, 38, 11, 3, -… ## $ Origin &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IA… ## $ Dest &lt;chr&gt; &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LA… ## $ Distance &lt;int&gt; 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379… ## $ TaxiIn &lt;int&gt; 10, 13, 15, 18, 12, 10, 14, 19, 12, 12, 20, 15, 12, … ## $ TaxiOut &lt;int&gt; 15, 19, 19, 39, 27, 12, 44, 25, 31, 13, 39, 18, 15, … ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ CancellationCode &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, … ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## [1] &quot;------------------------&quot; ## Rows: 95 ## Columns: 21 ## Groups: Month [6] ## $ Year &lt;int&gt; 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011, 2011… ## $ Month &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4… ## $ DayofMonth &lt;int&gt; 30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20, 19, 18, … ## $ DayOfWeek &lt;int&gt; 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3, 2, 1, 7, 6, 5, 4, 3… ## $ DepTime &lt;int&gt; 2116, 2109, 2111, 2112, 2124, 2145, 2114, 2110, 2116… ## $ ArrTime &lt;int&gt; 2255, 2252, 2255, 2326, 2321, 2327, 2324, 2301, 2305… ## $ UniqueCarrier &lt;chr&gt; &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;, &quot;CO&quot;… ## $ FlightNum &lt;int&gt; 815, 815, 815, 815, 815, 815, 815, 815, 815, 815, 81… ## $ TailNum &lt;chr&gt; &quot;N74856&quot;, &quot;N57852&quot;, &quot;N78866&quot;, &quot;N57863&quot;, &quot;N57868&quot;, &quot;N… ## $ ActualElapsedTime &lt;int&gt; 219, 223, 224, 254, 237, 222, 250, 231, 229, 217, 23… ## $ AirTime &lt;int&gt; 194, 191, 190, 197, 198, 200, 192, 187, 186, 192, 18… ## $ ArrDelay &lt;int&gt; 12, 6, 9, 40, 35, 41, 38, 18, 19, 10, 29, 33, 35, 4,… ## $ DepDelay &lt;int&gt; 6, -1, 1, 2, 14, 35, 4, 0, 6, 9, 6, 17, 38, 11, 3, -… ## $ Origin &lt;chr&gt; &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IAH&quot;, &quot;IA… ## $ Dest &lt;chr&gt; &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LAX&quot;, &quot;LA… ## $ Distance &lt;int&gt; 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379, 1379… ## $ TaxiIn &lt;int&gt; 10, 13, 15, 18, 12, 10, 14, 19, 12, 12, 20, 15, 12, … ## $ TaxiOut &lt;int&gt; 15, 19, 19, 39, 27, 12, 44, 25, 31, 13, 39, 18, 15, … ## $ Cancelled &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ CancellationCode &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, … ## $ Diverted &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… # print the first five rows of the `flight815` variable head(flight815) print( &quot;------------------------&quot;) # print the first five rows of the `grouped815` variable head(grouped815) ## # A tibble: 6 × 21 ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 2011 4 30 6 2116 2255 CO 815 ## 2 2011 4 29 5 2109 2252 CO 815 ## 3 2011 4 28 4 2111 2255 CO 815 ## 4 2011 4 27 3 2112 2326 CO 815 ## 5 2011 4 26 2 2124 2321 CO 815 ## 6 2011 4 25 1 2145 2327 CO 815 ## # … with 13 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;int&gt;, ## # AirTime &lt;int&gt;, ArrDelay &lt;int&gt;, DepDelay &lt;int&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, ## # Distance &lt;int&gt;, TaxiIn &lt;int&gt;, TaxiOut &lt;int&gt;, Cancelled &lt;int&gt;, ## # CancellationCode &lt;chr&gt;, Diverted &lt;int&gt; ## [1] &quot;------------------------&quot; ## # A tibble: 6 × 21 ## # Groups: Month [1] ## Year Month DayofMonth DayOfWeek DepTime ArrTime UniqueCarrier FlightNum ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; ## 1 2011 4 30 6 2116 2255 CO 815 ## 2 2011 4 29 5 2109 2252 CO 815 ## 3 2011 4 28 4 2111 2255 CO 815 ## 4 2011 4 27 3 2112 2326 CO 815 ## 5 2011 4 26 2 2124 2321 CO 815 ## 6 2011 4 25 1 2145 2327 CO 815 ## # … with 13 more variables: TailNum &lt;chr&gt;, ActualElapsedTime &lt;int&gt;, ## # AirTime &lt;int&gt;, ArrDelay &lt;int&gt;, DepDelay &lt;int&gt;, Origin &lt;chr&gt;, Dest &lt;chr&gt;, ## # Distance &lt;int&gt;, TaxiIn &lt;int&gt;, TaxiOut &lt;int&gt;, Cancelled &lt;int&gt;, ## # CancellationCode &lt;chr&gt;, Diverted &lt;int&gt; We see that by grouping we have not “lost” any information - the grouped data frame still looks identical to the original, “non-grouped” frame. In fact, the only indication that something is different is the new, inherited class and the Groups: .. line in the structure print. This means that grouping a frame is merely an indication that some further (most often aggregation) operations are not necessarily performed over the entire data frame, but over individual groups. Also, if we want, we can always easily “ungroup” the frame with the help of the ungroup function. For aggregation, we use the summarise function that receives data (a grouped table) and then combinations of aggregation functions and columns over which they are executed, for example: summarise(df, meanA = mean(a), sdA = sd(a)) As a rule, for the aggregation function we can use any function that reduces a vector to a single value (eg mean,max, sd etc.). Also, dplyr offers some useful functions like: first,last, nth - retreieves the first, last, or n-th element of the group n,n_distinct - counts the number of (distinct) values Exercise 11.8 - function summarise # calculate the average flight arrival delay for the data frames `flight815` and` grouped815` # use the `summarise` function summarise(flight815, meanDelay = mean(ArrDelay)) print( &quot;------------------------&quot;) summarise(grouped815, meanDelay = mean(ArrDelay)) ## # A tibble: 1 × 1 ## meanDelay ## &lt;dbl&gt; ## 1 21.6 ## [1] &quot;------------------------&quot; ## # A tibble: 6 × 2 ## Month meanDelay ## &lt;int&gt; &lt;dbl&gt; ## 1 4 21.1 ## 2 5 20.9 ## 3 6 28.0 ## 4 8 -27.5 ## 5 9 14 ## 6 10 -13 Note that it is advisable to name the aggregated columns. In practice, we usually do not create separate “grouped” data frames but rather we carry out the entire process of selecting rows and columns, grouping, and performing aggregation in a single command. If we use the operator %&gt;%, then it a possible example might look like this: filter(df, a&gt; 5) %&gt;% group_by(a, b) %&gt;% summarise(meanC = mean(c)) %&gt;% arrange(desc(b)) Finally, let us reiterate the readily apparent similarity of the above expression with SQL queries in relational databases, with the essential difference that we perform operations here procedurally, which greatly increases readability and allows us to easily store and check the intermediate result at any time. Now let’s try to take advantage of all the current knowledge of the dplyr package and solve the following examples. All tasks are related to the entire hflights dataset. Exercise 11.9 - advanced queries # print out how many flights were canceled because of # bad weather in each month # result table must have contain only columns called # `Month` and` BadWeatherCancellations` # print out the average time of arrival delay # for departing flights to airports # LAX, JFK and LGA on different days of the week # name the new column &#39;MeanArrDelay&#39; # ignore rows with NA values # sort the result by descending average delay time # print out how many flights were canceled because of # bad weather in each month # result table must have contain only columns called # `Month` and` BadWeatherCancellations` filter(hflights, CancellationCode == &#39;B&#39;) %&gt;% group_by(Month) %&gt;% summarise(BadWeatherCancellations = n()) ## # A tibble: 12 × 2 ## Month BadWeatherCancellations ## &lt;int&gt; &lt;int&gt; ## 1 1 149 ## 2 2 929 ## 3 3 50 ## 4 4 40 ## 5 5 106 ## 6 6 49 ## 7 7 29 ## 8 8 108 ## 9 9 38 ## 10 10 46 ## 11 11 13 ## 12 12 95 # print out the average time of arrival delay # for departing flights to airports # LAX, JFK and LGA on different days of the week # name the new column &#39;MeanArrDelay&#39; # ignore rows with NA values # sort the result by descending average delay time filter(hflights, Dest %in% c(&#39;LAX&#39;, &#39;JFK&#39;, &#39;LGA&#39;)) %&gt;% group_by(DayOfWeek, Dest) %&gt;% summarise(MeanArrDelay = round(mean(ArrDelay, na.rm = T), 2)) %&gt;% ungroup() %&gt;% arrange(desc(MeanArrDelay)) ## `summarise()` has grouped output by &#39;DayOfWeek&#39;. You can override using the ## `.groups` argument. ## # A tibble: 21 × 3 ## DayOfWeek Dest MeanArrDelay ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 4 LGA 21.3 ## 2 4 JFK 17.2 ## 3 5 LGA 13.6 ## 4 1 LGA 13.5 ## 5 7 JFK 10.8 ## 6 1 LAX 10.7 ## 7 3 LGA 10.5 ## 8 4 LAX 10.3 ## 9 2 LGA 9.85 ## 10 6 JFK 9.76 ## # … with 11 more rows Exercise 11.10 - advanced queries (2) # in the tasks that follow, imagine you wrote your own # function that finds the most common category: mostFreqValue &lt;- function(x) table(x) %&gt;% sort(decreasing = T) %&gt;% names%&gt;% `[`(1) # in the `hflights` table, divide the delay time # in 10 categories with the help of the `ntile` function # call the new column `ArrDelayCatId` # then print all the category ids, # total number of flights within a category, # minimum and maximum time of arrival delay # and which airport most often # appears in this category # repeat the previous example but instead of the function # `ntile` try the` cut` function Exercise 11.11 - advanced queries (2) # in the `hflights` table, divide the delay time # in 10 categories with the help of the `ntile` function # call the new column `ArrDelayCatId` # print an identifier for each category # categories, total number of flights within a category, # minimum and maximum time of arrival delay # and which airport is most often # appears in this category mutate(hflights, ArrDelayCatId = ntile(ArrDelay, 10)) %&gt;% group_by(ArrDelayCatId) %&gt;% summarise(totalFlights = n(), minArrDelay = min(ArrDelay), maxArrDelay = max(ArrDelay), mostFreqDest = mostFreqValue(Dest)) ## # A tibble: 11 × 5 ## ArrDelayCatId totalFlights minArrDelay maxArrDelay mostFreqDest ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 1 22388 -70 -14 ATL ## 2 2 22388 -14 -9 ATL ## 3 3 22388 -9 -6 DAL ## 4 4 22388 -6 -3 DAL ## 5 5 22387 -3 0 DAL ## 6 6 22387 0 3 DAL ## 7 7 22387 3 8 DAL ## 8 8 22387 8 15 DAL ## 9 9 22387 15 32 DAL ## 10 10 22387 32 978 DAL ## 11 NA 3622 NA NA DAL # repeat the previous example but instead of the function # `ntile` try the` cut` function mutate(hflights, ArrDelayCat = cut(ArrDelay, 10)) %&gt;% group_by(ArrDelayCat) %&gt;% summarise(totalFlights = n(), minArrDelay = min(ArrDelay), maxArrDelay = max(ArrDelay), mostFreqDest = mostFreqValue(Dest)) ## # A tibble: 11 × 5 ## ArrDelayCat totalFlights minArrDelay maxArrDelay mostFreqDest ## &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; ## 1 (-71,34.8] 203101 -70 34 DAL ## 2 (34.8,140] 18551 35 139 DAL ## 3 (140,244] 1849 140 244 ATL ## 4 (244,349] 294 245 346 ATL ## 5 (349,454] 48 350 450 ATL ## 6 (454,559] 16 458 556 ATL ## 7 (559,664] 4 579 663 DFW ## 8 (664,768] 4 685 766 DFW ## 9 (768,873] 4 775 861 DEN ## 10 (873,979] 3 918 978 DFW ## 11 &lt;NA&gt; 3622 NA NA DAL 11.8 Merging data frames with join functions Merging data frames is an operation known to all developers who have experience in working with relational databases and SQL. When stored in a relational database, tables are often decomposed into multiple tables by a process called “normalization”. The purpose of normalization is mainly to eliminate unnecessary redundancy - each table retains enough data to reconstruct the original data, i.e. sets of observations, which is done through the “join” operation. Although there are different forms of joining tables, by far the most common is the so-called a “natural” join where we have unique identifiers in one table related to the data in the other table. For example - suppose we have a student and city tables, where a student table may have a column that stores the zip code of the user’s place of residence, while other location-related information is in the city table. Storing this information in the students table would be redundant, since the zip code as such uniquely identifies a city, and if we want to see the names of the cities when printing users, we perform a natural join of two tables by the zip code of the city. This zip code is the so-called “foreign key” in the student table and at the same time “primary key” in table city. We will create two simple data frames to demonstrate this. # initializing the `user` and` place` data frames student &lt;- data.frame(id = c(1: 3), lastName = c(&quot;Ivic&quot;, &quot;Peric&quot;, &quot;Anic&quot;), zipResidence = c(10000, 31000, 10000)) city &lt;- data.frame(zip = c(10000, 21000, 31000), nameCity = c(&quot;Zagreb&quot;, &quot;Split&quot;, &quot;Osijek&quot;)) If we want to have a data frame with columns (id, lastName, zipResidence, nameCity), then we need to naturally merge the two data frames. For this, the dplyr package offers theinner_join function. For example, if we want to join data frames df1 and df2, then we can perform the natural join like this: inner_join(df1, df2, by = c(&quot;s1&quot; = &quot;s2&quot;)) where the character strings \"s1\" and \"s2\" denote the column names of these “left” and “right” frames that we are joining (note only one equality sign! ). If the column we use has the same name in both tables, we can only specify the name of that column (or the character vector of multiple columns if we connect via a so-called “composite” foreign key), or omit this parameter completely (if the columns to which we perform merging are the only columns whose names match). Exercise 11.12 - natural join # print the result of natural joining of data frames `student` and `city` # print the result of natural joining of data frames `student` and `city` inner_join(student, city, by = c(&quot;zipResidence&quot; = &quot;zip&quot;)) ## id lastName zipResidence nameCity ## 1 1 Ivic 10000 Zagreb ## 2 2 Peric 31000 Osijek ## 3 3 Anic 10000 Zagreb Keep in mind that joining is a very “expensive” operation; if you join frames with a very large number of rows, the operation could take a very long time and take up a lot of memory. In the event that such “large” joins are often required, then it is strongly recommended to use the data.table package, which implements algorithms with significantly better performance in join operations (using indexing), or - if we work with relational database as a data source - perform the joins on the database side and then pull the result in R. In the next section, we will show how this can potentially be done without having to explicitly write separate SQL commands. If we look at the result of the example above, we can see that we “lost” one row from the city table. Specifically, line (21000, ‘Split’) had no corresponding student and “disappeared” from the result. This is a completely expected result of a natural join, but there are times when we want to keep all the rows from one of the tables we merge. In this case, we have the option of using the so-called “outer natural join” which works identically to the “inner” natural join that we have already seen, but retains all rows from one or both of the tables so that the rows that fail to join remain in the result but with NA values on the opposite side. We distinguish between “left”, “right”, and “full” outer join, depending on whether we want to keep all rows from the left, right or both tables. The dplyr package offers functions that perform all these types of joins, they have a signature identical to the function seen above and are calledleft_join, right_join andfull_join. Exercise 11.13 - outer join # print the result of natural joining of data frames `student` and `city` # which retains all rows from the `city` table # print the result of natural joining of data frames `student` and `city` # which retains all rows from the `city` table right_join(student, city, by = c(&quot;zipResidence&quot; = &quot;zip&quot;)) ## id lastName zipResidence nameCity ## 1 1 Ivic 10000 Zagreb ## 2 2 Peric 31000 Osijek ## 3 3 Anic 10000 Zagreb ## 4 NA &lt;NA&gt; 21000 Split 11.9 Integrating dplyr with relational databases Relational databases are a common source of data for the analysis done in R. One of the standard procedures for preparing data for the analytical process is the exporting from the database to a CSV file, which is then loaded into R. Often, before creating a CSV file, the data from the database must first be adequately collected and prepared, which means that the overall data preparation process requires initial work done in the SQL language directly on the database. This is especially important if we are working with larger datasets that would unnecessarily burden the machine that runs R, and which could be processed much more efficiently in the database itself. The above does not necessarily mean that we need a separate tool to run SQL queries for collecting data from the database. R contains a number of packages for directly communicating with more popular databases, such as MySQL or PostgreSQL. With these packages, we can connect to the base and execute SQL queries directly within the R script, all with the help of R commands, with the result being a data frame ready for further analysis in R. For example, communicating with the MySQL database from R might look like this: install.packages(&quot;RMySQL&quot;) library(RMySQL) # enter real parameters in a realistic scenario! conn &lt;- dbConnect(MySQL(), user = &#39;user&#39;, password = &#39;password&#39;, dbname = &#39;database_name&#39;, host = &#39;host&#39;) df &lt;- dbGetQuery(conn, &quot;SELECT * FROM MY_TABLE&quot;) We see that we first need to load the package with support for connecting to the selected database, then establish a connection with that database (parameters are usually provided to us by the database administrator), followed by providing SQL queries wrapped in appropriate function calls. Although this approach is not complicated and is relatively common, it unmistakably requires mixing R and SQL and programming code. The dplyr package can offer assistance here by relieving the developer of the need to write SQL queries inside the R script. Specifically, dplyr contains integrated support for the automatic conversion of expressions that use dplyr functions directly into an SQL query, which is transparent to the user. In other words, to some extent, we can work on a relational database in the same way as if it we are working on an R data frame. We should be careful here because not all dplyr functionality is translatable into SQL, but most standard preparatory operations such as joining and filtering can be done directly in the database, all the while using just R programming code. In order to put this into practice, we need a working relational database. As it is somewhat impractical to install or prepare an existing relational database just for the needs of a quick example, we can use an efficient alternative offered by the RSQLite package. It allows us to create an SQLite database, a simple relational database that does not require any special installation, does not use a server and holds all its data in one file. In the next program section, we load the RSQLite package, create a new database by creating a new file and saving the mtcars dataframe into a relational table called CARS. # installing and loading the `RSQLite` library - perform if necessary # install.packages ( &quot;RSQLite&quot;) # library(RSQLite) # opening connection with the &quot;SQLite&quot; database # (we&#39;re actually creating a file called simply &#39;mybase&#39;) conn &lt;- dbConnect(drv = SQLite(), dbname = &quot;mybase&quot;) # creating the `CARS` relational table # by using R&#39;s `mtcars` sample dataset dbWriteTable(conn, &quot;CARS&quot;, mtcars, overwrite = T) # checking the list of all existing tables dbListTables(conn) ## [1] &quot;CARS&quot; The usual way to collect data from a database is to embed SQL queries directly into the R program code and store the results in a variable. # get information on the maximum speed and weight of all 6-cylinder cars dbGetQuery(conn, &quot;SELECT mpg, wt FROM CARS WHERE cyl = 6&quot;) ## mpg wt ## 1 21.0 2.620 ## 2 21.0 2.875 ## 3 21.4 3.215 ## 4 18.1 3.460 ## 5 19.2 3.440 ## 6 17.8 3.440 ## 7 19.7 2.770 If we know SQL well, then this method of data collection probably does not seem overly complex and in fact, many developers and analysts are happy to combine SQL and R, optimizing the process of data collection and preparation. However, if we are not too knowledgable in SQL (or simply want a “cleaner” code), we can use the tools provided by the dplyr package. In the following example, we will repeat the process of connecting to the database, but we will execute everything with dplyr functions. # connecting to an existing SQLite database! conn2 &lt;- src_sqlite(&quot;mybase&quot;, create = FALSE) # checking the list of the existing tables src_tbls(conn2) # getting a reference to the existing table # no data gets loaded yet! cars &lt;- tbl(conn2, &quot;CARS&quot;) # get information on the maximum speed and weight of all 6-cylinder cars cars %&gt;% filter(cyl == 6) %&gt;% select(mpg, wt) %&gt;% collect ## Warning: `src_sqlite()` was deprecated in dplyr 1.0.0. ## ℹ Please use `tbl()` directly with a database connection ## [1] &quot;CARS&quot; ## # A tibble: 7 × 2 ## mpg wt ## &lt;dbl&gt; &lt;dbl&gt; ## 1 21 2.62 ## 2 21 2.88 ## 3 21.4 3.22 ## 4 18.1 3.46 ## 5 19.2 3.44 ## 6 17.8 3.44 ## 7 19.7 2.77 One thing to note is that dplyr will not perform anything until we explicitly request the collection of the information, and even then it will retrieve it “in portions” to save memory. This is why we explicitly used the collect function in the example above, essentially forcing R to put the entire result into a data frame (our in our case, simply printting it on the screen). In this way, the R programmer can significantly relieve his R environment by shifting the responsibility of “heavy” operations to the database, while keeping a consistent syntax regardless of whether the operations are done on the database or in memory as R objects. Homework exercises IThe following exercises use the mammals sleep dataset available in the visualization package ggplot2. Load the ggplot2 package and then put the msleep data frame in the global environment using the data function. Before solving the exercises be sure to explore the dataset and learn its features. Then do the following For 10 herbivores who sleep the longest, print their name, how many hours per day they sleep and their average body weight in kg. Sort the entries by sleep length in a descending order. Print the average, longest and shortest sleep time of the animals depending on their type of diet. Categorize the total sleeping time in 5 bins of equal length. For each category, print the total number of animals belonging to the category, and then the total number of class members who are not herbivores. Results should be sorted by sleep length in a descending order. Columns should have meaningful names and the final table should not contain NA values. The following data frame contains the status codes for animal conservation status and their descriptions: conservationStatus &lt;- data.frame( code = c(&quot;ex&quot;, &quot;ew&quot;, &quot;cr&quot;, &quot;en&quot;, &quot;vu&quot;, &quot;nt&quot;, &quot;cd&quot;, &quot;lc&quot;), description = c(&quot;extinct&quot;, &quot;extinct in the wild&quot;, &quot;critically endangered&quot;, &quot;endangered&quot;, &quot;vulnerable&quot;, &quot;near threatened&quot;, &quot;conservation dependent&quot;, &quot;least concern&quot;)) Add a conservationDesc column to ¸the msleep dataframe which will contain the correspoding conservation status descriptions. Be sure not to lose any rows from the msleep data frame. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],["ggplot2.html", "12 Visualising data with ggplot2 package 12.1 Exploratory data analysis 12.2 Data visualization using the R language 12.3 Grammar of graphics and the ggplot2 package 12.4 TU 12.5 Graphs in Exploratory Analysis and Reporting Exercises", " 12 Visualising data with ggplot2 package 12.1 Exploratory data analysis EDA - exploratory data analysis is a process of dataset analysis which aims to allow for familiarization with the data and reaching certain conclusions (or at least hypotheses) about the behavior of variables and observations contained within. The book R for Data Science (co-authored by Hadley Wickham, author of megapopular R packages such as stringr,lubridate, plyr,dplyr, etc.) states that exploratory analysis in principle consists of three main parts: data wrangling data visualization model making whereby this is not a sequential process but rather a circular one, with the phases often being intertwined. It is important to emphasize that exploratory analysis is not a process that can be automated or passively conducted; the analyst is not only a data observer but plays an active role in the process. What initiates the process are precisely the questions about the data which the analyst needs to provide themselves, and which are then answered by exploratory analysis; questions can be diverse, concise or complex, general or specific, and very often require a high level of creativity and curiosity on the part of the analyst. If we try to give a general template for these questions it might be these: how does a variable behave? How is it changing? What are the relationships between two or more variables? Can changing one variable explain the change of another variable? We have already been familiar with data wrangling by learning the basic R as well as functions offered by the tidyr anddplyr packages. In this lesson we will learn what many consider to be the backbone of exploratory analysis - data visualization. The third mentioned phase of the process - model creation - concerns the creation of concise representations of data in the form of mathematical (or other) models, which describe the relationships and behavior of variables in a useful and/or easily interpretable way (e.g. recognizing that a linear relationship between two variables can be described with a simple mathematical equation). We will deal with modeling in one of the upcoming chapters. 12.2 Data visualization using the R language One of the frequently mentioned features of the R language is its superior data visualization functionality. There are a number of analysts and developers who use R almost exclusively as a visualization tool because they can produce professional, attractive and easily interpretable graphs in a very fast and easy way. The basic R language itself has very good support for graphing (the so-called base plot system), but the true power of visualization lies in the many additional packages available through the CRAN repository. Basic support for graphing is achieved with the help of the generic plot function which has a great advantage in its exceptional simplicity when used in the straightforward fashion. Almost every popular class has its own implementation of this function, which means that for “fast” visualization it is often enough to just pass the desired object (or objects) to the specified function. For example, if we pass two numerical vectors of equal size to the function, the plot function will automatically create a graph with the first vector mapped to the x-axis and the second to the y-axis (represented by a linear continuous scale). The function will also automatically add appropriate annotations such as axis lines, tick marks and associated values, axis labels, etc. Exercise 12.1 - plot function x &lt;- 1:50 # pass arguments `x` and `x * x` to the `plot` function x &lt;- 1:50 # pass `x` and `x * x` to the `plot` function plot (x, x * x) Basic support is functional and simple, but limited. New things can be added to the created graph, but not modified. Likewise, fine-tuning certain aspects of the graph often makes the calls cumbersome and unreadable, and the aspect of simplicity (which is the main reason for using the plot function) is lost. Some of the R extensions which offer additional ways to draw visualizations are popular visualization packages such as grid andlattice. The grid package offers a richer set of functions for creating visualizations than those available within basic support, but there is no ability to calculate statistics related to the visualization itself, which often then needs to be performed “manually” before visualization functions are called. The lattice package is especially popular for creating so-called “conditional” or “faceted” graphs, which means more graphs of the same type are quickly vreated where each graph corresponds to an individual value of a feature (e.g. multiple graphs created for each gender or age group). The lattice package also has support for the automatic creation of graph legends, etc., which often has to be done manually with other packages. The biggest issue this package is probably the fact that it is not based on any formal model, which amongst other things makes it difficult to extend it with additional functionality. There are other popular packages, either for general visualization needs or for specific applications, but we will ultimately focus on the most popular visualization packages of the R language today - the ggplot2 package. The author of this package is already mentioned prof. Hadley Wickham, and the package itself is based on the so-called “grammar of graphics” (which is why it’s called ggplot2, with 2 denoting that the package is primarily used for drawing two-dimensional visualizations). The popularity of this package lies in the fact that it tries to combine the benefits of basic graphing support and the lattice package, while relying of a formal, clearly defined model. The advantage of this approach is that it enables the creation of a wide range of visualizations based on concise and cleary syntax as well as allowing for easy extension with additional functionality. A potential problem is a slightly steeper initial learning curve since it is necessary to first adopt the “logic” of graph creation, or to be more precise to learn the basic principles of the “grammar of graphics”. But once this initial obstacle is overcome, creating high quality visualizations is quick, easy, and effective, as evidenced by the fact that ggplot2 is now one of the most popular data visualization packages that has gone beyond the boundaries of R and is being reimplemented in other programming languages for data analysis (e.g. package ggplot in Python, package gramm in Matlab etc.). 12.3 Grammar of graphics and the ggplot2 package Grammar of graphics gives us the following: principles that enable the creation and interpretation of complex visualizations guidelines to understand what represents a “well-crafted” visualization Just as grammar in linguistics enables the formation of “quality” sentences, so does the grammar of graphics allow to view graphs as a kind of “sentence” whose understanding depends on how to fit the individual components into a clear, understandable whole. But, again just like in “regular” grammar, a sentence can be grammatically correct but still meaningless - in other words, grammar is the basis for quality, but not a guarantee of the same; the meaningfulness and purposefulness of the final result still depends on the creativity and ability of the creator of the “sentence”, i.e. visualization. In order to facilitate the learning of grammar of graphics, which realistically represents the biggest obstacle to mastering the ggplot2 package, it is important to adhere to its basic principle that can be paraphrased in this way - quality visualization is actually a composition of a number of components, each of which has a clearly defined role. Consequently, we should not view the graph as one compact unit, but should try to identify the individual parts and learn how they contribute to the final visualization. These parts are not necessarily the visual components of the graph, that is, the parts that make up the graphics we are looking at, but the building blocks that the visualization system uses to produce the final result. 12.3.1 Aspects of data, aesthetics and geometries To begin with, let’s introduce a simplified grammar model which uses three basic components: data (which we want to visualize) aesthetics (which describe how to map data to graph elements) geometries (which describe how graph elements appear visually) Data is, of course, a key component of the graph. It represents the central thing we want the showcase with the visualization itself. It is also relatively independent of the other components of visualization - we can develop the same principles of visualization and then apply them over different datasets. Nevertheless, the creation of a new graph usually begins with the inspecting the features of the dataset and how they may dictate the direction of the further steps in the visualization process. Aesthetics does not really have to do with its literal interpretation of “set of principles concerned with the nature and appreciation of beauty” but is actually about choosing how to display particular segments of the dataset on a graph. Namely, in order for the visualization of the data to make sense, we must present that information in a visually interpretable way - or in another words, “map the dataset variables to graph elements”. A common principle is to display a variable value as a position on a two-dimensional plane with the help of a Cartesian coordinate system, which segments the plane orthogonally using two axes, called x and y, which in turn represent two “basic aesthetics”. These are not the only aesthetics available to us - there are also color, shape, pattern, etc. One way explain aesthetics is “that what is being explained by a legend beside a graph”; if aesthetics is actually a mapping to a visual component of a graph, the legend of the graph is its inverse - an explanation of what that component actually means. Finally, geometry is actually a description of how to actually draw specific graph elements. For example, if we mapped some dataset variables to the x and y axes, then an individual observation could be represented by a point, which in turn would mean we are using the so-called scatterplot graph which relies on point geometry. We could also decide on the line geometry and present the same data with a line which connects observations (which is a common way to depict time series data). In short, geometry can most easily be tied to that what we colloquially call the “graph type” - we choose to draw a scatterplot, a line graph, a boxplot, a bar chart, a histogram etc. and all this boils down to picking the appropriate “geometry” for our graph. Each geometry has its own parameters that can be either fixed or data dependent. For example, a point has the properties of its position (x and y coordinates in the Cartesian system), color and shape. We can depict all points on a graph by using a circle shape, or a cross, or any other available symbol, but we can also make it so the shape o this point depends on the value of a categorical variable. Furthermore, we can “stack” different geometries so that the same graph ultimately becomes a merged combination of various geometries. Let’s demonstrate all this with an example. For the first batch of examples, we will leverage the mtcars dataset that we received with the basic R language distribution (within the datasets package). Let’s load that dataset into the global environment using the `data¸function. Exercise 12.2 - getting acquainted with mtcars dataset # load the `mtcars` dataframe into the global environment # briefly explore the `mtcars` dataset (glimpse, head ...) # load the `mtcars` dataframe into the global environment data(mtcars) # briefly explore the `mtcars` dataset (glimpse, head ...) glimpse(mtcars) head(mtcars) ## Rows: 32 ## Columns: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,… ## $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,… ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16… ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180… ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,… ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.… ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18… ## $ vs &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,… ## $ am &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,… ## $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,… ## $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,… ## mpg cyl disp hp drat wt qsec vs am gear carb ## Mazda RX4 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## Mazda RX4 Wag 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## Datsun 710 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## Hornet 4 Drive 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## Hornet Sportabout 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 ## Valiant 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 We can see that this data describes the characteristics of 32 (old) cars such as: weight, maximum speed, horsepower, number of cylinders and the like. Since some of the variables in this dataset have a numeric datatype even though a categorical type would be more suitable, before we continue we can first factorize these variables. While we are doing this, we can also demonstrate a few as yet unused parameters and functions which help with the categorization - the labels parameter which allows “relabeling” of categories (according to the numerical and alphabetical order of the original categories) and the ordered function which is simply a shorthand way to convert a numerical variable into a categorical with the natural order of numbers being used as a template for ordering category levels. mtcars$vs &lt;- factor(mtcars$vs, labels = c(&quot;V&quot;, &quot;S&quot;)) mtcars$am &lt;- factor(mtcars$am, labels = c(&quot;automatic&quot;, &quot;manual&quot;)) mtcars$cyl &lt;- ordered(mtcars$cyl) mtcars$gear &lt;- ordered(mtcars$gear) mtcars$carb &lt;- ordered(mtcars$carb) When we create a ggplot2 visualization it often helps to think about the “layers” of the graph. Each layer in some way “overlays” a graph like a transparent sheet, which allows us to place multiple different types of data representations on the same graph (e.g. we show points and then also connect them with a line). Let’s say we’re interested in the relationship between car weight and its fuel consumption. An intuitive way of visualizing would be: car weight (wt) on the x axis of the graph fuel consumption (mpg) on the y axis of the graph Let’s see how to do this with the help of ggplot2 visualization. Note that we will intentionally use the “extended” syntax first - this way of creating ggplot2 graphs is almost never used in practice since there is a much more convenient, concise method, but doing it like this first we can more easily spot some of the essential elements of graph construction which correspond to the components outlined by the grammar of graphics. Let’s now create a scatterplot showing the relationship between the weights and fuel consumption values of cars as described by the mtcars dataset. 12.3.1.1 The first ggplot2 graph ggplot() + layer (data = mtcars, # 1. data mapping = aes(x = wt, y = mpg), # 2. mapping / aesthetics geom = &quot;point&quot;, # 3. geometry stat = &quot;identity&quot;, # ignore for now position = &quot;identity&quot;) # ignore for now The basic graph building function is the ggplot function (notice the absence of the number 2 in the function call! - to simplify things, we will call these graphs ggplot graphs from now on, to mimic the name of the function, not the package). This function initializes an object of class ggplot - ggplot visualizations are actually objects, and what we normally consider to be a graph is only their visual representation. This is where the power of this type of representation actually lies - a graph is something we can change, reshape, extend, and store at will, with the visualization being merely the final by-product of managing this object. We then add “layers” to this object using the layer function. We add a layer to the object with the help of the operator +, which is ubiquitous when creating ggplot graphs. This layer as such has those grammatical aspects that we discussed earlier - data, aesthetics, and geometry. In the call, we see two more aspects of graphic grammar - statistics and position - which we will explain later in this chapter; for now it suffices to say that identity corresponds to ‘leave it as is’, that is, we are not using some additional processing within the visualization process which may be provided by these aspects if needed. Although formally each layer has its own grammatical aspects, there are almost always aspects that are common to all layers (e.g. very often one graph shows just one data set and all layers share the x and y axes). If this is the case with our visualization, then we can define them as such immediately when creating the ggplot object, which then become the default parameters of the subsequent layers we add (although the layers always have the option of overriding these “default” parameters). Furthermore, when we add layers we rarely call the layer function itself; a much more convenient choice is calling one of the many helper functions offered by the ggplot2 package that have intuitive names and pre-set many of the parameters for us. For example, the geom_point function adds a layer that will inherit the default parameters set in the ggplot call and automatically sets the geometry aspect to “point”. In the following example we can see this “shorthand” way of creating a ggplot graph: # first `ggplot` graph, abbreviated graph construction method ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() There is another “simplified” way to create ggplot graphs which leverages the qplot function (from quick plot’*). This function is actually a wrapper that allows us to create ggplot graphs using a syntax which is very similar to the syntax of the default plot function. # first `ggplot2` graph created using the `qplot` function qplot(x = wt, y = mpg, data = mtcars) The main reason for the existence of this function is offering a quick and easy alternative to the programmers used to the default plot function. While this way may seem convenient and straightforward, but is not recommended to be used in the long run, since it shares the same issues as the default plot function - it is good for quick and simple calls, but soon loses the advantage of simplicity when our visualizations become more involved. Now let’s go back to our graph - what if we wanted to add an extra variable on it? For example, we can see that all cars have either 4, 6 or 8 cylinders, and we want to communicate this information of our graph. Would this mean that we need to add a third dimesion to our two-dimensional graph? Not necessarily - we can simply use some of the unused aesthetics so far, such as color, size or shape of the points. Exercise 12.3 - the shape aesthetic # create a `ggplot` graph of the `mtcars` dataset with mappings: x = wt, y = mpg, shape = cyl # use point geometry # what happens if we put `as.numeric(cyl)` instead of the categorical variant? ggplot(mtcars, aes(x = wt, y = mpg, shape = cyl)) + geom_point() ## Warning: Using shapes for an ordinal variable is not advised Exercise 12.4 - the color aesthetic # create the same graph, but instead of the `shape` aesthetic, use the `color` aesthetic ggplot(mtcars, aes(x = wt, y = mpg, color = cyl)) + geom_point() Exercise 12.5 - combining aesthetics # create the same graph, but now for the `cyl` column, combine both the `shape` and `color` aesthetics ggplot(mtcars, aes(x = wt, y = mpg, color = cyl, shape = cyl)) + geom_point () ## Warning: Using shapes for an ordinal variable is not advised By comparing the graphs, we can conclude that the color communicates information much better than the shape, which means that it is often the preferred aesthetic (but not appropriate if our graphs have to be in black and white). Also, notice that we can easily combine two aesthetics over the same variable if we wanted. 12.3.1.2 The labs function We have seen that ggplot automatically creates a legend for its aesthetics and that it names the axes with a variable name ( axes x and y are also “legends” of sorts). If we want to manually name axes and legends, and perhaps add a title to the graph, we can use the labs function, which we also add as a new layer using the following syntax: ggplot(...) + ... + labs (x = &quot;x axis title&quot;, y = &quot;y axis title&quot;, title = &quot;Graph Title&quot;) Let’s try this in the next assignment. Exercise 12.6 - labs function # rename axes and legend of the following graph # and add a suitable title (something that succintly describes what the graph represents) ggplot(mtcars, aes (x = wt, y = mpg, color = cyl, shape = cyl)) + geom_point() ## Warning: Using shapes for an ordinal variable is not advised # rename axes and legend of the following graph # and add a suitable title (something that succintly describes what the graph represents) ggplot(mtcars, aes(x = wt, y = mpg, color = cyl, shape = cyl)) + geom_point () + labs (x = &quot;Weight / 1000 lb&quot;, y = &quot;Consumption / miles per gallon&quot;, color = &quot;Number of cylinders&quot;, shape = &quot;Number of cylinders&quot;, title = &quot;Heavier cars spend more fuel&quot;) ## Warning: Using shapes for an ordinal variable is not advised 12.3.2 Fixed geometry parameters Before proceeding, let us focus on one rather important thing that we have not yet clarified: what if we want to tweak certain parameters of a selected geometry but want to keep them fixed, instead of tying them to a particular aesthetic (i.e. mapping to a particular variable)? Or, specifically - what if we wanted to make a scatterplot which has red points, or “X” shapes - that is, fixed color and shape, rather than dependent on a variable? To achieve this, we simply initialize the parameters inside a function corresponding to the geometry we are adding, and we pay attention NOT to wrap the parameters inside the aes function (which automatically corresponds to “mapping”). The values to which we initialize these parameters have to be meaningful for that aesthetic (i.e. “color” can be set to \"red\" or \"#FF0000\"', \"shape\" to numbers from0to25` which correspond to a specific table of shapes etc.) The appropriate values of each aesthetic can be found in the official documentation. Correct example of changing the “global” color of points: ggplot(mtcars, aes(wt, mpg)) + geom_point(color = &quot;blue&quot;) Example which uses the wrong syntax: #ggplot will do the mapping of the word &quot;blue&quot; to the `color` aesthetic ggplot(mtcars, aes(wt, mpg)) + geom_point(aes(color = &quot;blue&quot;)) Exercise 12.7 - fixed geometry parameters # draw a scatterplot showing the relationship between `wt` and `mpg` # make the points red, their shape to `4` and size to `3` ggplot(mtcars, aes(x = wt, y = mpg)) + geom_point(color = &quot;red&quot;, shape = 4, size = 3) 12.3.3 The aspects of statistics and position Let’s look again at the graph we keep drawing. What if instead of a scatterplot we opted for a different geometry, one that shows the relationship between weight and fuel consumption with connected lines? Try adding a line geometry layer to this graph. You can use the layer function with the geom parameter set to line, but the more popular approach is to use the geom_line helper function, which works similar to the geom_point function. Since we keep recycling the same foundation for our graph, we can first save it to an object before adding layers to it. Exercise 12.8 - adding a line layer # we store the foundation of our graph in a variable called `graph` graph &lt;- ggplot(mtcars, aes (x = wt, y = mpg)) # add the point geometry followed by the line geometry to this object and print it on screen # add the point geometry followed by the line geometry to this object and print it on screen graph + geom_point() + geom_line() We got what we asked for, but the result is not overly successful because this jagged line obscures rather than adds additional information. This type of data representation is popular for time series graph, but does not seem to be the correct choice for this dataset. What would probably be more appropriate for us is a line which showcases the relationship between weight and fuel consumption by simplifying it to a single, “smooth” line, that is, a line that approximates the positions of points that describe weight and consumption, rather than directly describing them. For this we need to get back to the we have met in our very first call to the ggplot function (more precisely, thelayer function). Those are aspects of statistics and position. Statistic is an aspect of the grammar of graphics that performs some additional calculations over a dataset before visualizing it. While we can always manually perform these calculations, usually it’s much more convenient to let them be calculated automatically, especially if said calculations are only for one-time use in our visualization (e.g. a bar chart which shows the frequencies of a categorical variable using the height of columns). Mostly statistics perform some sort of aggregation (although not always). Some of the more commonly used statistics are: count - counting occurrences (for categorical variables) bin - binning occurrences in trays and then counting (for continuous numerical variables) smooth - “smoothing”, i.e. “averaging” using the selected method (usuallylm for linear or loess for curved smoothing) unique - removing duplicates identity - direct mapping, i.e. “leaving it as is” These are just some of the statistics, and more can be found in the documentation. Each statistic has its own helper function that follows the form of stat_&lt;statistic_name&gt; and which creates its own layer related to that statistic. Let us return to our graph of weight vs consumption, but this time instead of adding a layer of linear geometry, we add a layer that will display “smoothing” (i.e. the smooth statistics). This is a good example of using how the statistic aspect works - instead of connecting the points directly with a line, a special function will “average” the values and then connect these averaged values with a (curved or straight) line. To achieve this, we leverage the stat_smooth function, which - if we use default parameters - will create a new layer with a “smoothed” representation of the y variable according to the x variable, using the so-called loess method, while also displaying the confidence intervals. Exercise 12.9 - linear smoothing # add the point geometry to the variable `graph` # then add a linear smoothing layer # use the `stat_smooth` function # with the `method` parameter set to `lm` (linear smoothing) graph + geom_point() + stat_smooth(method = &#39;lm&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Exercise 12.10 - ‘loess’ smoothing # repeat the process but se the smoothing method to `loess` graph + geom_point() + stat_smooth(method = &#39;loess&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; Exercise 12.11 - the group aesthetic # create the same graph again # but while creating a smoothing layer # add the `group` aesthetic and set it to `cyl` # What have we achieved by doing this? graph + geom_point () + stat_smooth(aes(group = cyl), method = &#39;loess&#39;) ## `geom_smooth()` using formula &#39;y ~ x&#39; In the last example we witnessed the so-called group aesthetic in action. It works similar to the group_by function in SQL or dplyr, that is, it performs aggregations (or more generally the statistic aspect) over the subgroups defined by the variable instead of the entire dataset. By using this aesthetic, we can show separate calculations in the graph according for each selected group, like in this example where the smoothing is done for each subgroup of cars which share the same number of cylinders. Certain visualizations will automatically set the “group” aesthetic when we put a categorical variable on some other aesthetic (like the “color” or “shape”). This can interfere with our statistic aspects we want to add, especially if we want to perform them over the entire dataset. In that case the easiest solution is to set the group aesthetic to the number 1, which will be interpreted as “there is just one big group” and will encompass the entire dataset when calculating the statistic aspect. 12.3.4 The relationship between geometry and statistics As a rule, certain geometries naturally use “their” statistics (for example, a bar chart naturally uses the statistics which counts the category frequencies). In practice, this means that when we add “statistical” layers, we can choose to use either the stat or geom helper function and the result will be identical, since both functions use the appropriate default geometry/statistics. For example, geom_bar uses the count statistic by default, and will give the same result as using stat_count which has the geometry set to bar. Which helper functions should we lean toward to then, stat orgeom? In practice, `geom ’functions are used somewhat more often, mostly because of the slightly more consistent syntax of graph construction, but ultimately the choice is irrelevant if the result is the same. Let’s now create a bar chart of the number of cylinders in cars described in the mtcars dataset. For this we willl use the geom_bar helper function and its preset count statistic. Exercise 12.12 - bar chart # draw a bar chart of the `cyl` variable from the `mtcars` table # use either the `geom_bar` or `stat_count` function # draw a bar chart of the `cyl` variable from the `mtcars` table # use either the `geom_bar` or `stat_count` function ggplot(mtcars, aes(x = cyl)) + geom_bar() We can use a similar representation for our numerical variable. Unlike a bar chart, which directly counts the occurencies of different categories, with numerical variables we first need to group values into so-called bins. After counting the number of observations in each bin we build a graph called a histogram. Again, this is done automatically using the bin statistic, and all we need to do is call the geom_histogram function and specify the number of bins. Exercise 12.13 - histogram # draw a histogram of the variable `wt` from the` mtcars` table # group the weights into 4 bins # use the `geom_histogram` function and the `bins` parameter ggplot(mtcars, aes(x = wt)) + geom_histogram(bins = 4) How does the statistical aspect work “under the hood”? As a rule, one or more new (most commonly aggregated) variables are calculated based on existing variables and then temporarily added as additional columns in the data frame. In the documentation we can find specific information about the names of these new variables and their meaning. For example, if we look at the documentation for the stat_bin function, we can see that it creates the variables named count, ncount, density and ndensity. Any of these variables can be used as a height attribute for our category columns, i.e. as a value to put on the y aesthetic. The reason why we did not explicitly state this aesthetic in the previous example is the fact that the statistics function automatically selects the aggregate function that is expected to be one most commonly used (in our case it was count). If we want to use a different aggregation variable, we can explicitly set the y aesthetic to one of the other variables, but we need to leverage the ggplot2 convention where we add a prefix and suffix .. to the names of such variables, like in this example: aes (x = hp, y = ..density ..) Let’s try this in the following assignment. Exercise 12.14 - explicitly specifying the aggregation variable # draw a histogram of the variable `wt` from the` mtcars` table # group the weights into 4 bins # use the `geom_histogram` function and the `bins` parameter # set `ncount` as the aggregation variable ggplot(mtcars, aes(x = wt, y = ..ncount..)) + geom_histogram(bins = 4) Let’s go one step further. Draw the same histogram (with default aggregation), but show how many cylinders are represented in each weight category. You can easily do this by adding a fill aesthetic to the geometry which will “fill” the columns with chosen color (as opposed to the color aesthetic that would paint the lines around a rectangles which represent columns in the bar chart). Exercise 12.15 - the fill aesthetic # draw a histogram of the variable `wt` # add the `cyl` variable on the `fill` aesthetic ggplot(mtcars, aes(x = wt, fill = cyl)) + geom_histogram (bins = 4) Here we see an example of a “combined” histogram - the histogram function will calculate the for each number of cylinders in each weight bin. One way this could be shown would be a three-dimensional graph, where base plane there would contain weight bins and cylinders, while the third dimension would be reserved for the height of the columns. However the ggplot2 is used for creating two-dimensional graphs, and projecting the aformentioned three-dimensional graph on the two-dimensional plane, would cause the columns to overlap. In order for the results to be effectively displayed on a two-dimensional graph, these columns are effectively “repositioned” so that the column for the individual cylinders is placed on top of one another within the same weight category, resulting in the histogram we see before us. In other words, the histogram function used the aspect of position to affect the visual representation of these variables. Position is an aspect of the grammar of graphics that allows you to “rearrange” the position of a particular aspect of a graph to achieve better representation clarity. In the previous example, we have already observed the “stratification” of columns based on a chosen categorical variable. Under the hood, the function actually used the stack positional aspect which stacks columns one on top of the other. An alternative to this might be to set the position aspect to dodge which would draw the subgroup columns next to each other. Exercise 12.16 - dodge positional aspect # draw the same histogram, but set the `position` aspect ratio to `dodge` ggplot(mtcars, aes (x = wt, fill = cyl)) + geom_histogram (bins = 4, position = &quot;dodge&quot;) Notice that by using dodge our histogram does not look like our initial histogram (with colored in ratios according to the categorical variable), but has allowed us a clearer view of the relationship between the representation of particular categories within each bin. What happens if we “turn off” the positional aspect by using the identity value? Exercise 12.17 - identity positional aspect # draw the same histogram, but set the `position` aspect ratio to `identity` # also set the `alpha` geometry parameter to `0.2` ggplot(mtcars, aes (x = wt, fill = cyl)) + geom_histogram(bins = 4, position = &quot;identity&quot;, alpha = 0.4) As we can see, the positional aspect of identity means “without repositioning”. The result is the pure projection of the aforementioned three-dimensional graph, which can only be seen by making the columns “transparent” and is not reallzy that useful to us compared to representations which use appropriate repositioning. Let’s show another positional aspect - fill (don’t confuse this with the fill aesthetic!). Exercise 12.18 - fill positional aspect # draw the same histogram, but set the `position` aspect ratio to `fill` # for better visibility, frame the rectangles with a black line # explain the result. What have we achieved with this histogram? ggplot(mtcars, aes(x = wt, fill = cyl)) + geom_histogram(bins = 4, color = &quot;black&quot;, position = &quot;fill&quot;) Another example of positional aspect is the addition of “noise” to the observations on a scatterplot to avoid overlap. By adding the jitter positional aspect, we can better visually communicate that there are more observations than it may appear. Jitterings is effective for smaller datasets; for larger we will often get better results by tweaking the transparency parameter, reducing the size of dots, or sampling the set before visualization. To demonstrate the jittering aspect in action, we will create an “artificial” data frame of 100 partially overlapping observations. Exercise 12.19 - jitter positional aspect df &lt;- data.frame( x = c(rep(1, 90), rep(2, 9), 3), y = c(rep(1, 70), rep(2, 25), rep(3, 5))) # create a scatterplot of the `df` data frame # create a scatterplot of the `df` data frame ggplot(df, aes(x = x, y = y)) + geom_point() Exercise 12.20 - jitter positional aspect (2) # display the same graph, but instead of using `geom_point` use `geom_jitter` # set `width` and` height` parameters to 0.3 (30% added noise) # additionally set the `color` geometry parameter to &#39;blue&#39; # and `alpha` parameter (transparency) to 0.4 ggplot(df, aes(x = x, y = y)) + geom_jitter(width = 0.3, height = 0.3, alpha = 0.4, color = &quot;blue&quot;) 12.3.5 Storing a graph to a file We often need to save our graph to a file so that it can be easily embedded in another report document, scientific paper, forwarded by email, etc. By default, R uses the screen as a “graphical device”. Optionally, we can “redirect” the graphics to another “device”, usually a file of a certain type (png,tiff, pdf, etc.). The list of all options can be viewed with the command ?Devices. The png andtiff formats are recommended for storing graphs in raster format, while it is common to use pdf for vector format. Saving graphs can be done by calling the function corresponding to the format in which we want to store the image (eg the pdf function will save the next image to a pdf file), but the ggplot2 package offers a more convenient way - the ggsave function will store the last drawn graph into a file of the selected name, whereby the image format will be deduced from the file extension we choose. This method is better because we have the chance to see the graph first and then decide on storage. Exercise 12.21 - saving graph to file # save the following graph in the files `figures1.pdf` and` figures1.png` ggplot(mtcars, aes(x = hp, y = mpg, col = as.factor (cyl))) + geom_point() + geom_smooth (aes(x = hp, y = mpg), method = &#39;loess&#39;, linetype = 4, color = &quot;gray&quot;, se = F, inherit.aes = F) + labs(x = &quot;horsepower&quot;, y = &quot;fuel consumption&quot;, col = &quot;number of cylinders&quot;) ## `geom_smooth()` using formula &#39;y ~ x&#39; ggsave(&quot;figure1.pdf&quot;) ggsave(&quot;figure1.png&quot;) 12.3.6 Aspects of scale, coordinate system and theme We have learned so far that creating ggplot graphs often boils down to mapping the dataset columns to the aesthetics of the graph. Scales are an aspect that controls the specifics how this mapping is performed, that is, the method of mapping the data itself to the visual elements of aesthetics. In the case of coordinate axes, it is a matter of mapping numerical or categorical values to specific distances on the axes themselves, while for for the color aesthetic, the scale decides which color corresponds to which value of the original data. The scale aspect is also the basis for creating a legend for the graph. This aspect has always been implicitly present so far, but we have allowed the ggplot2 package to choose the default values for us. In the general case, ggplot2 controls the aspect of the scale relatively well regardless of the aesthetics we use, but very often we want to be able to, for example, change the range of values contained in the graph, labels on axes, colors or shapes used etc. In this lesson, we will focus only on the scales that are relatively common in practice. Other functions and options related to the aspect of scale can be found in the official documentation. As with the aspects of geometry and statistics, when working with the scale aspect, we most often use the helper functions which look like this (* represents the name of aesthetics, such as x,y, color etc.): scale_*_continuous - for mapping continuous (numeric) values scale_*_discrete - for mapping discrete values Each of these functions has a number of parameters that we can use to influence the mapping process. For example, if we look at the documentation for scale_x_continuous we can see that we can set parameters for, among other things: name - the name of the scale which also becomes the name of the axis / legend breaks - the positions where ticks are placed labels - the labels which will be printed below the ticks limits - the range of values that will be on the axis etc. It is important to note that ggplot2 has a lot of additional helper functions that allow us to adjust some parts of the scale aspect, such as the already seen labs function for renaming the graph title and labels on the x and y axis. Another example are also xlim and ylim which only affects the axis range, etc. It is often worth to check out the documentation, and pick and choose the functions which will help our work the most by shortening the syntax for the visualization tasks we perform most often. Let’s try to leverage the scaling aspect in the following assignments. We will use a new dataset - diamonds from the ggplot2 itself. This dataset describes various features of diamonds along with their estimated value. Exercise 12.22 - getting acquainted with diamonds dataset # briefly explore the `diamonds` data frame # briefly explore the `diamonds` data frame glimpse(diamonds) head(diamonds) ## Rows: 53,940 ## Columns: 10 ## $ carat &lt;dbl&gt; 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22, 0.23, 0.… ## $ cut &lt;ord&gt; Ideal, Premium, Good, Premium, Good, Very Good, Very Good, Ver… ## $ color &lt;ord&gt; E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J, J, J, I,… ## $ clarity &lt;ord&gt; SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, SI1, VS1, … ## $ depth &lt;dbl&gt; 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1, 59.4, 64… ## $ table &lt;dbl&gt; 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, 54, 62, 58… ## $ price &lt;int&gt; 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 339, 340, 34… ## $ x &lt;dbl&gt; 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87, 4.00, 4.… ## $ y &lt;dbl&gt; 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78, 4.05, 4.… ## $ z &lt;dbl&gt; 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49, 2.39, 2.… ## # A tibble: 6 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 This dataset has around 54,000 observations which might slow down the drawing of our graphs somewhat. Let’s reduce it by sampling 5000 random observations. Exercise 12.23 - sampling the diamonds data frame set.seed(1001) # sample 5000 random rows from `diamonds` data frame # and store them in a data frame called `diamondsSample` set.seed (1001) # sample 5000 random rows from `diamonds` data frame # and store them in a data frame called `diamondsSample` diamondsSample &lt;- sample_n(diamonds, 5000) Let’s now take a scatterplot which shows the diamonds` volume, price and color and fine-tune its aesthetics with the help of the scale aspect. 12.4 TU Exercise 12.24 - tweaking the scaling aspect of aesthetics # &quot;fix&quot; the axes and the legend of the graph # - call the x and y axes &quot;volume in mm3&quot; and &quot;price in $&quot; # - call the color legend &quot;color quality&quot; # - limit x axis from 0 to 500 # - set the y axis breaks on 1000, 5000, 10000 and 20000 # - instead of D-J label the color &quot;quality&quot; as numbers from 1 to 7 ggplot(diamondsSample, aes (x * y * z, price, color = color)) + geom_point (alpha = 0.4) ggplot(diamondsSample, aes(x * y * z, price, color = color)) + geom_point(alpha = 0.6) + scale_x_continuous(name = &quot;volume in mm3&quot;, limits = c (0, 450)) + scale_y_continuous(name = &quot;price in $&quot;, breaks = c (1000, 5000, 10000, 15000)) + scale_color_discrete(name = &quot;color quality&quot;, labels = 1:7) ## Warning: Removed 3 rows containing missing values (geom_point). Notice that ggplot will report if some observations are not shown due to unavailable data (NA values in variables we want to depict). To prevent this warning, it is sufficient to add the argument na.rm = T to the geometry layer. It is very common to notice “an exponential trend” in our data - in other words, that the relationship between two variables reminds us of an exponential function. The previous graph is also a lesser example of such a scenario - it can be observed that the price of diamond initially only “slightly” increases with the diamond volume, and later it starts to rise more steeply. In such cases data analysts often make a decision to transform the data - for example, in our case we could use the natural logarithm of the price instead of its absolute value, to try and turn the exponential trend into a linear one, which we generally prefer. However, the transformation of price into logarithms carries with it a problem of interpretation - a graph is easier to interpret if it contains original data values instead of derived ones (1000 dollars versus 3 “dollar logarithms”). Scales can help us in this case - instead of changing the data, we simply switch to a different scale. Specifically, instead of the scale_*_continuous function we opt for: scale_*_log10 - uses a logarithm scale (base 10) scale_*_reverse - flips the scale from right to left (or down to up) scale_*_sqrt - uses square roots of values Exercise 12.25 - logarithmic scale # change the y axis to a logarithmic one ggplot(diamondsSample, aes (x * y * z, price, color = color)) + geom_point (na.rm = T, alpha = 0.6) + scale_x_continuous (name = &quot;volume in mm3&quot;, limits = c (0, 450)) + scale_y_continuous (name = &quot;price in $&quot;, breaks = c (1000, 5000, 10000, 15000)) + scale_color_discrete (name = &quot;color quality&quot;, labels = 1: 7) # change the y axis to a logarithmic one ggplot(diamondsSample, aes (x * y * z, price, color = color)) + geom_point (na.rm = T, alpha = 0.6) + scale_x_continuous (name = &quot;volume in mm3&quot;, limits = c (0, 450)) + scale_y_log10 (name = &quot;price in $&quot;, breaks = c (1000, 5000, 10000, 15000)) + scale_color_discrete (name = &quot;color quality&quot;, labels = 1: 7) One thing we often like to “fix” on graphs is colors - whether we want to better emphasize the information that the graph conveys, better fit the graph into the environment we would ultimately place it with, or simply want the graph to use colors that we find more aesthetically pleasing. This is why the color andfill aesthetics are very often further adjusted by scales. We have a good set of helper functions for this (we give examples for the fill aesthetics although similar functions also exist for color): scale_fill_brewer - selecting one of the pre-prepared color palettes for displaying discrete values; pallete names can be viewed in the documentation scale_fill_distiller - adjusts palettes for discrete values to continuous variables scale_fill_gradient - select start and end colors that will then “blend”into each other; we useit to display continuous values scale_fill_gradient2,scale_fill_gradientn - same as above but with more colors scale_fill_grey - for black and white visualizations Exercise 12.26 - color adjustment on graphs # adjust the `fill` aesthetic of the next graph by using # the `scale_fill_brewer` function # Set the `pallete` parameter to one of the following pallets: # `Blues`, `BuPu`, `Greens`, `Grays`, `Oranges`, `OrRd`, `PuBu`, # `PuRd`, `Purples`, `YlGn`, `YlOrRd` # (more palletes can be found in the documentation) ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(0, 500) # adjust the `fill` aesthetic of the next graph by using # the `scale_fill_brewer` function # Set the `pallete` parameter to one of the following pallets: # `Blues`, `BuPu`, `Greens`, `Grays`, `Oranges`, `OrRd`, `PuBu`, # `PuRd`, `Purples`, `YlGn`, `YlOrRd` # (more palletes can be found in the documentation) ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(0, 500) + scale_fill_brewer(palette = &quot;Greens&quot;) # adjust the `fill` aesthetic of the next graph by using # the `scale_fill_brewer` function # Set the `pallete` parameter to one of the following pallets: # `Blues`, `BuPu`, `Greens`, `Grays`, `Oranges`, `OrRd`, `PuBu`, # `PuRd`, `Purples`, `YlGn`, Y`lOrRd` # (more palletes can be found in the documentation) ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(0, 500) + scale_fill_brewer(palette = &quot;YlOrRd&quot;) The coordinate system aspect is somewhat more rarely used than other grammar of graphic concepts we covered. The reason is that in most cases we want to use Cartesian coordinate system which is used by default. If we feel that our visualization requires something else - whether it is a polar coordinate system, a “sideways” Cartesian system aside, or - which is especially important when analyzing geographic data - a map representation of our data, we can use the following functions: coord_polar - polar coordinate system coord_flip - changes the x and y axes coord_map - uses maps from maps and mapproj packages Unfortunately, there are currently no maps of the Republic of Croatia yet, but more ambitious readers can try to create one (and share with the local R community) by following the instructions at this and this links. Exercise 12.27 - inverted and polar coordinate systems # change the coordinate system to &quot;inverted&quot; and then to &quot;polar&quot; for the following graph ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(40, 100) # change the coordinate system to &quot;inverted&quot; and then to &quot;polar&quot; for the following graph ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(40, 100) + coord_polar() # change the coordinate system to &quot;inverted&quot; and then to &quot;polar&quot; for the following graph ggplot(diamondsSample, aes (x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(40, 100) + coord_flip() One thing to remember is that the xlim and ylim parameters work differently depending on whether we use them in terms of scale or the coordinate system; limiting the axis in aspect of the scale will “throw out” observations beyond the specified interval, which can affect the calculation of summary statistics, smoothing curves, etc. On the other hand, the limits given in aspect of the coordinate system (as parameters of the function coord_cartesian() will simply “zoom in” the graph to the selected area but will retain all observations when it comes to calculations. Both options are useful, depending on whether we want a graph that displays all the relevant data for a given visualization or a graph that represents an enlarged segment of another graph. Finally, the theme aspect allows us to influence all visual aspects of the graph that are not related to the data. This means we can choose the background color and layout, font type and size, margins, alignments, and many other graph parameters. The theme aspect gives us extremely detailed control over the appearance of the graph, and since the theme itself is actually an object (of class theme), it can be stored in a variable and then easily recycled for all subsequent visualizations. Likewise, ggplot2 offers a number of predefined themes for use and further customization, which we easily retrieve with a set of helper functions, some of which are: theme_gray - default theme theme_bw - black and white axes, projector-friendly theme_classic - “classic” theme similar to the one produced by the default plot function theme_void - “empty” theme Exercise 12.28 - the aspect of theme # change the theme of the following graph to `theme_classic` ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim (0, 500) # change the theme of the following graph to `theme_classic` ggplot(diamondsSample, aes(x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + xlim(0, 500) + theme_classic() Often we want to change only some of the theme-related aspects (i.e. size or orientation of letters, appearance of ticks on axes, etc.). For these things we use the theme function, which contains a very rich set of parameters easily found by checking out the documentation. Some of these parameters are objects called “theme elements” (e.g. element_line,element_text) that we set by calling the associated function within the theme function call, for example: # changing the visual appearance of the graph title # (recommended for font family to use # `serif`,` sans` or `mono`) ... + theme(title = element_text(family = &#39;serif&#39;, face = &#39;bold.italic&#39;)) Exercise 12.29 - modifying theme elements # change the orientation of the letter on the x axis # so that they are at an angle of 45 degrees # search for help in the official documentation if needed ggplot(diamondsSample, aes(cut)) + geom_bar() ggplot(diamondsSample, aes(cut)) + geom_bar() + theme (axis.text.x = element_text(angle = 45)) 12.4.1 Conditional (faceted) graphs We have previously mentioned the group aesthetic which, prior to visualization within a dataset, groups data into subsets defined by the selected grouping variable and then visualizes them accordingly. We have also learned that we can do implicit groupings using aesthetics of color, shape, size, etc. What is common to these principles is that visual separation, depending on a variable, is performed inside the same graph. In other words, we can use different aesthetics to represent several different visualizations within a single graph. Conditional (faceted) graphs work on the same principle, but separation by a selected variable (or variables) is done by visualizing multiple graphs which are then displayed side by side. The resultant graphs present the same information as the aesthetics of the (implicit or explicit) group, but in a way which makes it somewhat easier to study and compare each subgraph separately. Before demonstrating how to create conditional graphs, we need to perfom a brief detour and explain the term called “statistical formula notation”. This notation is often used in R, especially when training various statistical models, and is actually a way to concisely write a particular, formula-based relationship between variables of the dataset, and then incorporate it into the program code. The formulas will be discussed in more detail later, and for now we will restrict ourselves to a handful of very simple examples. If we want to write a simple formula of “y depending on x,” then the final formula looks like this: y ~ x # means &quot;y depending on x&quot; we can also read this as “y as a function of x” or - in the case of linear models - a more concise notation of y = ax + b. Hence, we read the tilde sign (~) as “depending on”. Here are some more simple formulas: # z depending on x and y (`+` means &quot;and&quot;, not arithmetic sum!) z ~ x + y # y depending on &quot;all other variables&quot; y ~ . # &quot;all other variables&quot; depending on y . ~ y # so-called &quot;one-sided&quot; formula, shorthand for &quot;depending on y&quot; ~ y The last two examples are a little harder to define mathematically, but are often used in function calls for different purposes, mostly because of their simple notation and easy interpretation. Let us now return to the conditional graphs. There are two basic ways to create conditional graphs, by using the functions: facet_grid - for organizing subgraphs into a grid (matrix) facet_wrap - for organizing subgraphs into one or more rows We commonly use the facet_grid function when we do subsets according to one or two categorical variables. Splitting two variables naturally does a “matrix” while splitting them one by one will make either a row or column, the choice we control by choosing a specific formula. On the other hand, we use the facet_wrap function when we want to create subgraphs only according to a single variable, but (due to a potentially larger number of categories) we want the subgraphs to span multimple rows (similar to how “word wrap” breaks sentences in text editors). Exercise 12.30 - facet_grid function # let&#39;s reduce the number of levels first to make the visualization simpler diamondsSample %&gt;% filter(color %in% c(&quot;G&quot;, &quot;H&quot;, &quot;I&quot;, &quot;J&quot;), cut %in% c(&quot;Very Good&quot;, &quot;Premium&quot;, &quot;Ideal&quot;)) -&gt; diamondsSample2 # turn the following graph into a conditional graph # splitting graphs by all combinations of # colors (`color`) and cuts (`cut`) # use the `facet_grid` function and # an appropriate statistical formula as its parameter ggplot(diamondsSample2, aes(depth, fill = clarity)) + geom_histogram(bins = 5, position = &#39;dodge&#39;) ggplot(diamondsSample2, aes(depth, fill = clarity)) + geom_histogram(bins = 5, position = &#39;dodge&#39;) + facet_grid(color ~ cut) Exercise 12.31 - facet_grid function (2) # repeat the same as above, but now group only by color # arrange subgraphs into a column # use the `facet_grid` function and formula notation with a dot ggplot(diamondsSample2, aes(depth, fill = clarity)) + geom_histogram(bins = 5, position = &#39;dodge&#39;) ggplot(diamondsSample2, aes(depth, fill = clarity)) + geom_histogram(bins = 5, position = &#39;dodge&#39;) + facet_grid(color ~ .) Note one important feature of conditional graphs - the coordinate axes (ie scales) are aligned. Specifically, before visualization, ggplot first check the scales of subgraphs in order to find the maximum range, which is then used as a common feature of all graphs. This is usually a preferred, feature, but if we do not want it, we can “free” a particular axis (or both) by setting the scales parameter free (for both axes) or free_x / free_y (if we only want one axis aligned and the other “free”). Let’s now take a look at how the facet_wrap function works: Exercise 12.32 - facet_wrap function # break the following graph into subgraphs based on transparency (`clarity`) # use the `facet_wrap` function and a one-sided formula ggplot(diamondsSample2, aes(x * y * z, price)) + geom_point() # break the following graph into subgraphs based on transparency (`clarity`) # use the `facet_wrap` function and a one-sided formula ggplot(diamondsSample2, aes(x * y * z, price)) + geom_point() + facet_wrap(~clarity) 12.5 Graphs in Exploratory Analysis and Reporting Before we finish the chapter, we can briefly look at the difference between graphs made for exploratory data analysis and those we use in reports, to communicate our conclusions to other people, and to share our findings. Exploratory data analysis often aims for efficiency and quantity. It has already been said that exploratory analysis comes down to answering a series of questions that an analyst asks regarding data. How pretty the graph looks is often irrelevant - the main thing is that graphs have enough information to be able to answer the questions being asked and offer foundations for setting up various hypotheses. The analyst will also often try and experiment with different combinations of aesthetics, geometries and statistics. Sometimes it is convenient to go beyond what ggplot2 package offers and use additional packages that are more suitable for what the analyst wants to achieve. For example, the ggpairs function of the GGally package allows pairing of multiple variables of a data frame and creating a matrix of single and two-variable graphs: #library (GGally) #if necessary # We only take four columns for a clearer view ggpairs(data = diamondsSample[, c (1, 2, 5, 7)]) Exploratory graphs are used by the analyst to visualize the data for themselves, to look for patterns, to recognize distributions or natural groupings of data, or to spot outliers. Likewise, exploratory graphs are often used to quickly gain insight into various aggregate statistics. In the case of predictive data analysis, the analyst will often create some simpler predictive models during the exploratory analysis, which they will then visualize on the graph to be able to more accurately evalute the models’ performance, and based on the identified shortcomings, develop a strategy for further steps of the analysis. For reporting, on the other hand, the quality of the graph is crucial, in terms of presenting information clearly and transparently. The graph must be easy to readily interpret, with carefully chosen explanations and selected use of the added metadata, most commonly in the form of additional text and annotations. To create such graphs, it is recommended to use additional geom_text layers for text tags in the corresponding areas of the graph where they can contribute most, as well as creative use of geom_point, geom_hline,geom_vline, geom_rect and similar geometric layers that will further clarify certain segments of the graph. It is also recommended to prepare a specific graph theme object in advance, which will then be consistently applied to all graphs. In many cases, the graphs used in reporting are a subset of chosen and “beautified” graphs obtained during exploratory analysis. However, the analyst should pay particular attention to the fact that graphs in reports are often made for an audience that is far less familiar with the dataset and the various in-depth knowledge that the analyst has obtained during exploratory analysis. Reporting graphs must therefore be end-user oriented and carefully designed for this purpose. Therefore, it is recommended that all elements of the graph - including the title, legends, etc., be oriented to the communication of information and clarification of what the graph is showing, which is in line with the conclusions drawn by the publication the graph is a part of. Sometimes, due to a lack of space or simply as a presentational choice, we want to put more different graphs inside a single image. We usually do this by storing separate graphs each in its own image file and then collating those images within the interface we use to write a publication containing those graphs. However, we can also combine graphs in advance using the gridExtra package. Among other things, this package offers the grid.arrange function with the help of which graphs are placed into a matrix with the selected number of rows and columns. #library (gridExtra) # if necessary # we first store graphs into separate variables g1 &lt;- ggplot(diamondsSample, aes (x * y * z, price, color = color)) + geom_point(alpha = 0.6) g2 &lt;- ggplot(diamondsSample, aes (x = x * y * z, fill = color)) + geom_histogram(bins = 30, na.rm = T) + scale_fill_brewer (palette = &quot;Greens&quot;) g3 &lt;- ggplot(diamondsSample, aes (x = cut)) + geom_bar(fill = &quot;blue&quot;, alpha = 0.5) g4 &lt;- ggplot(diamondsSample, aes (x = color, fill = clarity)) + geom_bar() + coord_polar() # and then we call the `grid.arrange` function grid.arrange(g1, g2, g3, g4, nrow = 2, ncol = 2) This concludes the story of the grammar of graphics, its aspects and its application with the help of the ggplot2 package. Not all of the features of this package are fully explained here, which is why it is strongly recommended that you read the documentation further and expand your knowledge beyond what was covered in this chapter. Also, one should not forget about a number of additional packages that further extend the capabilities of the ggplot2 package, which we should look for depending on our requirements and desires regarding the visualizations we want to create for our data analysis projects. For inspiration, it’s convenient to look at the gallery of graphs and images created using the R language, available at this link . Exercises Load the diamonds data frame which comes with the ggplot2 package. Show the diamond price distribution with the aid of two graphs - a histogram and the so-called. frequency polygons (function geom_freqpoly)). Divide the prices into 10 bins. add the “clarity” of the diamond to the graphs from a) (the clarity column) which you will set to the fill aesthetics (‘histogram’) or color aesthetic (frequency polygons). Do you see any difference pertaining to the default position aspect? Load the mpg dataset. Then find the graphs called 2a.pdf, 2b.pdf and 2c.pdf in the folder of this homework assignment. Try to reconstruct them. If you do not recognize the geometry used, try to consult the RStudio ggplot2 cheat sheet. Adjust the scale aspect of the following graph in the following fashion: the x axis should be called \"the number of cylinders\" the y axis should be called \"total\" and should have its range expanded to 100 the legend should be named \"year\" the rectangles color should use the Dark2 palette ggplot(mpg, aes(x = as.factor(cyl), fill = as.factor(year))) + geom_bar() Change the theme of the next graph as follows: use the theme which is suitable for use with a projector turn the x axis labels vertically ggplot(mpg, aes(x = as.factor(trans), y = displ)) + geom_boxplot() The following graph shows the highway miles per gallon consumption histogram, with the color of the rectangle reflecting the number of cylinders. Try to improve the graphs’ interpretability by replacing the aesthetics of color and using multiple graphs that are conditioned by the number of cylinders. Organize the graphs in a 2 x 2 matrix. ggplot(mpg, aes(hwy, fill = as.factor(cyl))) + geom_histogram(bins = 10, position = &quot;dodge&quot;) Assume you have the following data frame: sales &lt;- data.frame(month = 1:12, total = c(10000, 5000, 12000, 3000, 5000, 7000, 10000, 2000, 4000, 8000, 11000, 14000)) and you visualize it with a bar graph. However, the geom_bar function by default uses only one variable and automatically calculates the count statistics. How to solve this problem? Suggest a solution and create the appropriate bar graph. Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ #(PART) Machine Learning and Predictive Analytics {-} "],["regression.html", "13 Selected Machine Learning Methods: Regression Analysis 13.1 Machine learning - a short introduction 13.2 Simple linear regression 13.3 Multiple linear regression Homework exercises", " 13 Selected Machine Learning Methods: Regression Analysis 13.1 Machine learning - a short introduction Machine learning is a field of computer science that deals with a specific type of programming in which we do not give explicit instructions to the computer, but rather expect the computer to come to certain insights on its own based on selected datasets and a particular “learning” method. Machine learning is often broadly divided into so-called supervised learning, where we have clearly defined inputs and outputs(goals), and unsupervised learning, where we do not have predefined outputs, but expect the computer to analyze only they enter to come up with some useful insights into the data itself. There are additional machine learning disciplines such as reinforced learning, anomaly detection, etc. which we will not further explain as they fall outside the scope of this textbook. The term machine learning is often associated with the term “data mining” and “knowledge discovery from datasets”(KDD - knowledge discovery from data). It is difficult to determine the extent to which these terms overlap and what their exact definitions are, since various conventions that are not mutually consistent are often encountered in the literature. For the purposes of this tutorial, machine learning can be considered as a kind of “toolset” that we can use to discover(mine) useful information from datasets. The result of this process gives us some knowledge of the domain we can use to make certain decisions. In this lesson, we will focus on one very commonly used machine learning method - linear regression - with emphasis on its use in the programming language R. Linear regression is an extremely popular method of machine learning that, with the help of mathematical and statistical foundations, describes the potential linear relationship of dataset variables. If sufficient evidence is found that a linear relationship exists, then we have a potentially useful insight into the actual relationship of the two(or more) variables. Likewise, the resulting “formula” can be used to estimate the value of another(target) variable based on the known value of one variable(predictor). Although more advanced predictive methods exist today, linear regression is still often used both for its simplicity and for its fact, understanding linear regression lays a solid foundation for learning and understanding more advanced machine learning methods. Because of this, linear regression is a logical and very important first step when entering the field of predictive analysis. 13.2 Simple linear regression Simple linear regression is a method of supervised machine learning for predicting a target numeric variable using a linear function of the input variable. In this way, the creation of a predictive model is reduced to the process of determining the direction coefficient of direction and intercept value, which will form a simple formula for calculating the target variable using the input parameter. Since this method boils down to estimating the above parameters, the linear regression method belongs to so-called “parametric methods” of machine learning, i.e. predictive analysis. Parameters of simple linear regression can often be determined mathematically. The most commonly used method of determining the direction and section coefficient is the least squares method, which draws a line between the points of the graph so that the sum of squares of the distance between estimations and real values is minimal (these distances are called “residuals”). Using a mathematical procedure, we can derive a formula that will calculate these parameters exactly, provided we have sufficient number of observations. The motivation to perform simple linear regression is often found during the process of exploratory data analysis, especially during the visualization of two numerical variables by using scatterplots. If one of these variables is of interest to us as the target variable of the predictive model, and putting another variable on scatterplot results in a shape resembling a line, then this other variable is an obvious candidate for the simple linear regression method. In the following exercise, we will use an “artificial” data frame in which we have an input variable x and four possible target variables y1, y2, y3 and y4. Each of these variables is created by a specific input transformation with the addition of a certain amount of noise. The idea of the assignment is to study the relationship between input and possible output variables and to identify which of these relationships is a good candidate for simple linear regression. Exercise 13.1 - suspecting potential linear relationships # load data from the `data1.csv` file # in a variable called `df` # draw four scatterplots of relationships # between variable `x` and every variable `y` in the data box above # add a smoothing line to each graph using the `lm` method # answer the following questions: # in which graphs do you see a possible linear relationship between the variables? # which graphs show a nonlinear relationship? # for which graph would you say the variables are independent? df &lt;- read.csv(&quot;data1.csv&quot;, stringsAsFactors = F, encoding = &quot;UTF-8&quot;) g1 &lt;- ggplot(df, aes(x, y1)) + geom_point() + geom_smooth(method = &#39;lm&#39;) g2 &lt;- ggplot(df, aes(x, y2)) + geom_point() + geom_smooth(method = &#39;lm&#39;) g3 &lt;- ggplot(df, aes(x, y3)) + geom_point() + geom_smooth(method = &#39;lm&#39;) g4 &lt;- ggplot(df, aes(x, y4)) + geom_point() + geom_smooth(method = &#39;lm&#39;) grid.arrange(g1, g2, g3, g4) ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; ## `geom_smooth()` using formula &#39;y ~ x&#39; Visualizing the relationship between two variables can usually provide us with relatively good intuition when we can expect a linear model to work relatively well. If we want to numerically describe the power of the linear relationship of two variables, we can use a so-called “Pearson correlation coefficient” which will always the value from the interval [-1,1], where -1 means complete “negative” correlation, 1 complete positive, and 0 that there is no correlation. In R, we can get this coefficient easily with the help of the cor function. Let’s calculate the correlation coefficient for all the pairs of input and output variables shown in the graphs from the previous exercise. Exercise 13.2 - calculating the correlation coefficient # calculate and print the Pearson&#39;s correlation coefficient # between all pairs visualized in the previous exercise cor(df$x, df$y1) cor(df$x, df$y2) cor(df$x, df$y3) cor(df$x, df$y4) ## [1] 0.9758326 ## [1] 0.6765991 ## [1] -0.04977038 ## [1] 0.1783745 In addition to this measure, to describe the power of a linear relationship, we often use a variable called the “coefficient of determination”, more commonly known as the “R squared” measure. The name of this variable is derived from the fact that the correlation coefficient for a simple linear model (which we calculated in the previous task) is often called R, and the value of R squared is exactly equal to its square for a simple linear model. This measure can take values between 0 and 1, where a value close to 1 indicates a near-perfect linear relationship while a value close to 0 indicates its absence. The “R squared” measure is actually defined as “the amount of variability explained by the model”. We can simply interpret this as follows - we compare how “scattered” the points are around an imaginary line (of linear regression) compared to their “general scatter” around a horizontal line that passes through their arithmetic mean. It is important to note that the “R squared” is one of the more important criteria for evaluating the quality of a linear model and as such is often contained in the description of model results. Despite this, there is no clear concensus on what constitutes a “good R squared” value - even a model with a small “R squared” can prove useful, depending on other criteria and the specific circumstances where the model is applied. Now let’s show how we create linear models in the R language. 13.2.1 The lm function In R, we create simple linear models using the lm function, which is short for linear model. This function has a number of parameters, and we will use the most important ones - the statistical formula which defines how the model should be trained, and the data set we train on: lm(formula, data) The “formula” is simply a short notation which describes how the left side of the formula “depends” on the right side of the formula. If we want to train a linear regression model where the target variable y depends on the variablex for the data frame df, and save the final model in the variable linMod, then in the code may look like this: linMod &lt;- lm(y ~ x, data = df) Let’s try this on our own in the next task. Exercise 13.3 - creating a simple linear model # Use the `lm` function to create a linear data model for the `df` dataset # where `x` is the input and `y1` is the output variable # Save the result to the `linMod` variable # print the `linMod` variable # Use the `lm` function to create a linear data model for the `df` dataset # where `x` is the input and `y1` is the output variable # Save the result to the `linMod` variable linMod &lt;- lm(y1 ~ x, data = df) # print the `linMod` variable linMod ## ## Call: ## lm(formula = y1 ~ x, data = df) ## ## Coefficients: ## (Intercept) x ## 46.733 3.999 The output of the linMod variable shows us the formula used to create the model and the calculated parameters - direction coefficient and the intercept (which we are generally far less interested in). The direction coefficient is interpreted as follows - if the input variables changes for one measure, the output changes by the amount of the coefficient. The linMod variable from the previous example is an object of class lm. This is a relatively complex object which contains not only the finally calculated coefficients, but also a rich set of information related to the linear model created, which includes even the data set by which the model was created. To get all this information, we can use the following set of helper functions: coef - returns coefficients in the form of vectors fitted.values - returns the prediction vector obtained by applying the model to the set by which the model was created residuals - returns the error vector obtained by applying the model to the set by which the model was created summary - provides a summary of the most important model information predict - applies the model to the new dataset (shown later) Let’s try the summary function onr our linear model. Exercise 13.4 - linear model summary # execute the `summary` function on the trained linear model summary(linMod) ## ## Call: ## lm(formula = y1 ~ x, data = df) ## ## Residuals: ## Min 1Q Median 3Q Max ## -74.010 -15.492 -1.021 15.613 77.371 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 46.73328 5.26089 8.883 3.2e-14 *** ## x 3.99851 0.09045 44.208 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 26.35 on 98 degrees of freedom ## Multiple R-squared: 0.9522, Adjusted R-squared: 0.9518 ## F-statistic: 1954 on 1 and 98 DF, p-value: &lt; 2.2e-16 We see that we have received a wealth of information related to our trained model. Below, we will offer a simple, layman’s explanation on how to interpret this information. To begin with, we note that when creating a linear model from some data, we have several different “uncertainties” in the result: Is there a linear trend at all, or could the observed collinearity occur at random? If there is a trend, how confident are we that the calculated direction coefficient corresponds to the “real” coefficient? Finally, if a trend exists and we have managed to estimate the “true” coefficient well enough, how much does the additional “noise” affect the accuracy of the predictions? The summary below provides an answer to some of these questions. Let’s first look at the columns t value andPr(&gt;|t|), that is, their values concerning our input variable x. They answer the question - do we even have enough evidence to suggest that there is a linear relationship between inputs and outputs? The answer to this question is given by the result of so-called t - test, which a measure of the strength of evidence against the null hypothesis that states that there is no linear relationship at all and that the direction coefficient is actually zero. The value under Pr(&gt;|t|) is a so called “p-value”, i.e. the estimate of the probability that collinearity was observed by chance, and here we see that it is extremely small. The stars next to this value help us to quickly interpret the coefficients for which we are relatively certain to have a linear relationship with the target variable. If we are satisfied with the proof that the direction coefficient is non-zero, then we can also look at the column Std. Error that describes how “confident” we are in the resulting coefficient of direction. Statistically, with the help of the so-called “central theorem”, we believe with 95% certainty that we can say the “real” coefficient is in the interval between two standard errors from the coefficient obtained. The standard error depends on the number of observations in our set - simply stated, the more data we have, the more “confident” we will be in our estimated coefficient. The most interesting information from the summary is the Residual standard error and Adjusted R-squared entries. The “residual” is actually the difference between the prediction given by the model and the actual value observed. The residual standard error is an estimate of how much (on average!) the model “misses” with its predictions of the target variable. Because the residual standard error is expressed in units of measurement of the target variable, it is often very informative to check it out when trying to evaluate the quality of a model. Of course, estimating what we truly consider to be a “acceptable” error depends on the particular use case, i.e. the nature of the target variable, the units of measurement we use, and the desires, that is, the requirements of whoever will actually use the predictive model. For example, if we were trying to estimate the value of a property in millions of euros, a mistake of several thousand euros might not be such a huge deal; on the other hand, if we are trying to predict how much we can get by selling an old car, a mistake in the same absolute amount would probably be unacceptable. The reason why our model “misses” is the so-called “unexplained variability” - everything that we did not measure and which in some way potentially affected the final amount of the target variable. This “noise”, i.e. the impact of unexplained variability, is described by the previously explained R-squared measure (for now, we will not pay too much attention to theAdjusted prefix, which will be discussed further in the lesson). Ideally, our model will have a small standard residual error and a high “R squared” measure- that is, the model will both guess well and contain a very small amount of unexplained variability. In practice, of course, we will not often come across such an ideal scenario, so we will have to evaluate, often individually or in cooperation with domain experts, what we consider to be “good enough”, since statistics here is not able to provide an exact, generally applicable answer. Finally, some other details that are summarized are: “five number summary” (minimum, maximum and averages) of the residuals that helps us to estimate the distribution of errors obtained “F-statistics” that tell us the chance that none of the input variables are affecting the output (this statistic will make more sense when we include more predictor variables) We said that by linear regression we get a “predictive model”, which we can use on new data to create new predictions. How can we use this model to create new predictions? R offers us the generic predict method, to which we commonly give the generated predictive model and a data frame with new data as parameters, taking care that the data frame has columns that match the expected model inputs. Because predictive models often include the set used to create the model, we can actually drop the dataset parameter, in which case the model will simply return the predictions obtained by applying the model on the dataset it was trained on (i.e., the same result that we get by using the fitted.values function). Exercise 13.5 - creating new predictions # the following vector has the &quot;new&quot; values of the input variable `x` new_x &lt;- c(-5, 10, 50, 102) # apply our predictive model on these new variables by using # the `predict` function and`and linear model `linMod` linear model # be sure to wrap new data in the form of a data frame first # calculate predictions &quot;manually&quot; by reading the obtained # coefficients from the linear model # apply our predictive model on these new variables by using # the `predict` function and`and linear model `linMod` linear model # be sure to wrap new data in the form of a data frame first predict(linMod, data.frame(x = new_x)) # calculate predictions &quot;manually&quot; by reading the obtained # coefficients from the linear model coef(linMod)[1] + new_x * coef(linMod)[2] ## 1 2 3 4 ## 26.74074 86.71835 246.65864 454.58101 ## [1] 26.74074 86.71835 246.65864 454.58101 Often, analysts do not want to rely on simple numerical values which describe the resulting model, but want to further evaluate the quality of the model through various visualizations. Analysts are often interested in the behavior of errors, i.e. residuals. Specifically, if the linear model describes the data well, we would expect the points to scatter uniformly around the regression line, without some obvious patterns. Eg. if we look how variable y3 dependes on the variable x in the graph at the beginning of this lesson, we see a clear pattern pattern where the direction first passes above the set of observations, then below and finally again above - which clearly tells us that the linear model poorly explains the relationship between these two variables. So the two questions that an analyst can potentially ask about residuals are: are there obvious patterns in residual behavior with respect to the sequence of original observations and whether the residuals have a normal distribution The first question can be easily answered by creating visualizations which use the following geometries of the ggplot2 package: geom_point - simple scatterplot with predictors on x-axis and residuals on y-axis we expect a “cloud” of residuals with no obvious patterns geom_density - draws the estimated density function of a variable normal distribution of residuals will be bell-shaped, without noticeable “tails” geom_qq - draws the so-called QQ (quantile-quantile graph) the normal distribution in this graph takes the form of a line Before creating these visualizations, we encounter a tricky problem - the ggplot2 package implies the existence of a data frame with the variables we want to display, but we do not have the residuals neatly wrapped in a data frame! Unfortunately, the base R language does not give us an easy way to get that information in the form of a data frame. We can relatively easily assemble it by using functions such as fitted.values to retrieve predictions, residuals to retrieve residuals, and then wrap everything in a new data frame, but these are some boilerplate coding steps that we would like to avoid if possible. The broom package, co-authored by Hadley Wickham, who we already met as developers ofdplyr and ggplot2, comes to our aid. This package offers a number of functions for easily extracting information from our trained models - for example, thetidy function gives us model results packaged in an easily readable data frame, while the glance function does the sam, but for the parameters that describe the quality of the model. Below we will show how a function called augment works. This function is similar to the predict function, but is “augmented” in various useful ways. If we simply give it our predictive model, it will return the original data frame used to create the model, but expanded with a number of useful columns such as: .fitted - Predictions obtained using the model .se.fit - standard prediction error .resid - the amount of residuals, ie errors .std.resid - residuals standardized to intervals [0,1] .hat - a measure of the” extremity “of an input variable of observation(* leverage *); .cooksd - a measure of the” influential point “of an observation on a model Analysts usually appreciate seeing some of these new values since they provide useful insight in how our model performs and where possible issues arise. For example, observations with a high leverage measure are potential outliers and as such deserve additional attention. Particularly problematic are the observations with high “impact” on the model - these are observations that have both a high leverage and a large residual. Observations like these can strongly can often “draw” the regression line towards themselves, worsening the quality of the model. Let’s apply the augment function over our predictive model. Exercise 13.6 - the augment function # apply the `augment` function on the `linMod` model # store the resulting data frame in the `predictions` variable # look at the first few lines from this variable #library(broom) # if necessary predictions &lt;- augment(linMod) head(predictions) ## # A tibble: 6 × 8 ## y1 x .fitted .resid .hat .sigma .cooksd .std.resid ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 380. 77.1 355. 25.4 0.0184 26.4 0.00887 0.972 ## 2 296. 63.3 300. -3.54 0.0120 26.5 0.000111 -0.135 ## 3 439. 86.7 393. 45.2 0.0256 26.1 0.0396 1.74 ## 4 325. 67.8 318. 6.88 0.0136 26.5 0.000475 0.263 ## 5 479. 94.5 425. 54.6 0.0330 25.9 0.0757 2.11 ## 6 160. 24.5 145. 15.2 0.0179 26.4 0.00310 0.583 (NOTE: as stated, we can also use the augment method as an alternative to the generic predict method - we just need to pass it new data using the newdata parameter) Now that we have a data frame that (amongst other things) contains a column which stores the residuals, we can easily create the visualizations we mentioned before. In particular, we will create a scatterplot with predictions on the x axis and (standardized) residuals on the y axis graph which shows the density function of standardized residuals quantile-quantile graph of standardized residuals The reason why we work with standardized instead of “true” residuals is simply easier interpretation, that is, a simpler comparison of results with a “standard” normal distribution having a mean of 0 and a standard deviation of 1. Exercise 13.7 - checking the ‘normality’ of residuals # using the `predictions` data frame # create a scatterplot between fitted values and std. residuals # also draw a horizontal line that goes through zero # create a density graph of the std. residuals # use the `geom_density` geometry function # create a quantile quantile graph ofstd. residuals # use the `geom_qq` geometry function # set residuals to the `sample` aesthetic (not `x`!) #library(gridExtras) # if necessary g1 &lt;- ggplot(predictions, aes(.fitted, .std.resid)) + geom_point() + geom_hline(yintercept = 0, color = &quot;blue&quot;) g2 &lt;- ggplot(predictions, aes(x = .std.resid)) + geom_density() g3 &lt;- ggplot(predictions, aes(sample = .std.resid)) + geom_qq() grid.arrange(g1, g2, g3, ncol = 2) In our (artificially created) example, we have received very convincing arguments that the linear model we have obtained describes the relationship of variables very well and as such is probably a good choice for creating predictions. In actual practice, we often do not get such “clean” results and will have to make a good assessment of whether the model is good enough, whether additional work is needed on the data or the process of building the model, or whether a completely new approach is required. For example, some of the possible conclusions after creating visualizations of residuals might be: if a scatterplot with predictions and residuals shows obvious patterns, it is possible that the linear model is not good for describing the predictor-target relationship and we should try to train model able to describe the more complex nature of the relationship if the residual graph takes the form of a “funnel”, ie if the residuals grow with increasing value of the predictions, it may be necessary to transform the input and/or output variables, for example by using the root or logarithm function if we notice some values that “pop up” strongly in the residual graph, we need to look at them more carefully and potentially remove them from the training set Another thing an analyst can try is to draw a graph of residuals in the order that the observations were obtained in the original set (a scatterplot where the x-axis is the ordinal number of observations and the y is the residual). If a high level of interdependence of residuals close to each other is observed, it is possible that our data actually represents a “time series data”. We should not use linear regression for this kind of data, but rather a more advanced, specialized time series method, such as ARIMA (autoregressive integrated moving average). 13.2.2 Linear regression and categorical variables In the previous section, we presented the process of creating a predictive linear regression model where the input variable was of numerical type. It is justified to ask - can a categorical variable also be an input to a predictive model? The answer is - yes.. but with some adjustment. Let us show how, in a simple example, a two-level categorical variable can be used to train a linear regression model (we will later easily extend this approach to category variables with more than two levels). A categorical variable cannot, by its nature, be a part of a linear equation, since the category itself usually does not have a numerical equivalent that could be meaningfully used to calculate the target variable. However, we can easily turn a categorical variable into a binary (indicator) variable that describes whether a particular observation belongs to the selected category or not (if it does not, then it logically belongs to the second category, which we will call a reference or baseline category). The linear regression will then determine the coefficient that will define the direction by adding the coefficient if the indicator variable is 1, or be ignored if the indicator variable is0. For a two-level categorical variable, one indicator variable is sufficient - the second would simply be an inverse of the first and would not hold any additional information. How many indicator variables do we need for a categorical variable with more than two levels? The answer is logical - one less than the number of categories, since “not belonging” to all but one category necessarily indicates belonging to that one, remaining category. We can mention here that sometimes we can make one indicator variable for each level, if we wish, but then we must make sure that the existence of a single redundant column will not adversely affect the model. Linear regression is one of the methods that is sensitive to such redundant columns, which is why we will avoid them here. Below we present a linear regression with a two-level categorical variable, since this is actually a variant of the simple linear regression. For a larger number of categories, the problem is reduced to the multiple linear regression that we will address later. Let’s now load a new, also artificially created data frame, with one two-level categorical variable x representing input and the target numeric variabley. Exercise 13.8 - sample data frame with a categorical predictor # load data from the `data2.csv` file in the `df2` data frame # examine the loaded data frame # draw a scatterplot of a relationship between `x` and `y` # load data from the `data2.csv` file in the `df2` data frame # examine the loaded data frame df2 &lt;- read_csv(&quot;data2.csv&quot;) df2$x &lt;- factor(df2$x) glimpse(df2) ## Rows: 100 Columns: 2 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): x ## dbl (1): y ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Rows: 100 ## Columns: 2 ## $ x &lt;fct&gt; B, A, B, A, A, B, B, A, A, A, A, A, B, A, A, B, B, B, A, A, B, A, B,… ## $ y &lt;dbl&gt; 70.341025, 13.613923, 70.445313, 16.628283, 31.565885, 88.678272, 73… # draw a scatterplot of a relationship between `x` and `y` ggplot(df2, aes(x, y)) + geom_point() We see that the distribution of the target variable is different for different categories of input variable. We can model this increase by linear regression, although the interpretation of the model will be slightly different compared to the interpretation of the numerical inputs, as we will see after constructing the model itself. One advantage of using factor variables in the R language is that when training linear models, we do not have to “manually” create indicator variables. We just need to put the factors as input variables into the model’s training formula, and R will automatically create indicator variables for us (this also applies to creating predictions once the model is created - provided the factors do not contain “unknown” categories in the new data) . Exercise 13.9 - creating a simple linear model with a categorical input # Use the `lm` function to create a linear data model from the `df2` data frame # where `x` is input and `y` is output variable # save the result to the `linMod2` variable # print out the summary of `linMod2` linMod2 &lt;- lm(y ~ x, data = df2) summary(linMod2) ## ## Call: ## lm(formula = y ~ x, data = df2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -21.610 -7.265 -2.279 6.529 27.337 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 23.711 1.427 16.61 &lt;2e-16 *** ## xB 49.442 2.018 24.50 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 10.09 on 98 degrees of freedom ## Multiple R-squared: 0.8596, Adjusted R-squared: 0.8582 ## F-statistic: 600.1 on 1 and 98 DF, p-value: &lt; 2.2e-16 We see that the summary of the linear model is very similar to the summary already presented where the input variable was of numerical type. The difference in interpretation is as follows - the direction coefficient is related to a specific category (seen in the name of the variable), and refers to the expected difference in the amount of the target variable when the observation has the specified category, relative to the reference category. To conclude this section, let us emphasize only that when using categorical variables as inputs to the linear model, it is important to take into account the representation of categories, that is, we should take care to not have categories that are very poorly represented in the training dataset. The reason is that such observations very often have a great influence on the regression direction, which can have adverse effects on the quality of the linear model. 13.3 Multiple linear regression The principle of simple linear regression is easily extended to a scenario when we have multiple input variables - simply, we are looking for a function that will express the target variable as a linear combination of input variables. The problem of building a model again comes down to finding the “good” direction coefficients that will be given to each input variable (plus the intercept), although formally we cannot talk now about the “regression line”, but rather must a somewhat more complex notion of a “hyper-plane.” Multiple linear regression presents a number of additional challenges that we must face, but when it comes to training the model we still use the already familiar lm function, to which we simply forward the desired formula which expresses the desired relationship between the target and predictors, for example: y ~ x1 + x2 # `y` as a linear combination of `x1` and `x2` y ~ . # `y` as a linear combination of all other variables y ~ . - x1 - x2 # `y` as a linear combination of all other variables EXCEPT `x1` and `x2` log(y) ~ x1 + log(x2) # natural logarithm of `y` as a linear combination of `x1` and natural logarithm of `x2` y ~ x1 + I(x2 ^ 2) # `y` as a linear combination of` x1` and a square of `x2` Note that the formula can also contain transformed inputs (and outputs!). We have to be careful here, because sometimes mathematical notation “clashes” with formula notation (for example, we see that - is not really an arithmetic subtraction but a method of determining an undesirable variable). Therefore, in the last example, we used the I function, which simply indicates that the expression between parentheses should be treated “as is”. The presented transformations of inputs and outputs are moving away from simpler linear regression models with multiple inputs, so we will not use them below. Here, we list them only for reasons of completeness and as a motivation to try more complex regression formulas in situations where the need arises for such an approach. Now let’s try to create a predictive model with multiple input variables. In the assignment, we will use the familiar mtcars dataset (this time we will use a version which comes shipped with our R distribution). data(mtcars) # factorize the `vs` and` am` columns cols &lt;- c(&quot;vs&quot;, &quot;am&quot;) mtcars[, cols] &lt;- lapply(mtcars[, cols], factor) # quicker way to factorize columns by name glimpse(mtcars) ## Rows: 32 ## Columns: 11 ## $ mpg &lt;dbl&gt; 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,… ## $ cyl &lt;dbl&gt; 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4, 4, 4, 8,… ## $ disp &lt;dbl&gt; 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7, 140.8, 16… ## $ hp &lt;dbl&gt; 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180, 180, 180… ## $ drat &lt;dbl&gt; 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3.92, 3.92,… ## $ wt &lt;dbl&gt; 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190, 3.150, 3.… ## $ qsec &lt;dbl&gt; 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00, 22.90, 18… ## $ vs &lt;fct&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,… ## $ am &lt;fct&gt; 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,… ## $ gear &lt;dbl&gt; 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4, 4, 3, 3,… ## $ carb &lt;dbl&gt; 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2, 1, 1, 2,… Exercise 13.10 - creating a linear model with multiple predictors # Use the `lm` function to create a linear data model from the `mtcars` table # use the variables `am`,` cyl` and `wt` as input # and the variable `mpg` as output # name the model `linMod` # # check the model summary linMod &lt;- lm(mpg ~ am + cyl + wt, data = mtcars) summary(linMod) ## ## Call: ## lm(formula = mpg ~ am + cyl + wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.1735 -1.5340 -0.5386 1.5864 6.0812 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 39.4179 2.6415 14.923 7.42e-15 *** ## am1 0.1765 1.3045 0.135 0.89334 ## cyl -1.5102 0.4223 -3.576 0.00129 ** ## wt -3.1251 0.9109 -3.431 0.00189 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.612 on 28 degrees of freedom ## Multiple R-squared: 0.8303, Adjusted R-squared: 0.8122 ## F-statistic: 45.68 on 3 and 28 DF, p-value: 6.51e-11 We can see that this summary is not that much different from the summaries we saw when we were creating simple linear models. The biggest difference is that each input numeric variable has its own coefficient, as well as each category for each individual categorical variable (minus one for the reference category). Also, each coefficient has its own “p-value”, i.e. the probability that the linear relationship with the target variable is random - so in this case, because of the high p-value of the input variable am (category 1), we can conclude that the difference between automatic and manual transmissions are very likely irrelevant when looking at the fuel cost, which is not an unexpected conclusion). However, there are several important differences in the interpretation of the results obtained. For example, the coefficient represents how much the target variable changes when the input variable changes by the amount of one, but provided that all other input parameters remain fixed. In other words, the coefficients are treated as being independent of one another. Of course, this assumption is often not the case in the real world, and how this fact affects the results is something that will be discussed later. Now let’s look at the “R-squared” measure. Although this measure has the same interpretation for multiple regression as in the case of simple regression, it can be somewhat problematic due to the fact that the multiple R-squared value, as a rule, always decreases as we add variables to the model. Therefore, in multiple regression, we prefer to look at a adjusted R-squared that compensates for this, because it is designed to “penalize” a larger number of input variables. Finally, the F-statistic makes more sense here than in the simple regression model. It tells us what is the chance that no input variable is associated with the target. This is especially important as we use more input variables - assuming an extremely large number of input variables, the possibility of accidentally detecting a relationship which doesn’t really exist becomes much more probable, and this statistic allows us to estimate how probable it is that such a situation occured. Let us return to the stated assertion that multiple linear regression coefficients treat the input variables as being independent of one another. This is relatively often not true, i.e. the input variables are not only collinear to the target but also to each other. This is a known problem of input collinearity. If it may not be immediately clear why this is potentially problematic, then it is enough to look again at the interpretation of multiple linear regression coefficients. The model assumes that we can select one predictor and change it freely while the values of other predictors remain fixed. Suppose now that the two predictors are extremely collinear, that is, changing one always results in changing the other. This means that the model assumes a much larger number of combinations of the values of these two predictors than is realistically possible, and since a large number of these combinations does not exist in the input set, this results in a greater “uncertainty” of the model in the results obtained. In particular, the calculated p-values of the predictors can be quite large to the extent that they can be treated as irrelevant, although they are actually strongly linearly related to the target. What can we do about this phenomenon? First, we can directly check the collinearity between the input variables with the help of the already used cor function, which will give us a correlation matrix as a result. There are no solid guidelines what represents a “too high” collinearity, but usually values that are in absolute terms above 0.7 probably deserve special attention. Alternatively, we can call the ggpairs function, which will give us not only correlation coefficients, but also visualizations by which we can observe linear trends between the input variables. Exercise 13.11 - collinearity of input variables # put all (truly) numeric columns from the `mtcars` dataset # into a `mtCarsNumPred` data frame # (do not include the `mpg` variable since we will treat it as our goal) # print the correlation matrix using the `cor` function # and the `mtCarsNumPred` as input parameter # pass this data frame to the `ggpairs` function of the `GGally` package # put all (truly) numeric columns from the `mtcars` dataset # into a `mtCarsNumPred` data frame # (do not include the `mpg` variable since we will treat it as our goal) mtcars %&gt;% select(-mpg) %&gt;% select_if(is.numeric) -&gt; mtCarsNumPred # print the correlation matrix using the `cor` function # and the `mtCarsNumPred` as input parameter mtCarsNumPred %&gt;% cor ## cyl disp hp drat wt qsec ## cyl 1.0000000 0.9020329 0.8324475 -0.69993811 0.7824958 -0.59124207 ## disp 0.9020329 1.0000000 0.7909486 -0.71021393 0.8879799 -0.43369788 ## hp 0.8324475 0.7909486 1.0000000 -0.44875912 0.6587479 -0.70822339 ## drat -0.6999381 -0.7102139 -0.4487591 1.00000000 -0.7124406 0.09120476 ## wt 0.7824958 0.8879799 0.6587479 -0.71244065 1.0000000 -0.17471588 ## qsec -0.5912421 -0.4336979 -0.7082234 0.09120476 -0.1747159 1.00000000 ## gear -0.4926866 -0.5555692 -0.1257043 0.69961013 -0.5832870 -0.21268223 ## carb 0.5269883 0.3949769 0.7498125 -0.09078980 0.4276059 -0.65624923 ## gear carb ## cyl -0.4926866 0.5269883 ## disp -0.5555692 0.3949769 ## hp -0.1257043 0.7498125 ## drat 0.6996101 -0.0907898 ## wt -0.5832870 0.4276059 ## qsec -0.2126822 -0.6562492 ## gear 1.0000000 0.2740728 ## carb 0.2740728 1.0000000 # pass this data frame to the `ggpairs` function of the `GGally` package #library (GGally) # if necessary ggpairs(mtCarsNumPred) Another convenient way of visualizing collinearity is the corrplot function of the package of the same name. Exercise 13.12 - function corrplot # load the `corrplot` package (install if necessary) # invoke the `corrplot` function to which you will pass the correlation matrix # made from the `mtCarsNumPred` data frame # load the `corrplot` package (install if necessary) # invoke the `corrplot` function to which you will pass the correlation matrix # made from the `mtCarsNumPred` data frame #library(corrplot) if needed corrplot(cor(mtCarsNumPred)) We see that some variables (eg disp andwt, representing engine volume and vehicle weight) have very high levels of collinearity, which means that they require special attention. The collinearity of the variables we have been looking at is related to pairs of variables, but there is one interesting phenomenon - the so-called multicollinearity. With this phenomenon, it is possible that collinearity is manifested only in the combination of three or more variables, that is, when looking at single pairs we do not see anything unusual, but the negative effect of collinearity persists. In order to facilitate the detection of this phenomenon, a so-called VIF measure (variance inflation factor) can be used. We will not explain in detail the meaning and theory behind this measure, but merely state the fact how to use it in our linear model and how to interpret the values obtained. We calculate the VIF measure using the vif function found in the car package. This function expects the linear model we are analyzing as a parameter. As a result, we obtain a numerical vector with VIF values for each input variable. Again, there is no exact guideline what represents a “high” VIF value - in the literature we come across different references, sometimes it’s a value of 5, a value of 10, etc. As a general rule, we can remember that a two-digit VIF indicates a “problematic” input variable. Exercise 13.13 - multicollinearity and VIF measure # train the linear model `lm_all` which uses the `mtcars` data frame # and has `mpg` as target and all other variables as predictors #pass the above model to the `vif` function of the` cars` package and print the result #library(car) # if necessary lm_all &lt;- lm(mpg ~., data = mtcars) vif(lm_all) ## cyl disp hp drat wt qsec vs am ## 15.373833 21.620241 9.832037 3.374620 15.164887 7.527958 4.965873 4.648487 ## gear carb ## 5.357452 7.908747 Now that we know that the collinearity of input variables is a potential problem, we can ask the question - what to do when we notice this phenomenon occuring? Some possible solutions are: throw out one variable from a pair of “problematic” variables combine collinear variables into a a unique input variable keep all variables in the model (i.e. “do nothing”) Let’s try to train three separate linear models and try to notice the effect of colinear variables on the results. Exercise 13.14 - linear model with collinear inputs # train the following linear models: # `lm1` -` mpg` depending on `disp` # `lm2` -` mpg` depending on `wt` # `lm3` -` mpg` depending on `disp` and` wt` # study the summaries of the linear models obtained, # especially the t-values of the parameters and the R-squared measure lm1 &lt;- lm(mpg ~ disp, data = mtcars) lm2 &lt;- lm(mpg ~ wt, data = mtcars) lm3 &lt;- lm(mpg ~ disp + wt, data = mtcars) summary(lm1) ## ## Call: ## lm(formula = mpg ~ disp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.8922 -2.2022 -0.9631 1.6272 7.2305 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 29.599855 1.229720 24.070 &lt; 2e-16 *** ## disp -0.041215 0.004712 -8.747 9.38e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.251 on 30 degrees of freedom ## Multiple R-squared: 0.7183, Adjusted R-squared: 0.709 ## F-statistic: 76.51 on 1 and 30 DF, p-value: 9.38e-10 summary(lm2) ## ## Call: ## lm(formula = mpg ~ wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.5432 -2.3647 -0.1252 1.4096 6.8727 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 37.2851 1.8776 19.858 &lt; 2e-16 *** ## wt -5.3445 0.5591 -9.559 1.29e-10 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.046 on 30 degrees of freedom ## Multiple R-squared: 0.7528, Adjusted R-squared: 0.7446 ## F-statistic: 91.38 on 1 and 30 DF, p-value: 1.294e-10 summary(lm3) ## ## Call: ## lm(formula = mpg ~ disp + wt, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4087 -2.3243 -0.7683 1.7721 6.3484 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 34.96055 2.16454 16.151 4.91e-16 *** ## disp -0.01773 0.00919 -1.929 0.06362 . ## wt -3.35082 1.16413 -2.878 0.00743 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.917 on 29 degrees of freedom ## Multiple R-squared: 0.7809, Adjusted R-squared: 0.7658 ## F-statistic: 51.69 on 2 and 29 DF, p-value: 2.744e-10 Comparing the results of the linear models obtained, we can conclude that the linear model lm3 has the smallest standard residual error and the largest” R-squared “measure and is thus the best of the three options. However, a potential problem is evident when we look at p-values, which are both significantly larger than when we train models with each variable separately. Thus, the collinearity of variables does not necessarily affect the predictive power of the model, but it does introduce a potentially large uncertainty in the model sense that we remove all collinear predictors from the model as irrelevant. This could prove to be a big problem when we have more potential predictors and are trying to select a relevant subset, which is a topic we will address below. In the previous exercise, we have seen that even with the relatively simple predictive multiple regression method with two potential input variables, a number of questions arise regarding the choice of formula, i.e. which input variables to choose. This issue gets much more complicated as the number of potential input variables grows. This is why variable selection is one of the key challenges we face in designing predictive models, not only in linear regression but also in general. Obviously, a good model should include input variables that “explain” the target variable while being as independent between each other as possible. A possible criterion for deciding which variable to choose to fit into the model may thus be to increase the common “R-squared” measure, reduce the residual standard error, or p-value of the coefficient for that input variable. In addition to these “standard” criteria, there are various others, such as using the popular AIC (Akaike information criterion), which assesses the model’s informativeness by penalizing having more variables thrown into the model. We can select variables manually, but it’s much easier to leave that job to your computer. Statistical tools, including the R language, often have built-in algorithms that, based on a given criterion, construct a predictive model by iteratively selecting variables. The most common model building strategies are: “backwards” from the complete model, e.g. iteratively throwing out the variables with the highest p-value “forwards” from the blank model, e.g. iteratively adding the variables that reduce the RMSE the most various hybrid methods The language R has a step function to iteratively (stepwise) create predictive models, but in practice, using the much better stepAIC function found in the MASS package is recommended. This function, among other things, expects the following parameters: object - initial (linear) model scope - range of models that we include in the strategy; this is only needed for the “forward” approach and we pass it a list with the “poorest” (lower) and ‘richest’ (upper) model direction - forward (forward), backward (backward) or hybrid (both) trace - A binary variable that describes whether we want to print the entire variable selection process (Note: the MASS package has its own select function, which can “override” the select function from thedplyr package; this does not mean that this function is unavailable, but when these two packages are used together, it is better to rely on full function names, i.e. MASS::select and dplyr::select) Finally, we will iteratively create a predictive model for the mtcars data frame where again the target variable will be fuel cost (the variablempg), while the candidates for the input variable will be all other variables. Exercise 13.15 - stepwise variable selection for linear regression #library (MASS) # if necessary # We create a &quot;complete&quot; and &quot;empty&quot; model lm_all &lt;- lm(mpg ~., data = mtcars) lm_blank &lt;- lm(mpg ~ 1, data = mtcars) # check out summaries on these models # to get the feeling of &quot;baseline&quot; performance # use the `stepAIC` function to create the `lm1` and `lm2` models # `lm1` - created by backward selection from full model # (parameter direction = &quot;backward&quot;) # `lm2` - is created by selecting&quot; advance &quot;from the empty model # (parameters direction = &quot;forward&quot;, # scope = list (upper = lm_all, lower = lm_ blank)) # # check out summaries of these models summary(lm_all) ## ## Call: ## lm(formula = mpg ~ ., data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4506 -1.6044 -0.1196 1.2193 4.6271 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 12.30337 18.71788 0.657 0.5181 ## cyl -0.11144 1.04502 -0.107 0.9161 ## disp 0.01334 0.01786 0.747 0.4635 ## hp -0.02148 0.02177 -0.987 0.3350 ## drat 0.78711 1.63537 0.481 0.6353 ## wt -3.71530 1.89441 -1.961 0.0633 . ## qsec 0.82104 0.73084 1.123 0.2739 ## vs1 0.31776 2.10451 0.151 0.8814 ## am1 2.52023 2.05665 1.225 0.2340 ## gear 0.65541 1.49326 0.439 0.6652 ## carb -0.19942 0.82875 -0.241 0.8122 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.65 on 21 degrees of freedom ## Multiple R-squared: 0.869, Adjusted R-squared: 0.8066 ## F-statistic: 13.93 on 10 and 21 DF, p-value: 3.793e-07 summary(lm_blank) ## ## Call: ## lm(formula = mpg ~ 1, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.6906 -4.6656 -0.8906 2.7094 13.8094 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 20.091 1.065 18.86 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.027 on 31 degrees of freedom lm1 &lt;- stepAIC(lm_all, direction = &quot;backward&quot;, trace = 0) summary(lm1) ## ## Call: ## lm(formula = mpg ~ wt + qsec + am, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4811 -1.5555 -0.7257 1.4110 4.6610 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 9.6178 6.9596 1.382 0.177915 ## wt -3.9165 0.7112 -5.507 6.95e-06 *** ## qsec 1.2259 0.2887 4.247 0.000216 *** ## am1 2.9358 1.4109 2.081 0.046716 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.459 on 28 degrees of freedom ## Multiple R-squared: 0.8497, Adjusted R-squared: 0.8336 ## F-statistic: 52.75 on 3 and 28 DF, p-value: 1.21e-11 lm2 &lt;- stepAIC (lm_blank, scope = list (upper = lm_all, lower = lm_blank), direction = &quot;forward&quot;, trace = 0) summary(lm2) ## ## Call: ## lm(formula = mpg ~ wt + cyl + hp, data = mtcars) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9290 -1.5598 -0.5311 1.1850 5.8986 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 38.75179 1.78686 21.687 &lt; 2e-16 *** ## wt -3.16697 0.74058 -4.276 0.000199 *** ## cyl -0.94162 0.55092 -1.709 0.098480 . ## hp -0.01804 0.01188 -1.519 0.140015 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.512 on 28 degrees of freedom ## Multiple R-squared: 0.8431, Adjusted R-squared: 0.8263 ## F-statistic: 50.17 on 3 and 28 DF, p-value: 2.184e-11 We can see that the two strategies resulted in two completely different models of similar performance. This means that we cannot commony expect to find the optimal model, but just perform an automated attempt to find the “best” with the criteria and conditions we initially set. If we are not satisfied with the result, we can always try alternative function parameters, a different strategy, or an initial set of input variables. In any case, finding a satisfactory predictive model is not a simple problem, and it is always advisable to remember a quote often found in statistics and machine learning books allegedly made by statistician George Box - “all models are wrong, but some are useful”. Homework exercises Load the iris dataset into the global environment. Draw a scatterplot showing the relationship between the sepal length and the petal length. Using the geometry for drawing diagonal reference lines (function geom_abline) try to guess the slope and intercept for the “best fit” line. Do not cheat by using the geom_smooth function! Example: ... + geom_abline(slope = 1, intercept = -3) Train a linear regression model with sepal length as a predictor and petal length as a target variable. Compare gained parameters with the ones you estimated on your own. Redraw a scatterplot from 1) but color the dots based on Species. Explain why a single regression model might not be the best choice for predicting petal length from sepal length. Train a stepwise linear regression model where the petal length is a target variable and all other variables are potential predictors (including the Species variable). Compare model summaries between the model from 2) and the final stepwise model. Which one would you suggest using? Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],["predictive_Modeling.html", "14 Introduction to Predictive Modeling 14.1 What is predictive modeling? 14.2 Creating training and test datasets 14.3 Classification Predictive Models - kNN Classification 14.4 Package caret and predictive modeling", " 14 Introduction to Predictive Modeling 14.1 What is predictive modeling? In the previous chapter, we have shown that with the help of linear regression we can investigate and model the interplay of two (or more) variables. If there is collinearity between the two observed variables, then their relation can be modeled with the help of a simple straight line equation, which then allows to estimate the value of the second variable from the known value of one of the variables. This is the basic idea of so-calle. “predictive modeling” - a process which relies on familiar inputs and developed predictive models to get information about unknown outputs, i.e. goals. Better models provide better, or more accurate results. Or, as defined by Kuhn and Johnson in his book “Applied Predictive Modeling”: Predictive modeling is a process of developing a mathematical tool or model that is able to create accurate predictions. Predictive modeling has numerous usage options in various domains eg: predicting the quantity of sold products or expected profits identification of users planning to cancel the subscription disease diagnostics real estate evaluation estimation of movies’s profits on opening weekened spam detection etc. When developing predictive models, it is very important to: have adequately prepared, high quality input data choose a suitable method for creating a model properly conduct the evaluation and validation process The role of analysts in this process is to conduct all the steps of analysis thoroughly and to avoid common traps and errors. Namely, predictive models can ultimately have poor performance for a number of reasons - poorly prepared data, poorly conducted validation, creating models which “overfit” the training data, etc. In this chapter, we will get acquainted with some of the basic guidelines we must adhere to during predictive modeling. We will also familiarize ourselves with R language packages that allow us to leave complex steps to the computer, that is, to approach predictive modelling from high-level perspective while the computer independently performs low-level data preparation, modeling, and validation steps. 14.2 Creating training and test datasets When we created linear models in the previous chapter, after the creation of the model, we looked at summarized model information with the help of summary function - estimation of predictor and target relationship, the value of residual standard error, the value of adjusted R-squared measure etc. This information has enabled us to evaluate the quality of developed models. But in the whole process we ignored one key thing - all of this information was based on data used to create the model itself. In other words, we have gained insight into how well a model works over known data, which the model has already seen during the model’s creation. As a rule, it is much more important to evaluate how well a model works over unknown data, i.e. to estimate the quality of predictions once the model gets completely new data. We say that we want models that generalize well, i.e. models that have learned generic features of the domain entities described in the obtained data, and not just the specifics of the dataset used to develop the model itself. This latter phenomenon is called “overfitting”. Very often, at the time of creating a model, we only have one dataset that we should also for training and model evaluation. As we have already said, using the same data for both purposes does not give us enough good information about how the model generalizes. A common procedure in this case is to split the initial set into two parts - a training set and a test set. How do you decide which observations to assign to which set? How many observations should be put into training, and how many in the testing set? As a rule, the training set and test set should have enough observations so that the obtained results could be statistically relevant so assuming a sufficiently large input dataset, splitting in two equal parts could be a satisfactory solution. However, in practice, we usually reserve more observations for training than for testing, so it is customary to use a 70:30 spluit, so 70% observations go into the training set, and the rest go in the test set. As for the procedure of selecting observations (i.e. “sampling”), there are several common procedures: random selection stratified random selection (ensuring equal representation of certain categories in both sets) timestamp-based selection (if the time component is key, i.e. it is important to estimate how well past information predicts future events) Random selection is a most commonly used method while more sophisticated sampling methods can be chosen if there is a clear need for them based on the goals of the analysis. Let’s now apply this knowledge to the development of a linear regression model by comparing its effectiveness on the training set, and then on the test data. For this we will use a new set of data related to the characteristics and quality of wine, wines.csv. This set was created by adapting the “Wine Quality Data Set” from the UCI Machine Learning Repository, available at[this link] (https://archive.ics.uci.edu/ml/datasets/wine+quality). In the next exercise, we’ll load this dataset and make some adjustments - we’ll categorize columns that obviously contain a category variable and remove rows with missing values. Lines with missing values often require a little more attention and a more sophisticated approach than simple removal, especially with larger amounts of missing values or potential additional information that these values denote. But in our case there are very few such rows, and when applying certain predictive modeling methods, they can create problems, so we will simply remove them. One of the quick ways to do this is by using the complete.cases function, which returns the indexes of all rows which do not have any missing value for the given data frame, or na.omit which will filter out all rows with NA values. Exercise 14.1 - wine quality dataset # load data from the` wines.csv` file # in a variable called `wine` # examine the loaded data frame # categorize columns as needed # and remove rows with missing values # hint: check out a function called `na.omit` (or `complete.cases`) # load data from the` wines.csv` file # in a variable called `wine` # examine the loaded data frame # categorize columns as needed # and remove rows with missing values wine &lt;- read_csv(&quot;wines.csv&quot;) glimpse(wine) wine$type &lt;- factor(wine$type) wine &lt;- wine[complete.cases(wine),] # or na.omit(wine) ## Rows: 6497 Columns: 13 ## ── Column specification ──────────────────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): type ## dbl (12): fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlo... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. ## Rows: 6,497 ## Columns: 13 ## $ fixed.acidity &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7.5… ## $ volatile.acidity &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600, … ## $ citric.acid &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00, 0… ## $ residual.sugar &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.1,… ## $ chlorides &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069, … ## $ free.sulfur.dioxide &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, 16… ## $ total.sulfur.dioxide &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 102,… ## $ density &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978, 0… ## $ pH &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39, 3… ## $ sulphates &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47, 0… ## $ alcohol &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 10.… ## $ quality &lt;dbl&gt; 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5, 7… ## $ type &lt;chr&gt; &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, … Suppose the attribute quality is the target variable and all the other columns are potential predictors. How do you split this set on the training set and test set using random selection? There are several ways to do this, and even specialized functions and packages for this purpose, but we will learn a simple and easy-to-understand method that uses the sample function. If we assume that df is the data frame that we want to split into a training and test set that we will store in df.train and df.test variables, then random selection can be done as follows: train_size &lt;- 0.7 * nrow(df) %&gt;% round # about 70% train_ind &lt;- sample(1:nrow(df), train_size) # indexes of training observations df.train &lt;- df[train_ind, ] df.test &lt;- df[-train_ind, ] Let’s apply this to our wines dataset and then train a linear regression model. Exercise 14.2 - Splitting the input dataset and training a model set.seed(1234) # split the `wine` dataframe into `wine.train` and `wine.test` # using a random selection method and 70:30 ratio # train a linear regression model using the `wine.train` set # store the model in a variable called `linMod` # target variable is `quality` and all other variables are predictors # check the summary for the obtained model set.seed(1234) # split the `wine` dataframe into `wine.train` and `wine.test` # using a random selection method and 70:30 ratio train_ind &lt;- sample(1:nrow(wine), 0.7 * nrow(wine) %&gt;% round) wine.train &lt;- wine[train_ind, ] wine.test &lt;- wine[-train_ind, ] # train a linear regression model using the `wine.train` set # store the model in a variable called `linMod` # target variable is `quality` and all other variables are predictors # check the summary for the obtained model linMod &lt;- lm(quality ~ ., data = wine.train) summary(linMod) ## ## Call: ## lm(formula = quality ~ ., data = wine.train) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6077 -0.4616 -0.0409 0.4637 3.0352 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.226e+02 1.846e+01 6.643 3.44e-11 *** ## fixed.acidity 9.438e-02 1.955e-02 4.828 1.42e-06 *** ## volatile.acidity -1.600e+00 9.558e-02 -16.736 &lt; 2e-16 *** ## citric.acid -1.373e-01 9.414e-02 -1.459 0.144664 ## residual.sugar 6.943e-02 7.438e-03 9.335 &lt; 2e-16 *** ## chlorides -6.106e-01 3.887e-01 -1.571 0.116274 ## free.sulfur.dioxide 5.488e-03 9.148e-04 5.999 2.14e-09 *** ## total.sulfur.dioxide -1.340e-03 3.805e-04 -3.522 0.000432 *** ## density -1.218e+02 1.871e+01 -6.513 8.18e-11 *** ## pH 5.275e-01 1.106e-01 4.770 1.90e-06 *** ## sulphates 7.499e-01 9.096e-02 8.244 &lt; 2e-16 *** ## alcohol 2.069e-01 2.335e-02 8.859 &lt; 2e-16 *** ## typewhite -4.546e-01 6.927e-02 -6.563 5.87e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7292 on 4533 degrees of freedom ## Multiple R-squared: 0.3106, Adjusted R-squared: 0.3088 ## F-statistic: 170.2 on 12 and 4533 DF, p-value: &lt; 2.2e-16 We see that the adjusted R-squared measure is relatively low and the average error is quite high, so linear regression is perhaps not the best option this scenario (or maybe the obtained characteristics of the wines are simply not good enough to predict its quality). In spite of this, we will now check how well the model works on unseen observations. In order to check the quality of our model, we have to choose the criterion to which we will adhere. For numeric goals we often use the RMSE measure (root mean square error): \\[RMSE = \\sqrt{\\frac{\\sum_{i = 1}^{n}(\\widehat{y}_{i} - y_{i})^2}{n}}\\] An alternative to this measure is MAE (mean absolute error), which is calculated with the help of the following formula: \\[MAE = \\frac{\\sum_{i = 1}^{n}|\\widehat{y}_{i} - y_{i}|}{n}\\] The measure MAE is a bit more intuitive but also mathematically more complex when performing optimization functions. Furthermore, RMSE measures takes priority when we want to more strongly penalize larger prediction mistakes. Although there are packages that contain functions that calculate these measures, it is not difficult to create them manually. Below we will focus on the RMSE measure, while leaving measure MAE for use as an optional exercise. Exercise 14.3 - Function for calculating the RMSE measure # create a `rmse` function that will use prediction vector # and a vector of actual target vales as parameters # and calculate the RMSE measure according to the above formula # create a `rmse` function that will use prediction vector # and a vector of actual target vales as parameters # and calculate the RMSE measure according to the above formula rmse &lt;- function(pred, act) sqrt(mean((pred - act) ** 2)) Let us now add prection columns to wine.train andwine.test datasets and then calculate the value of the RMSE measure for both. Exercise 14.4 - Model evaluation for the training and test set # add a `predQualityLR` column to `wine.train` and `wine.test` # using the `predict` function and the `linMod` model # print the value of the RMSE measure for both sets # remove the `predQualityLR` column from the `wine.train` dataset # add a `predQualityLR` column to `wine.train` and `wine.test` # using the `predict` function and the `linMod` model wine.train$predQualityLR &lt;- predict(linMod, wine.train) wine.test$predQualityLR &lt;- predict(linMod, wine.test) # print the value of the RMSE measure for both sets rmse(wine.train$predQualityLR, wine.train$quality) rmse(wine.test$predQualityLR, wine.test$quality) # remove the `predQualityLR` column from the `wine.train` dataset wine.train$predQualityLR &lt;- NULL ## [1] 0.7281868 ## [1] 0.7435729 We see that the RMSE for the training set roughly corresponds to the residual standard error obtained in the model summary (a small difference is a result of the fact that we used the total number of observations, while in the calculation of the residual standard error we used the number of degrees of freedom - “dependent” variables, i.e. predictors). What is interesting is the fact that the value of RMSE measure of the test set is not dramatically different than the RMSE measure, which means that the model works almost equally well (or badly) on new data. 14.3 Classification Predictive Models - kNN Classification The linear regression method allowed us to “guess” the numerical target variable. It is reasonable to ask the question - can we build a predictive model that will try to estimate the value of a categoric variable? This is an extremely important goal of predictive modeling since many domains have problems with determining the value of a particular category (the patient is ill or not, the device is corrupt or correct, the transaction is regular ora result of fraud, the client will default on the loan or not etc.) These are so-called “classification” problems for which - just as for the “regression” problems which use numerical goals - there is an extremely large set of developed methods. Very often, similar methods can be used for both purposes (sometimes with certain adjustments), so for example even if linear regression is not specifically suited for classification problems, its related “logistic regression” method is a very effective and popular approach to such a type of problem. In this section, we will focus on another, very popular classification method, which is very intuitive and easy to understand, but which allows us to inspect a number of interesting predictive modeling elements that we have not mentioned so far. It is a method called “k nearest neighbors”, or “kNN classification”. This method works in a very simple way. If we do not know the category of some observation, we simply find a number of observations which are the “closest” to that new observation. We then look at their categories, and then by majority vote determine which category to assign to the new observation. Let’s try to visualize this. Exercise 14.5 - Visualization of the main idea for the method of kNN classification # create a scatterplot for `wine.train` dataset # put `chlorides` on `x` axis # and `volatile.acidity` on `y` axis # color point based on wine type # set the transparency of the points to 0.5 # create a scatterplot for `wine.train` dataset # put `chlorides` on `x` axis # and `volatile.acidity` on `y` axis # color point based on wine type # set the transparency of the points to 0.5 ggplot(wine.train, aes(chlorides, volatile.acidity, col = type)) + geom_point(alpha = 0.5) On the graph we can clearly see how the points form “clusters” of the same colors in certain areas. If we take a new observation regarding the wine of an unknown type, but the known values of the measures set on the x and y axes, by looking at its immediate neighborhood, we could conclude which type of wine the new wine belongs to. Observations deep within one of the “clouds” are very likely to belong to the type shown (although we see that there are exceptions!). On the other hand, observation within the “boundary” areas is much more likely to be misdiagnosed, and the different choice of neighboring numbers could result in different classification results. kNN classification is based on the concept of distance. Although there are different options for distance selection, we often rely on the so-called “Euclidean distance”, which is easily presented in the two- and three-dimensional Cartesian system by the shortest path between two points, and we can easily calculate its value using their coordinates and Pythagorean theorem. This distance is easily applied to n-dimensional spaces, so although we can not easily visualize points in a space whose dimension corresponds to the number of predictors, we can still easily calculate the value of the Euclidean distance between points. So, the way kNN classification works can be easily described as follows: the training set itself represents “domain knowledge”, that is, the predictive model itself for each new observation, we find k closest observations from the training set and assign the category to it by using the majority vote Although this process is relatively simple, there are some common questions which require answers. Firstly - how do we select the parameter k? Secondly, do we need to make some additional preparatory actions on the data set before we perform the kNN classification? Let’s deal with the second question first. Examine the graph we have drawn, more precisely its coordinate axes. We can see that the lengths of the axes do not necessarily scale to their numerical values equally, i.e. the unit interval on the x axis is not necessarily equal to the unit interval on y axis. This is normal and expected, since the values on axes do not necessarily have to use the same scale nor even the measuring unit. But there is one problem - when we use Euclidean distance, it treats all axes equally, which means that variables with larger ranges will automatically gain greater importance (e.g. the maximum value of the chlorides is around 0.611 while the maximum value of sulfur dioxide variable reaches over 3000!) Here we see the importance of data pre-processing . For kNN classification, normalization of numeric variables is recommended, i.e. the transformation of numeric variables in such a way that we deduct their average and divide them by standard deviation, which brings them all to the same scale. This process is somewhat more complex than it seems, because we have to make sure that new observations are also normalized in the same way. This means that we need to make sure both training and test numerical variables need to come from the same distribution. If we have enough data and both training and test observations are representative, then their means and standard deviations should be close enough so we can easily normalize each set independently. If we have small test sets, then the normalization of the test set should be done by remembering using the mean and standard deviation of the training set. To simplify this process somwhat, we will return to the original wine dataset and simply normalize the numeric columns beforehand, and then store it in the wineNorm variable (we can pretend that we knew means and standard deviations of populations of these variables beforehand so we used them to normalize both training and test set independetly). Then, we will split this set into sets of wineNorm.train andwineNorm.test analogously to the previous procedure. `r zadHead(“Normalization of numeric variables of the input dataset”) # normalize all numeric columns of the `wine` data frame # store the result in the `wineNorm` variable # use the `train_ind` object to split `wineNorm` # into `wineNorm.train` and `wineNorm.test` # normalize all the numeric columns of the `wine` data frame # store the result in the `wineNorm` variable wineNorm &lt;- lapply(wine, function(x) { if(is.numeric(x))(x - mean(x)) / sd(x) else x })%&gt;% as.data.frame # use the `train_ind` object to split `wineNorm` # into `wineNorm.train` and `wineNorm.test` wineNorm.train &lt;- wineNorm[train_ind,] wineNorm.test &lt;- wineNorm[-train_ind,] Let’s go back to the problem of selecting the value of the k parameter. How to pick the right value? Generally speaking, this problem is called “choosing a hyperparameter of a model,” since the model besides input data requires some additional input parameters to perform its function. Usually, when no mathematical method of calculating optimal hyperparameters exist, the only option we have is training models with various combinations of hyperparameters and finally selecting those hyperparameter which result in models showing the best performance. In this case, we often need another (third) part of the original dataset, usually called a “validation” dataset, which represents an additional step before testing the set in which we select the values of the hyperparameters for the “final” model. Specifically, for kNN classification: we train a larger number of models for different parameter values k over the training set with the help of the validation set, we find the model that has the best performance and choose its k we use chosen k and the entire training and validation set to train the model we make a final evaluation on the test set Below we will use a simplified version of this process which uses only the training and test set while setting the k parameter arbitrarily to 5. We will leave the entire process of possibly finding a better hyperparameter as an optional exercise. Likewise, as we will see at the end of this chapter, we often do not manually program instruction for finding best hyperparameters, but rather use high-level functions which allow us to only set things up declaratively, without having to deal with low-grade details about the implementation of the process itself. Let’s try to see if we can properly classify wine as “white” or “red” with the help of variables describing its chemical composition. We will use the kNN method, with the number of neighbors set to 5. Predictors will be all available variables except quality andtype. For kNN classification the base R offers the knn function . For our needs we will use the knn3 function from thecaret package. The function knn3 expands the basic knn function in a way that allows us to call it by following the standard programming conventions we have already learned when training linear regression models: model &lt;- selected_method(formula, training_dataset, additional_parameters) predictions &lt;- predict(model, test_dataset, additional_parameters) In the case of the knn3 function, an additional parameter in training the model is k, set to 5. When creating predictions, we will set the type parameter to class, meaning we want to predict the class itself (alternative is probs, returning predicted probabilities for each class). Let’s try to create a kNN model with the help of wineNorm.train and then find wine type predictions for wineNorm.test. Exercise 14.6 - kNN Classification # create the variable `kNN5Mod` which will be # the result of calling `knn3` over the `wineNorm.train` set # target variable is `type` # predictors are all other variables except `quality` # add a `predictedType` column to `wineNorm.test` # which will store a result of calling the `predict` function # with `kNN5Mod` as a model and `wineNorm.test` as new data # set the `type` parameter to `class` # create the variable `kNN5Mod` which will be # the result of calling `knn3` over the `wineNorm.train` set # target variable is `type` # predictors are all other variables except `quality` #library(caret) # if needed kNN5Mod &lt;- knn3(type ~ . - quality, data = wineNorm.train, k = 5) # add a `predictedType` column to `wineNorm.test` # which will store a result of calling the `predict` function # with `kNN5Mod` as a model and `wineNorm.test` as new data # set the `type` parameter to `class` wineNorm.test$predictedType &lt;- predict(kNN5Mod, wineNorm.test, type = &quot;class&quot;) Note that we did not look for a model summary since we can not get too much information in this case. The kNN classifier can not provide us with some aggregated information about “learned” knowledge, it is just a “map of the domain space” that is then used for each new observation to determine which category it belongs to. How do we check the classifier’s performance? A typical procedure (with binary classifiers) is the creation of a so-calle “confusion matrix”. This simply means that we will create a table that will show how well the predicted values match the actual values. The easiest way to create this table is simply to call the table function with the prediction column and the actual value column as parameters. Exercise 14.7 - Simple Confusion Matrix # print a confusion matrix by calling the `table` function # over the appropriate columns of the `wineNorm.test` dataset # print a confusion matrix by calling the `table` function # over the appropriate columns of the `wineNorm.test` dataset table(wineNorm.test$predictedType, wineNorm.test$type) ## ## red white ## red 464 5 ## white 10 1470 Looking at the results we can intuitively conclude that in this case the classifier works extremely well, that is, white and red wines can be very easily classified by looking at their chemical properties. But we often want to describe the quality of the classifier using an objective, numerical measure. There are a number of such measures, and most of them can be directly calculated using values from the confusion matrix. Specifically, if we call the confusion matrix cells TP, TN, FP, FN (true positive, true negative, false positive, false negative), where we treat one class as “positive” and hence assigning the names of the corresponding cells depending on whether the classifier correctly guessed the class (main diagonal) or not (side diagonal). In this case, from the confusion matrix we can directly calculate the following measures: accuracy: \\(\\frac{TP + TN}{TP + FP + TN + FN}\\) sensitivity (recall): \\(\\frac{TP}{TP + FN}\\) precision: \\(\\frac{TP}{TP + FP}\\) false positive rate/false negative rate: \\(\\frac{FP}{FP + TN}\\) ; \\(\\frac{FN}{TP + FN}\\) etc. These are just some of the possible measures. Although accuracy may be the most logical choice (because we actually get a percentage of correctly guessed observations), we often have to be careful because it can give us a distorted picture of the classifier’s effectiveness, especially in when there is a huge disbalance in categories or when one type of error is far more dangerous than the other. A typical example is the diagnosis of rare diseases - if the disease occurs in only 0.1% of cases, then the trivial classifier, which for all observations diagnoses that the disease is not present, works well in 99.9% cases. Also, if it is a dangerous disease, then FP error (the disease is diagnosed although not present) is far less important than FN errors (the disease is present but is not recognized). In these cases, selecting another measure (eg “sensitivity” or “false negative rate”) is often a much better quality indicator of the classifier. We can very easily manually calculate all these measures using base R. However, the confusionMatrix function from the caret package (which also leverages the e1071 package) gives us the same result as the table function but with the added convenience of computing a large number of measures that can help us evaluate the classifier’s quality. Exercise 14.8 - confusionMatrix function # create a variable called `confMat` variable # which will stort the result of the `confusionMatrix` function # using the appropriate columns of the `wineNorm.test` dataset as parameters # print the `confMat` variable # create a variable called `confMat` variable # which will stort the result of the `confusionMatrix` function # using the appropriate columns of the `wineNorm.test` dataset as parameters #library(e1071) # if needed #library(caret) # if needed confMat &lt;- confusionMatrix(wineNorm.test$predictedType, wineNorm.test$type) # print the `confMat` variable confMat ## Confusion Matrix and Statistics ## ## Reference ## Prediction red white ## red 464 5 ## white 10 1470 ## ## Accuracy : 0.9923 ## 95% CI : (0.9873, 0.9957) ## No Information Rate : 0.7568 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.979 ## ## Mcnemar&#39;s Test P-Value : 0.3017 ## ## Sensitivity : 0.9789 ## Specificity : 0.9966 ## Pos Pred Value : 0.9893 ## Neg Pred Value : 0.9932 ## Prevalence : 0.2432 ## Detection Rate : 0.2381 ## Detection Prevalence : 0.2406 ## Balanced Accuracy : 0.9878 ## ## &#39;Positive&#39; Class : red ## Since the variable confMat is an S3 object, with the help of the unlist function and the selection of the desired element, we can easily obtain only the numeric value of the measure we are interested in. This is useful if we want to integrate this function in our programming scripts. 14.4 Package caret and predictive modeling In the previous chapter we were already introduced to the caret package, more specifically some of its functions that help us with predictive modeling. This package actually offers a lot more than we have seen. Specifically, the caret package provides a set of tools to effectively perform all elements of the predictive modeling process: splitting data pre-processing of data feature selection adjusting the model with the help of re-sampling variable importance estimation As the dplyr package actually changes the way we use the R language to manage the data frames, so thecaret package enables a thorough modification of the predictive modeling approach used when programming in R. The functions of the caret package not only provide a cleaner syntax for low-level jobs, they also give the possibility of leveraging high-level approach for predictive modeling, where we declare declarative calls for what we want to do, and let R do low-level jobs returning us the corresponding result. Details of this package can be found at [this link] (https://topepo.github.io/caret/index.html), and below we will only give you a brief insight into some of the most useful features of this package. To demonstrate the declarative nature of predictive modeling with this package, we will look at two functions: train andtrainControl. The train function is actually a generic interface to a large number of predictive models (a list of all the models currently supported by the function can be [found here] (http://topepo.github.io/caret/train-models-by-tag.html) . In a large number of cases, the this function call is not different from the call of the predictive modeling method functions we have learned so far, specifically lm andknn3. The biggest difference is that instead of calling a specific function, here we define the method of predictive modeling using the method parameter. Let’s try to train a linear regression model using the train function and the previously prepared wine.train dataset. Exercise 14.9 - train function and linear regression # using the `train` function from the `caret` package # train a linear regression model using the `wine.train` dataset # target variable is `quality` and all other variables are predictors # set the `method` `&quot;lm&quot;` # store the resuling model in a variable called `linMod` # using the `train` function from the `caret` package # train a linear regression model using the `wine.train` dataset # target variable is `quality` and all other variables are predictors # set the `method` `&quot;lm&quot;` # store the resuling model in a variable called `linMod` linMod &lt;- train(quality ~., data = wine.train, method = &quot;lm&quot;) # read the summary of the obtained model summary(linMod) ## ## Call: ## lm(formula = .outcome ~ ., data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.6077 -0.4616 -0.0409 0.4637 3.0352 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.226e+02 1.846e+01 6.643 3.44e-11 *** ## fixed.acidity 9.438e-02 1.955e-02 4.828 1.42e-06 *** ## volatile.acidity -1.600e+00 9.558e-02 -16.736 &lt; 2e-16 *** ## citric.acid -1.373e-01 9.414e-02 -1.459 0.144664 ## residual.sugar 6.943e-02 7.438e-03 9.335 &lt; 2e-16 *** ## chlorides -6.106e-01 3.887e-01 -1.571 0.116274 ## free.sulfur.dioxide 5.488e-03 9.148e-04 5.999 2.14e-09 *** ## total.sulfur.dioxide -1.340e-03 3.805e-04 -3.522 0.000432 *** ## density -1.218e+02 1.871e+01 -6.513 8.18e-11 *** ## pH 5.275e-01 1.106e-01 4.770 1.90e-06 *** ## sulphates 7.499e-01 9.096e-02 8.244 &lt; 2e-16 *** ## alcohol 2.069e-01 2.335e-02 8.859 &lt; 2e-16 *** ## typewhite -4.546e-01 6.927e-02 -6.563 5.87e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.7292 on 4533 degrees of freedom ## Multiple R-squared: 0.3106, Adjusted R-squared: 0.3088 ## F-statistic: 170.2 on 12 and 4533 DF, p-value: &lt; 2.2e-16 We see that we get the same result as when we called the lm function directly at the beginning of this chapter. Note that in this case we haven’t used a large number of parameters of the train function that we can see in the documentation. For example, using the preprocess parameter can automatically perform some data preparation procedures such as normalization, BoxCox transformation, imputation of missing values, and so on. Let’s create a little more complex predictive model now. First, let’s examine a “control object” we can create using a function called trainControl. This function provides us with a “control panel” which allows us to fine-tune all parameters related to the training of our predictive model. Some of these parameters relate to so-called “data re-sampling” - this means that we can get a better estimate of the behavior of developed predictive models if we perform a process called “cross-validation”, where the training set is split multiple times and then a model is trained over and over again, always using a separate holdover part as the test set. In this way we actually get a number of models, each with their own results, which give us information not only about the quality but also the stability of the model. Additionally, we have the option of tweaking additional parameters, such as which summary funcation we want to apply on the resulting model, or whether we want the model (in the case of classification) to return the probabilities or just the resulting category. When finally create such a “control object”, we can recycle it as much as we want in the future steps of predictive modeling without the need to enter a large number of training-related parameters every time. A simple control object can look like this: # We use repeated cross-validation with 5 repeats ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, repeats = 5) Finally, let’s try to use two slightly more advanced methods of predictive modeling - the random forest method (the ranger method from the package of a same name) and the support vector method (thesvmRadial method from the kernlab) package. We will not go deeper into the details of these methods, just focus on how to call them with the help of the trainCtrl andtrain functions. In the following code we will also leverage a function called expand.grid. This function requires a vector different values of the hyperparameters we have provided, and will result in a dataframe containing all combinations of these hyperparameters. It is most commonly used in conjunction with the tuneGrid parameter to assign the candidate sets for predictive model hyperparameter - in this way, the train function will try out all combinations of the default parameters and (in conjunction with the cross validation method) select the parameters that showcase the best performance for the final model. #library(ranger) # if needed #library(kernlab) # if needed # we will use the same control object for both models # set `verboseIter` to TRUE # for better insight into training speed! ctrl &lt;- trainControl( method = &quot;repeatedcv&quot;, number = 5, repeats = 2, verboseIter = FALSE) # random forest model rfMod &lt;- train(quality ~., data = wine.train, method = &#39;ranger&#39;, tuneLength = 10, trControl = ctrl, num.trees = 10) # support vector model # we use a grid of hyperparameters # and pre-process data with normalization svm.grid &lt;- expand.grid(C = c(2, 4, 8), sigma = c(0.25, 1, 2)) svmMod &lt;- train(quality ~., data = wine.train, method = &quot;svmRadial&quot;, trControl = ctrl, tuneGrid = svm.grid, preProcess = c(&quot;center&quot;, &quot;scale&quot;)) Finally, we make a simple evaluation of the model with the help of RMSE measures. Exercise 14.10 - Simple Model Evaluation # with the help of the `predict` function and` rfMod` and `svmMod` models # add `predQualityRF` and `predQualitySVM` columns to `wine.test` dataset # print the value of the RMSE measure for all obtained models # with the help of the `predict` function and` rfMod` and `svmMod` models # add `predQualityRF` and `predQualitySVM` columns to `wine.test` dataset wine.test$predQualityRF &lt;- predict(rfMod, wine.test) ## Warning in predict.ranger.forest(forest, data, predict.all, num.trees, type, : ## Forest grown in ranger version &lt;0.11.5, converting ... wine.test$predQualitySVM &lt;- predict(svmMod, wine.test) # print the value of the RMSE measure for all obtained models cat(&quot;RMSE Linear regression:&quot;, rmse(wine.test$predQualityLR, wine.test$quality)) cat(&quot;\\nRMSE Random Forest:&quot;, rmse(wine.test$predQualityRF, wine.test$quality)) cat(&quot;\\nRMSE Support Vectors:&quot;, rmse(wine.test$predQualitySVM, wine.test$quality)) ## RMSE Linear regression: 0.7435729 ## RMSE Random Forest: 0.4237221 ## RMSE Support Vectors: 0.4010154 Programirajmo u R-u by Damir Pintar is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License.Based on a work at https://ratnip.github.io/FER_OPJR/ "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
