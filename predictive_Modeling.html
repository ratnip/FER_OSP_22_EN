<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>14 Introduction to Predictive Modeling | Let’s program in R</title>
  <meta name="description" content="R Coursebook" />
  <meta name="generator" content="bookdown 0.29 and GitBook 2.6.7" />

  <meta property="og:title" content="14 Introduction to Predictive Modeling | Let’s program in R" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="R Coursebook" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="14 Introduction to Predictive Modeling | Let’s program in R" />
  
  <meta name="twitter:description" content="R Coursebook" />
  

<meta name="author" content="Damir Pintar" />


<meta name="date" content="2023-01-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="regression.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>

<style type="text/css">
.showopt {
  background-color: #004c93;
  color: #FFFFFF; 
  width: 140px;
  height: 4	0px;
  text-align: center;
  vertical-align: middle !important;
  float: right;
  font-family: sans-serif;
  font-size:10 pt;
  border-radius: 8px;
}

.showopt:hover {
    background-color: #dfe4f2;
    color: #004c93;
}

pre.plot {
  background-color: white !important;
}
</style>

<script>
 $(document).ready(function() {

  $chunks = $('.fold');

  $chunks.each(function () {

    // add button to source code chunks
    if ( $(this).hasClass('s') ) {
      $('pre.r', this).prepend("<div class=\"showopt\">Show solution</div><br style=\"line-height:22px;\"/>");
      $('pre.r', this).children('code').attr('class', 'folded');
    }

    // add button to output chunks
    if ( $(this).hasClass('o') ) {
      $('pre:not(.r)', this).has('code').prepend("<div class=\"showopt\">Show results</div><br style=\"line-height:22px;\"/>");
      $('pre:not(.r)', this).children('code:not(r)').addClass('folded');

      // add button to plots
      $(this).find('img').wrap('<pre class=\"plot\"></pre>');
      $('pre.plot', this).prepend("<div class=\"showopt\">Show figure</div><br style=\"line-height:22px;\"/>");
      $('pre.plot', this).children('img').addClass('folded');

    }
  });

  // hide all chunks when document is loaded
  $('.folded').css('display', 'none')

  // function to toggle the visibility
  $('.showopt').click(function() {
    var label = $(this).html();
    if (label.indexOf("Show") >= 0) {
      $(this).html(label.replace("Show", "Hide"));
    } else {
      $(this).html(label.replace("Hide", "Show"));
    }
    $(this).siblings('code, img').slideToggle('fast', 'swing');
  });
});
</script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>(“Statistical Programming Fundamentals” course book)</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#foreword"><i class="fa fa-check"></i>Foreword</a></li>
</ul></li>
<li class="part"><span><b>I Basic elements of R</b></span></li>
<li class="chapter" data-level="1" data-path="introduction.html"><a href="introduction.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction.html"><a href="introduction.html#what-is-r"><i class="fa fa-check"></i><b>1.1</b> What is R?</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="introduction.html"><a href="introduction.html#general-facts-about-r"><i class="fa fa-check"></i><b>1.1.1</b> General facts about R</a></li>
<li class="chapter" data-level="1.1.2" data-path="introduction.html"><a href="introduction.html#r-alternatives"><i class="fa fa-check"></i><b>1.1.2</b> R alternatives</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="introduction.html"><a href="introduction.html#installing-software-support"><i class="fa fa-check"></i><b>1.2</b> Installing Software Support</a></li>
<li class="chapter" data-level="1.3" data-path="introduction.html"><a href="introduction.html#overview-of-the-development-interface-rstudio"><i class="fa fa-check"></i><b>1.3</b> Overview of the development interface <em>RStudio</em></a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introduction.html"><a href="introduction.html#interactive-console"><i class="fa fa-check"></i><b>1.3.1</b> Interactive console</a></li>
<li class="chapter" data-level="1.3.2" data-path="introduction.html"><a href="introduction.html#writing-r-scripts"><i class="fa fa-check"></i><b>1.3.2</b> Writing R scripts</a></li>
<li class="chapter" data-level="1.3.3" data-path="introduction.html"><a href="introduction.html#r-markdown"><i class="fa fa-check"></i><b>1.3.3</b> <em>R Markdown</em></a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introduction.html"><a href="introduction.html#how-to-use-this-coursebook"><i class="fa fa-check"></i><b>1.4</b> How to use this coursebook?</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="tipovi.html"><a href="tipovi.html"><i class="fa fa-check"></i><b>2</b> Basic data types and operators</a>
<ul>
<li class="chapter" data-level="2.1" data-path="tipovi.html"><a href="tipovi.html#basic-data-types"><i class="fa fa-check"></i><b>2.1</b> Basic data types</a></li>
<li class="chapter" data-level="2.2" data-path="tipovi.html"><a href="tipovi.html#operators"><i class="fa fa-check"></i><b>2.2</b> Operators</a></li>
<li class="chapter" data-level="2.3" data-path="tipovi.html"><a href="tipovi.html#missing-unknown-and-non-existent-values"><i class="fa fa-check"></i><b>2.3</b> Missing, unknown, and non-existent values</a></li>
<li class="chapter" data-level="" data-path="tipovi.html"><a href="tipovi.html#homework-exercises"><i class="fa fa-check"></i>Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="vektori.html"><a href="vektori.html"><i class="fa fa-check"></i><b>3</b> Vectors, matrices and lists</a>
<ul>
<li class="chapter" data-level="3.1" data-path="vektori.html"><a href="vektori.html#vector"><i class="fa fa-check"></i><b>3.1</b> Vector</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="vektori.html"><a href="vektori.html#creating-a-vector"><i class="fa fa-check"></i><b>3.1.1</b> Creating a vector</a></li>
<li class="chapter" data-level="3.1.2" data-path="vektori.html"><a href="vektori.html#vector-concatenation"><i class="fa fa-check"></i><b>3.1.2</b> Vector concatenation</a></li>
<li class="chapter" data-level="3.1.3" data-path="vektori.html"><a href="vektori.html#operator"><i class="fa fa-check"></i><b>3.1.3</b> Operator <code>[</code></a></li>
<li class="chapter" data-level="3.1.4" data-path="vektori.html"><a href="vektori.html#principles-of-vectorization-and-recycling"><i class="fa fa-check"></i><b>3.1.4</b> Principles of vectorization and recycling</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="vektori.html"><a href="vektori.html#index-vectors"><i class="fa fa-check"></i><b>3.2</b> Index vectors</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="vektori.html"><a href="vektori.html#positional-indexing"><i class="fa fa-check"></i><b>3.2.1</b> Positional Indexing</a></li>
<li class="chapter" data-level="3.2.2" data-path="vektori.html"><a href="vektori.html#conditional-indexing"><i class="fa fa-check"></i><b>3.2.2</b> Conditional indexing</a></li>
<li class="chapter" data-level="3.2.3" data-path="vektori.html"><a href="vektori.html#label-based-indexing"><i class="fa fa-check"></i><b>3.2.3</b> Label-based indexing</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="vektori.html"><a href="vektori.html#matrices-and-arrays"><i class="fa fa-check"></i><b>3.3</b> Matrices and arrays</a></li>
<li class="chapter" data-level="3.4" data-path="vektori.html"><a href="vektori.html#matrix-slicing"><i class="fa fa-check"></i><b>3.4</b> Matrix slicing</a></li>
<li class="chapter" data-level="3.5" data-path="vektori.html"><a href="vektori.html#example-3.2---matrix-slicing"><i class="fa fa-check"></i><b>3.5</b> Example 3.2 - matrix slicing</a></li>
<li class="chapter" data-level="3.6" data-path="vektori.html"><a href="vektori.html#lists"><i class="fa fa-check"></i><b>3.6</b> Lists</a></li>
<li class="chapter" data-level="" data-path="vektori.html"><a href="vektori.html#homework-exercises-1"><i class="fa fa-check"></i>Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="okviri.html"><a href="okviri.html"><i class="fa fa-check"></i><b>4</b> Data frames and factors</a>
<ul>
<li class="chapter" data-level="4.1" data-path="okviri.html"><a href="okviri.html#data-frames"><i class="fa fa-check"></i><b>4.1</b> Data frames</a></li>
<li class="chapter" data-level="4.2" data-path="okviri.html"><a href="okviri.html#selecting-rows-and-columns"><i class="fa fa-check"></i><b>4.2</b> Selecting rows and columns</a></li>
<li class="chapter" data-level="4.3" data-path="okviri.html"><a href="okviri.html#adding-and-deleting-rows-and-columns"><i class="fa fa-check"></i><b>4.3</b> Adding and deleting rows and columns</a></li>
<li class="chapter" data-level="4.4" data-path="okviri.html"><a href="okviri.html#factors"><i class="fa fa-check"></i><b>4.4</b> Factors</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="okviri.html"><a href="okviri.html#example-4.1---mishandling-factorization"><i class="fa fa-check"></i><b>4.4.1</b> Example 4.1 - mishandling factorization</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="okviri.html"><a href="okviri.html#homework-exercises-2"><i class="fa fa-check"></i>Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="control.html"><a href="control.html"><i class="fa fa-check"></i><b>5</b> Conditional statements and programming loops</a>
<ul>
<li class="chapter" data-level="5.1" data-path="control.html"><a href="control.html#flow-control-commands"><i class="fa fa-check"></i><b>5.1</b> Flow control commands</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="control.html"><a href="control.html#conditional-execution"><i class="fa fa-check"></i><b>5.1.1</b> Conditional execution</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="control.html"><a href="control.html#loops"><i class="fa fa-check"></i><b>5.2</b> Loops</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="control.html"><a href="control.html#therepeat-loop"><i class="fa fa-check"></i><b>5.2.1</b> The<code>repeat</code> loop</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="control.html"><a href="control.html#example-5.1---the-repeat-loop"><i class="fa fa-check"></i><b>5.3</b> Example 5.1 - the <code>repeat</code> loop</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="control.html"><a href="control.html#the-while-loop"><i class="fa fa-check"></i><b>5.3.1</b> The <code>while</code> loop</a></li>
<li class="chapter" data-level="5.3.2" data-path="control.html"><a href="control.html#the-for-loop"><i class="fa fa-check"></i><b>5.3.2</b> The <code>for</code> loop</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="control.html"><a href="control.html#homework-exercises-3"><i class="fa fa-check"></i>Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="packages.html"><a href="packages.html"><i class="fa fa-check"></i><b>6</b> Packages, built-in functions and environments</a>
<ul>
<li class="chapter" data-level="6.1" data-path="packages.html"><a href="packages.html#working-with-packages"><i class="fa fa-check"></i><b>6.1</b> Working with packages</a></li>
<li class="chapter" data-level="6.2" data-path="packages.html"><a href="packages.html#built-in-functions"><i class="fa fa-check"></i><b>6.2</b> Built-in functions</a></li>
<li class="chapter" data-level="6.3" data-path="packages.html"><a href="packages.html#environments"><i class="fa fa-check"></i><b>6.3</b> Environments</a></li>
<li class="chapter" data-level="" data-path="packages.html"><a href="packages.html#homework-exercises-4"><i class="fa fa-check"></i>Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="user.html"><a href="user.html"><i class="fa fa-check"></i><b>7</b> User Defined Functions</a>
<ul>
<li class="chapter" data-level="7.1" data-path="user.html"><a href="user.html#user-defined-functions"><i class="fa fa-check"></i><b>7.1</b> User defined functions</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="user.html"><a href="user.html#defining-a-new-function"><i class="fa fa-check"></i><b>7.1.1</b> Defining a new function</a></li>
<li class="chapter" data-level="7.1.2" data-path="user.html"><a href="user.html#the-copy-on-modify-principle"><i class="fa fa-check"></i><b>7.1.2</b> The “copy-on-modify” principle</a></li>
<li class="chapter" data-level="7.1.3" data-path="user.html"><a href="user.html#example-7.1---copy-on-modify"><i class="fa fa-check"></i><b>7.1.3</b> Example 7.1 - “copy-on-modify”</a></li>
<li class="chapter" data-level="7.1.4" data-path="user.html"><a href="user.html#example-7.2---copy-on-modify-2"><i class="fa fa-check"></i><b>7.1.4</b> Example 7.2 - “copy-on-modify (2)”</a></li>
<li class="chapter" data-level="7.1.5" data-path="user.html"><a href="user.html#example-7.3---copy-on-modify-and-environments"><i class="fa fa-check"></i><b>7.1.5</b> Example 7.3 - “copy-on-modify” and environments</a></li>
<li class="chapter" data-level="7.1.6" data-path="user.html"><a href="user.html#example-7.4---the---operator"><i class="fa fa-check"></i><b>7.1.6</b> Example 7.4 - the <code>&lt;&lt;-</code> operator</a></li>
<li class="chapter" data-level="7.1.7" data-path="user.html"><a href="user.html#example-7.5---changing-a-data-frame-column-with-the---operator"><i class="fa fa-check"></i><b>7.1.7</b> Example 7.5 - changing a data frame column with the <code>&lt;&lt;-</code> operator</a></li>
<li class="chapter" data-level="7.1.8" data-path="user.html"><a href="user.html#function-as-an-object"><i class="fa fa-check"></i><b>7.1.8</b> Function as an object</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="user.html"><a href="user.html#the-apply-family"><i class="fa fa-check"></i><b>7.2</b> The <code>apply</code> family</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="user.html"><a href="user.html#the-apply-function"><i class="fa fa-check"></i><b>7.2.1</b> The <code>apply</code> function</a></li>
<li class="chapter" data-level="7.2.2" data-path="user.html"><a href="user.html#the-lapply-sapply-and-vapply-functions"><i class="fa fa-check"></i><b>7.2.2</b> The <code>lapply</code>, <code>sapply</code> and <code>vapply</code> functions</a></li>
<li class="chapter" data-level="7.2.3" data-path="user.html"><a href="user.html#other-functions-from-the-apply-family-and-the-available-alternatives"><i class="fa fa-check"></i><b>7.2.3</b> Other functions from the <code>apply</code> family and the available alternatives</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="user.html"><a href="user.html#homework-exercises-5"><i class="fa fa-check"></i>Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="objects.html"><a href="objects.html"><i class="fa fa-check"></i><b>8</b> Object oriented systems in R</a>
<ul>
<li class="chapter" data-level="8.1" data-path="objects.html"><a href="objects.html#object-oriented-systems-in-r"><i class="fa fa-check"></i><b>8.1</b> Object-oriented systems in R</a></li>
<li class="chapter" data-level="8.2" data-path="objects.html"><a href="objects.html#overview-of-the-s3-object-model"><i class="fa fa-check"></i><b>8.2</b> Overview of the S3 object model</a></li>
<li class="chapter" data-level="8.3" data-path="objects.html"><a href="objects.html#generic-functions"><i class="fa fa-check"></i><b>8.3</b> Generic functions</a></li>
<li class="chapter" data-level="8.4" data-path="objects.html"><a href="objects.html#adding-generic-functions-to-an-object"><i class="fa fa-check"></i><b>8.4</b> Adding generic functions to an object</a>
<ul>
<li class="chapter" data-level="8.4.1" data-path="objects.html"><a href="objects.html#conclusion-on-s3-objects"><i class="fa fa-check"></i><b>8.4.1</b> Conclusion on S3 objects</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="objects.html"><a href="objects.html#short-overview-of-s4-objects"><i class="fa fa-check"></i><b>8.5</b> Short overview of S4 objects</a></li>
<li class="chapter" data-level="" data-path="objects.html"><a href="objects.html#homework-exercises-6"><i class="fa fa-check"></i>Homework exercises</a></li>
</ul></li>
<li class="part"><span><b>II Data management and visualizations</b></span></li>
<li class="chapter" data-level="9" data-path="pipe.html"><a href="pipe.html"><i class="fa fa-check"></i><b>9</b> Pipeline operator and tidy data</a>
<ul>
<li class="chapter" data-level="9.1" data-path="pipe.html"><a href="pipe.html#pipeline-operator"><i class="fa fa-check"></i><b>9.1</b> Pipeline operator</a></li>
<li class="chapter" data-level="9.2" data-path="pipe.html"><a href="pipe.html#tidy-data"><i class="fa fa-check"></i><b>9.2</b> Tidy data</a>
<ul>
<li class="chapter" data-level="9.2.1" data-path="pipe.html"><a href="pipe.html#the-pivot_longer-andpivot_wider-functions"><i class="fa fa-check"></i><b>9.2.1</b> The <code>pivot_longer</code> and<code>pivot_wider</code> functions</a></li>
<li class="chapter" data-level="9.2.2" data-path="pipe.html"><a href="pipe.html#the-separate-and-unite-functions"><i class="fa fa-check"></i><b>9.2.2</b> The <code>separate</code> and <code>unite</code> functions</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="pipe.html"><a href="pipe.html#homework-exercises-7"><i class="fa fa-check"></i>Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="dates.html"><a href="dates.html"><i class="fa fa-check"></i><b>10</b> Working with dates and character strings</a>
<ul>
<li class="chapter" data-level="10.1" data-path="dates.html"><a href="dates.html#working-with-dates"><i class="fa fa-check"></i><b>10.1</b> Working with dates</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="dates.html"><a href="dates.html#the-date-class"><i class="fa fa-check"></i><b>10.1.1</b> The <code>Date</code> class</a></li>
<li class="chapter" data-level="10.1.2" data-path="dates.html"><a href="dates.html#theposixct-andposixlt-classes"><i class="fa fa-check"></i><b>10.1.2</b> The<code>POSIXct</code> and<code>POSIXlt</code> classes</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="dates.html"><a href="dates.html#the-lubridate-package"><i class="fa fa-check"></i><b>10.2</b> The <code>lubridate</code> package</a></li>
<li class="chapter" data-level="10.3" data-path="dates.html"><a href="dates.html#working-with-character-strings"><i class="fa fa-check"></i><b>10.3</b> Working with character strings</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="dates.html"><a href="dates.html#text-analysis-and-regular-expressions"><i class="fa fa-check"></i><b>10.3.1</b> Text analysis and regular expressions</a></li>
<li class="chapter" data-level="10.3.2" data-path="dates.html"><a href="dates.html#the-stringr-package"><i class="fa fa-check"></i><b>10.3.2</b> The <code>stringr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="dates.html"><a href="dates.html#homework-exercises-8"><i class="fa fa-check"></i>Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="dplyr.html"><a href="dplyr.html"><i class="fa fa-check"></i><b>11</b> Data wrangling</a>
<ul>
<li class="chapter" data-level="11.1" data-path="dplyr.html"><a href="dplyr.html#data-wrangling-with-package-dplyr"><i class="fa fa-check"></i><b>11.1</b> Data wrangling with package <code>dplyr</code></a></li>
<li class="chapter" data-level="11.2" data-path="dplyr.html"><a href="dplyr.html#dataset-titanic"><i class="fa fa-check"></i><b>11.2</b> Dataset: <em>Titanic</em></a></li>
<li class="chapter" data-level="11.3" data-path="dplyr.html"><a href="dplyr.html#creating-a-subset-of-observations-with-filter-andslice"><i class="fa fa-check"></i><b>11.3</b> Creating a subset of observations with <code>filter</code> and<code>slice</code></a></li>
<li class="chapter" data-level="11.4" data-path="dplyr.html"><a href="dplyr.html#creating-a-subset-of-columns-with-select"><i class="fa fa-check"></i><b>11.4</b> Creating a subset of columns with <code>select</code></a></li>
<li class="chapter" data-level="11.5" data-path="dplyr.html"><a href="dplyr.html#creating-new-columns-with-mutate"><i class="fa fa-check"></i><b>11.5</b> Creating new columns with <code>mutate</code></a></li>
<li class="chapter" data-level="11.6" data-path="dplyr.html"><a href="dplyr.html#sample-dataset-houston-flights"><i class="fa fa-check"></i><b>11.6</b> Sample Dataset: <em>Houston flights</em></a></li>
<li class="chapter" data-level="11.7" data-path="dplyr.html"><a href="dplyr.html#grouping-and-aggregation-with-group_by-andsummarise"><i class="fa fa-check"></i><b>11.7</b> Grouping and aggregation with <code>group_by</code> and<code>summarise</code></a></li>
<li class="chapter" data-level="11.8" data-path="dplyr.html"><a href="dplyr.html#merging-data-frames-with-join-functions"><i class="fa fa-check"></i><b>11.8</b> Merging data frames with join functions</a></li>
<li class="chapter" data-level="11.9" data-path="dplyr.html"><a href="dplyr.html#integrating-dplyr-with-relational-databases"><i class="fa fa-check"></i><b>11.9</b> Integrating <code>dplyr</code> with relational databases</a></li>
<li class="chapter" data-level="" data-path="dplyr.html"><a href="dplyr.html#homework-exercises-9"><i class="fa fa-check"></i>Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="ggplot2.html"><a href="ggplot2.html"><i class="fa fa-check"></i><b>12</b> Visualising data with <code>ggplot2</code> package</a>
<ul>
<li class="chapter" data-level="12.1" data-path="ggplot2.html"><a href="ggplot2.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>12.1</b> Exploratory data analysis</a></li>
<li class="chapter" data-level="12.2" data-path="ggplot2.html"><a href="ggplot2.html#data-visualization-using-the-r-language"><i class="fa fa-check"></i><b>12.2</b> Data visualization using the R language</a></li>
<li class="chapter" data-level="12.3" data-path="ggplot2.html"><a href="ggplot2.html#grammar-of-graphics-and-the-ggplot2-package"><i class="fa fa-check"></i><b>12.3</b> Grammar of graphics and the <code>ggplot2</code> package</a>
<ul>
<li class="chapter" data-level="12.3.1" data-path="ggplot2.html"><a href="ggplot2.html#aspects-of-data-aesthetics-and-geometries"><i class="fa fa-check"></i><b>12.3.1</b> Aspects of data, aesthetics and geometries</a></li>
<li class="chapter" data-level="12.3.2" data-path="ggplot2.html"><a href="ggplot2.html#fixed-geometry-parameters"><i class="fa fa-check"></i><b>12.3.2</b> Fixed geometry parameters</a></li>
<li class="chapter" data-level="12.3.3" data-path="ggplot2.html"><a href="ggplot2.html#the-aspects-of-statistics-and-position"><i class="fa fa-check"></i><b>12.3.3</b> The aspects of statistics and position</a></li>
<li class="chapter" data-level="12.3.4" data-path="ggplot2.html"><a href="ggplot2.html#the-relationship-between-geometry-and-statistics"><i class="fa fa-check"></i><b>12.3.4</b> The relationship between geometry and statistics</a></li>
<li class="chapter" data-level="12.3.5" data-path="ggplot2.html"><a href="ggplot2.html#storing-a-graph-to-a-file"><i class="fa fa-check"></i><b>12.3.5</b> Storing a graph to a file</a></li>
<li class="chapter" data-level="12.3.6" data-path="ggplot2.html"><a href="ggplot2.html#aspects-of-scale-coordinate-system-and-theme"><i class="fa fa-check"></i><b>12.3.6</b> Aspects of scale, coordinate system and theme</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="ggplot2.html"><a href="ggplot2.html#tu"><i class="fa fa-check"></i><b>12.4</b> TU</a>
<ul>
<li class="chapter" data-level="12.4.1" data-path="ggplot2.html"><a href="ggplot2.html#conditional-faceted-graphs"><i class="fa fa-check"></i><b>12.4.1</b> Conditional (faceted) graphs</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="ggplot2.html"><a href="ggplot2.html#graphs-in-exploratory-analysis-and-reporting"><i class="fa fa-check"></i><b>12.5</b> Graphs in Exploratory Analysis and Reporting</a></li>
<li class="chapter" data-level="" data-path="ggplot2.html"><a href="ggplot2.html#exercises"><i class="fa fa-check"></i>Exercises</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>13</b> Selected Machine Learning Methods: Regression Analysis</a>
<ul>
<li class="chapter" data-level="13.1" data-path="regression.html"><a href="regression.html#machine-learning---a-short-introduction"><i class="fa fa-check"></i><b>13.1</b> Machine learning - a short introduction</a></li>
<li class="chapter" data-level="13.2" data-path="regression.html"><a href="regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>13.2</b> Simple linear regression</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="regression.html"><a href="regression.html#the-lm-function"><i class="fa fa-check"></i><b>13.2.1</b> The <code>lm</code> function</a></li>
<li class="chapter" data-level="13.2.2" data-path="regression.html"><a href="regression.html#linear-regression-and-categorical-variables"><i class="fa fa-check"></i><b>13.2.2</b> Linear regression and categorical variables</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="regression.html"><a href="regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>13.3</b> Multiple linear regression</a></li>
<li class="chapter" data-level="" data-path="regression.html"><a href="regression.html#homework-exercises-10"><i class="fa fa-check"></i>Homework exercises</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="predictive_Modeling.html"><a href="predictive_Modeling.html"><i class="fa fa-check"></i><b>14</b> Introduction to Predictive Modeling</a>
<ul>
<li class="chapter" data-level="14.1" data-path="predictive_Modeling.html"><a href="predictive_Modeling.html#what-is-predictive-modeling"><i class="fa fa-check"></i><b>14.1</b> What is predictive modeling?</a></li>
<li class="chapter" data-level="14.2" data-path="predictive_Modeling.html"><a href="predictive_Modeling.html#creating-training-and-test-datasets"><i class="fa fa-check"></i><b>14.2</b> Creating training and test datasets</a></li>
<li class="chapter" data-level="14.3" data-path="predictive_Modeling.html"><a href="predictive_Modeling.html#classification-predictive-models---knn-classification"><i class="fa fa-check"></i><b>14.3</b> Classification Predictive Models - kNN Classification</a></li>
<li class="chapter" data-level="14.4" data-path="predictive_Modeling.html"><a href="predictive_Modeling.html#package-caret-and-predictive-modeling"><i class="fa fa-check"></i><b>14.4</b> Package <code>caret</code> and predictive modeling</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Let’s program in R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="predictive_Modeling" class="section level1 hasAnchor" number="14">
<h1><span class="header-section-number">14</span> Introduction to Predictive Modeling<a href="predictive_Modeling.html#predictive_Modeling" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<hr />
<div id="what-is-predictive-modeling" class="section level2 hasAnchor" number="14.1">
<h2><span class="header-section-number">14.1</span> What is predictive modeling?<a href="predictive_Modeling.html#what-is-predictive-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous chapter, we have shown that with the help of linear regression we can investigate and model the interplay of two (or more) variables. If there is collinearity between the two observed variables, then their relation can be modeled with the help of a simple straight line equation, which then allows to estimate the value of the second variable from the known value of one of the variables. This is the basic idea of so-calle. “predictive modeling” - a process which relies on familiar inputs and developed predictive models to get information about unknown outputs, i.e. goals. Better models provide better, or more accurate results. Or, as defined by Kuhn and Johnson in his book “Applied Predictive Modeling”:</p>
<p><em>Predictive modeling is a process of developing a mathematical tool or model that is able to create accurate predictions.</em></p>
<p>Predictive modeling has numerous usage options in various domains eg:</p>
<ul>
<li>predicting the quantity of sold products or expected profits</li>
<li>identification of users planning to cancel the subscription</li>
<li>disease diagnostics</li>
<li>real estate evaluation</li>
<li>estimation of movies’s profits on opening weekened</li>
<li>spam detection</li>
<li>etc.</li>
</ul>
<p>When developing predictive models, it is very important to:</p>
<ul>
<li>have adequately prepared, high quality input data</li>
<li>choose a suitable method for creating a model</li>
<li>properly conduct the evaluation and validation process</li>
</ul>
<p>The role of analysts in this process is to conduct all the steps of analysis thoroughly and to avoid common traps and errors. Namely, predictive models can ultimately have poor performance for a number of reasons - poorly prepared data, poorly conducted validation, creating models which “overfit” the training data, etc.</p>
<p>In this chapter, we will get acquainted with some of the basic guidelines we must adhere to during predictive modeling. We will also familiarize ourselves with R language packages that allow us to leave complex steps to the computer, that is, to approach predictive modelling from high-level perspective while the computer independently performs low-level data preparation, modeling, and validation steps.</p>
</div>
<div id="creating-training-and-test-datasets" class="section level2 hasAnchor" number="14.2">
<h2><span class="header-section-number">14.2</span> Creating training and test datasets<a href="predictive_Modeling.html#creating-training-and-test-datasets" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>When we created linear models in the previous chapter, after the creation of the model, we looked at summarized model information with the help of <code>summary</code> function - estimation of predictor and target relationship, the value of residual standard error, the value of adjusted R-squared measure etc. This information has enabled us to evaluate the quality of developed models.</p>
<p>But in the whole process we ignored one key thing - all of this information was based on <em>data used to create the model itself</em>. In other words, we have gained insight into how well a model works over <em>known data</em>, which the model has already seen during the model’s creation. As a rule, it is much more important to evaluate how well a model works over <em>unknown data</em>, i.e. to estimate the quality of predictions once the model gets completely new data. We say that we want models that <strong>generalize well</strong>, i.e. models that have learned generic features of the domain entities described in the obtained data, and not just the specifics of the dataset used to develop the model itself. This latter phenomenon is called “overfitting”.</p>
<p>Very often, at the time of creating a model, we only have one dataset that we should also for training and model evaluation. As we have already said, using the same data for both purposes does not give us enough good information about how the model generalizes. A common procedure in this case is to <em>split the initial set into two parts - a training set and a test set</em>.</p>
<p>How do you decide which observations to assign to which set? How many observations should be put into training, and how many in the testing set? As a rule, the training set and test set should have enough observations so that the obtained results could be statistically relevant so assuming a sufficiently large input dataset, splitting in two equal parts could be a satisfactory solution. However, in practice, we usually reserve more observations for training than for testing, so it is customary to use a <em>70:30 spluit</em>, so 70% observations go into the training set, and the rest go in the test set.</p>
<p>As for the procedure of selecting observations (i.e. “sampling”), there are several common procedures:</p>
<ul>
<li>random selection</li>
<li>stratified random selection (ensuring equal representation of certain categories in both sets)</li>
<li>timestamp-based selection (if the time component is key, i.e. it is important to estimate how well past information predicts future events)</li>
</ul>
<p>Random selection is a most commonly used method while more sophisticated sampling methods can be chosen if there is a clear need for them based on the goals of the analysis.</p>
<p>Let’s now apply this knowledge to the development of a linear regression model by comparing its effectiveness on the training set, and then on the test data. For this we will use a new set of data related to the characteristics and quality of wine, <code>wines.csv</code>. This set was created by adapting the “Wine Quality Data Set” from the UCI Machine Learning Repository, available at[this link] (<a href="https://archive.ics.uci.edu/ml/datasets/wine+quality" class="uri">https://archive.ics.uci.edu/ml/datasets/wine+quality</a>).</p>
<p>In the next exercise, we’ll load this dataset and make some adjustments - we’ll categorize columns that obviously contain a category variable and remove rows with missing values. Lines with missing values often require a little more attention and a more sophisticated approach than simple removal, especially with larger amounts of missing values or potential additional information that these values denote. But in our case there are very few such rows, and when applying certain predictive modeling methods, they can create problems, so we will simply remove them. One of the quick ways to do this is by using the <code>complete.cases</code> function, which returns the indexes of all rows <em>which do not have any missing value</em> for the given data frame, or <code>na.omit</code> which will filter out all rows with <code>NA</code> values.</p>
<p><strong>Exercise 14.1 - <code>wine quality</code> dataset</strong></p>
<div class="sourceCode" id="cb701"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb701-1"><a href="predictive_Modeling.html#cb701-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load data from the` wines.csv` file</span></span>
<span id="cb701-2"><a href="predictive_Modeling.html#cb701-2" aria-hidden="true" tabindex="-1"></a><span class="co"># in a variable called `wine`</span></span>
<span id="cb701-3"><a href="predictive_Modeling.html#cb701-3" aria-hidden="true" tabindex="-1"></a><span class="co"># examine the loaded data frame</span></span>
<span id="cb701-4"><a href="predictive_Modeling.html#cb701-4" aria-hidden="true" tabindex="-1"></a><span class="co"># categorize columns as needed</span></span>
<span id="cb701-5"><a href="predictive_Modeling.html#cb701-5" aria-hidden="true" tabindex="-1"></a><span class="co"># and remove rows with missing values </span></span>
<span id="cb701-6"><a href="predictive_Modeling.html#cb701-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb701-7"><a href="predictive_Modeling.html#cb701-7" aria-hidden="true" tabindex="-1"></a><span class="co"># hint: check out a function called `na.omit` (or `complete.cases`)</span></span></code></pre></div>
<div class="fold s o">
<div class="sourceCode" id="cb702"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb702-1"><a href="predictive_Modeling.html#cb702-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load data from the` wines.csv` file</span></span>
<span id="cb702-2"><a href="predictive_Modeling.html#cb702-2" aria-hidden="true" tabindex="-1"></a><span class="co"># in a variable called `wine`</span></span>
<span id="cb702-3"><a href="predictive_Modeling.html#cb702-3" aria-hidden="true" tabindex="-1"></a><span class="co"># examine the loaded data frame</span></span>
<span id="cb702-4"><a href="predictive_Modeling.html#cb702-4" aria-hidden="true" tabindex="-1"></a><span class="co"># categorize columns as needed</span></span>
<span id="cb702-5"><a href="predictive_Modeling.html#cb702-5" aria-hidden="true" tabindex="-1"></a><span class="co"># and remove rows with missing values </span></span>
<span id="cb702-6"><a href="predictive_Modeling.html#cb702-6" aria-hidden="true" tabindex="-1"></a>wine <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(<span class="st">&quot;wines.csv&quot;</span>)</span>
<span id="cb702-7"><a href="predictive_Modeling.html#cb702-7" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(wine)</span>
<span id="cb702-8"><a href="predictive_Modeling.html#cb702-8" aria-hidden="true" tabindex="-1"></a>wine<span class="sc">$</span>type <span class="ot">&lt;-</span> <span class="fu">factor</span>(wine<span class="sc">$</span>type)</span>
<span id="cb702-9"><a href="predictive_Modeling.html#cb702-9" aria-hidden="true" tabindex="-1"></a>wine <span class="ot">&lt;-</span> wine[<span class="fu">complete.cases</span>(wine),]  <span class="co"># or na.omit(wine)</span></span></code></pre></div>
<pre><code>## Rows: 6497 Columns: 13
## ── Column specification ────────────────────────────────────────────────────────
## Delimiter: &quot;,&quot;
## chr  (1): type
## dbl (12): fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlo...
## 
## ℹ Use `spec()` to retrieve the full column specification for this data.
## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.</code></pre>
<pre><code>## Rows: 6,497
## Columns: 13
## $ fixed.acidity        &lt;dbl&gt; 7.4, 7.8, 7.8, 11.2, 7.4, 7.4, 7.9, 7.3, 7.8, 7.5…
## $ volatile.acidity     &lt;dbl&gt; 0.700, 0.880, 0.760, 0.280, 0.700, 0.660, 0.600, …
## $ citric.acid          &lt;dbl&gt; 0.00, 0.00, 0.04, 0.56, 0.00, 0.00, 0.06, 0.00, 0…
## $ residual.sugar       &lt;dbl&gt; 1.9, 2.6, 2.3, 1.9, 1.9, 1.8, 1.6, 1.2, 2.0, 6.1,…
## $ chlorides            &lt;dbl&gt; 0.076, 0.098, 0.092, 0.075, 0.076, 0.075, 0.069, …
## $ free.sulfur.dioxide  &lt;dbl&gt; 11, 25, 15, 17, 11, 13, 15, 15, 9, 17, 15, 17, 16…
## $ total.sulfur.dioxide &lt;dbl&gt; 34, 67, 54, 60, 34, 40, 59, 21, 18, 102, 65, 102,…
## $ density              &lt;dbl&gt; 0.9978, 0.9968, 0.9970, 0.9980, 0.9978, 0.9978, 0…
## $ pH                   &lt;dbl&gt; 3.51, 3.20, 3.26, 3.16, 3.51, 3.51, 3.30, 3.39, 3…
## $ sulphates            &lt;dbl&gt; 0.56, 0.68, 0.65, 0.58, 0.56, 0.56, 0.46, 0.47, 0…
## $ alcohol              &lt;dbl&gt; 9.4, 9.8, 9.8, 9.8, 9.4, 9.4, 9.4, 10.0, 9.5, 10.…
## $ quality              &lt;dbl&gt; 5, 5, 5, 6, 5, 5, 5, 7, 7, 5, 5, 5, 5, 5, 5, 5, 7…
## $ type                 &lt;chr&gt; &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, &quot;red&quot;, …</code></pre>
</div>
<p>Suppose the attribute <code>quality</code> is the target variable and all the other columns are potential predictors.</p>
<p>How do you split this set on the training set and test set using random selection? There are several ways to do this, and even specialized functions and packages for this purpose, but we will learn a simple and easy-to-understand method that uses the <code>sample</code> function. If we assume that <code>df</code> is the data frame that we want to split into a training and test set that we will store in <code>df.train</code> and <code>df.test</code> variables, then random selection can be done as follows:</p>
<div class="sourceCode" id="cb705"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb705-1"><a href="predictive_Modeling.html#cb705-1" aria-hidden="true" tabindex="-1"></a>train_size <span class="ot">&lt;-</span> <span class="fl">0.7</span> <span class="sc">*</span> <span class="fu">nrow</span>(df) <span class="sc">%&gt;%</span> round <span class="co"># about 70%</span></span>
<span id="cb705-2"><a href="predictive_Modeling.html#cb705-2" aria-hidden="true" tabindex="-1"></a>train_ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(df), train_size) <span class="co"># indexes of training observations</span></span>
<span id="cb705-3"><a href="predictive_Modeling.html#cb705-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb705-4"><a href="predictive_Modeling.html#cb705-4" aria-hidden="true" tabindex="-1"></a>df.train <span class="ot">&lt;-</span> df[train_ind, ]</span>
<span id="cb705-5"><a href="predictive_Modeling.html#cb705-5" aria-hidden="true" tabindex="-1"></a>df.test <span class="ot">&lt;-</span> df[<span class="sc">-</span>train_ind, ]</span></code></pre></div>
<p>Let’s apply this to our wines dataset and then train a linear regression model.</p>
<p><strong>Exercise 14.2 - Splitting the input dataset and training a model</strong></p>
<div class="sourceCode" id="cb706"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb706-1"><a href="predictive_Modeling.html#cb706-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb706-2"><a href="predictive_Modeling.html#cb706-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb706-3"><a href="predictive_Modeling.html#cb706-3" aria-hidden="true" tabindex="-1"></a><span class="co"># split the `wine` dataframe into `wine.train` and `wine.test`</span></span>
<span id="cb706-4"><a href="predictive_Modeling.html#cb706-4" aria-hidden="true" tabindex="-1"></a><span class="co"># using a random selection method and 70:30 ratio</span></span>
<span id="cb706-5"><a href="predictive_Modeling.html#cb706-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb706-6"><a href="predictive_Modeling.html#cb706-6" aria-hidden="true" tabindex="-1"></a><span class="co"># train a linear regression model using the `wine.train` set</span></span>
<span id="cb706-7"><a href="predictive_Modeling.html#cb706-7" aria-hidden="true" tabindex="-1"></a><span class="co"># store the model in a variable called `linMod`</span></span>
<span id="cb706-8"><a href="predictive_Modeling.html#cb706-8" aria-hidden="true" tabindex="-1"></a><span class="co"># target variable is `quality` and all other variables are predictors</span></span>
<span id="cb706-9"><a href="predictive_Modeling.html#cb706-9" aria-hidden="true" tabindex="-1"></a><span class="co"># check the summary for the obtained model</span></span></code></pre></div>
<div class="fold s o">
<div class="sourceCode" id="cb707"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb707-1"><a href="predictive_Modeling.html#cb707-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb707-2"><a href="predictive_Modeling.html#cb707-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb707-3"><a href="predictive_Modeling.html#cb707-3" aria-hidden="true" tabindex="-1"></a><span class="co"># split the `wine` dataframe into `wine.train` and `wine.test`</span></span>
<span id="cb707-4"><a href="predictive_Modeling.html#cb707-4" aria-hidden="true" tabindex="-1"></a><span class="co"># using a random selection method and 70:30 ratio</span></span>
<span id="cb707-5"><a href="predictive_Modeling.html#cb707-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb707-6"><a href="predictive_Modeling.html#cb707-6" aria-hidden="true" tabindex="-1"></a>train_ind <span class="ot">&lt;-</span> <span class="fu">sample</span>(<span class="dv">1</span><span class="sc">:</span><span class="fu">nrow</span>(wine), <span class="fl">0.7</span> <span class="sc">*</span> <span class="fu">nrow</span>(wine) <span class="sc">%&gt;%</span> round)</span>
<span id="cb707-7"><a href="predictive_Modeling.html#cb707-7" aria-hidden="true" tabindex="-1"></a>wine.train <span class="ot">&lt;-</span> wine[train_ind, ]</span>
<span id="cb707-8"><a href="predictive_Modeling.html#cb707-8" aria-hidden="true" tabindex="-1"></a>wine.test <span class="ot">&lt;-</span> wine[<span class="sc">-</span>train_ind, ]</span></code></pre></div>
</div>
<div class="fold s o">
<div class="sourceCode" id="cb708"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb708-1"><a href="predictive_Modeling.html#cb708-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train a linear regression model using the `wine.train` set</span></span>
<span id="cb708-2"><a href="predictive_Modeling.html#cb708-2" aria-hidden="true" tabindex="-1"></a><span class="co"># store the model in a variable called `linMod`</span></span>
<span id="cb708-3"><a href="predictive_Modeling.html#cb708-3" aria-hidden="true" tabindex="-1"></a><span class="co"># target variable is `quality` and all other variables are predictors</span></span>
<span id="cb708-4"><a href="predictive_Modeling.html#cb708-4" aria-hidden="true" tabindex="-1"></a><span class="co"># check the summary for the obtained model</span></span>
<span id="cb708-5"><a href="predictive_Modeling.html#cb708-5" aria-hidden="true" tabindex="-1"></a>linMod <span class="ot">&lt;-</span> <span class="fu">lm</span>(quality <span class="sc">~</span> ., <span class="at">data =</span> wine.train)</span>
<span id="cb708-6"><a href="predictive_Modeling.html#cb708-6" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linMod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = quality ~ ., data = wine.train)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6077 -0.4616 -0.0409  0.4637  3.0352 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)           1.226e+02  1.846e+01   6.643 3.44e-11 ***
## fixed.acidity         9.438e-02  1.955e-02   4.828 1.42e-06 ***
## volatile.acidity     -1.600e+00  9.558e-02 -16.736  &lt; 2e-16 ***
## citric.acid          -1.373e-01  9.414e-02  -1.459 0.144664    
## residual.sugar        6.943e-02  7.438e-03   9.335  &lt; 2e-16 ***
## chlorides            -6.106e-01  3.887e-01  -1.571 0.116274    
## free.sulfur.dioxide   5.488e-03  9.148e-04   5.999 2.14e-09 ***
## total.sulfur.dioxide -1.340e-03  3.805e-04  -3.522 0.000432 ***
## density              -1.218e+02  1.871e+01  -6.513 8.18e-11 ***
## pH                    5.275e-01  1.106e-01   4.770 1.90e-06 ***
## sulphates             7.499e-01  9.096e-02   8.244  &lt; 2e-16 ***
## alcohol               2.069e-01  2.335e-02   8.859  &lt; 2e-16 ***
## typewhite            -4.546e-01  6.927e-02  -6.563 5.87e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7292 on 4533 degrees of freedom
## Multiple R-squared:  0.3106, Adjusted R-squared:  0.3088 
## F-statistic: 170.2 on 12 and 4533 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<p>We see that the adjusted R-squared measure is relatively low and the average error is quite high, so linear regression is perhaps not the best option this scenario (or maybe the obtained characteristics of the wines are simply not good enough to predict its quality). In spite of this, we will now check how well the model works on unseen observations.</p>
<p>In order to check the quality of our model, we have to choose the criterion to which we will adhere. For numeric goals we often use the <strong>RMSE</strong> measure (root mean square error):</p>
<p><span class="math display">\[RMSE = \sqrt{\frac{\sum_{i = 1}^{n}(\widehat{y}_{i} - y_{i})^2}{n}}\]</span></p>
<p>An alternative to this measure is <strong>MAE</strong> (<em>mean absolute error</em>), which is calculated with the help of the following formula:</p>
<p><span class="math display">\[MAE = \frac{\sum_{i = 1}^{n}|\widehat{y}_{i} - y_{i}|}{n}\]</span>
The measure <strong>MAE</strong> is a bit more intuitive but also mathematically more complex when performing optimization functions. Furthermore, <strong>RMSE</strong> measures takes priority when we want to more strongly penalize larger prediction mistakes.</p>
<p>Although there are packages that contain functions that calculate these measures, it is not difficult to create them manually. Below we will focus on the <strong>RMSE</strong> measure, while leaving measure <strong>MAE</strong> for use as an optional exercise.</p>
<p><strong>Exercise 14.3 - Function for calculating the RMSE measure</strong></p>
<div class="sourceCode" id="cb710"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb710-1"><a href="predictive_Modeling.html#cb710-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a `rmse` function that will use prediction vector</span></span>
<span id="cb710-2"><a href="predictive_Modeling.html#cb710-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and a vector of actual target vales as parameters</span></span>
<span id="cb710-3"><a href="predictive_Modeling.html#cb710-3" aria-hidden="true" tabindex="-1"></a><span class="co"># and calculate the RMSE measure according to the above formula</span></span></code></pre></div>
<div class="fold s o">
<div class="sourceCode" id="cb711"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb711-1"><a href="predictive_Modeling.html#cb711-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a `rmse` function that will use prediction vector</span></span>
<span id="cb711-2"><a href="predictive_Modeling.html#cb711-2" aria-hidden="true" tabindex="-1"></a><span class="co"># and a vector of actual target vales as parameters</span></span>
<span id="cb711-3"><a href="predictive_Modeling.html#cb711-3" aria-hidden="true" tabindex="-1"></a><span class="co"># and calculate the RMSE measure according to the above formula</span></span>
<span id="cb711-4"><a href="predictive_Modeling.html#cb711-4" aria-hidden="true" tabindex="-1"></a>rmse <span class="ot">&lt;-</span> <span class="cf">function</span>(pred, act) <span class="fu">sqrt</span>(<span class="fu">mean</span>((pred <span class="sc">-</span> act) <span class="sc">**</span> <span class="dv">2</span>))</span></code></pre></div>
</div>
<p>Let us now add prection columns to <code>wine.train</code> and<code>wine.test</code> datasets and then calculate the value of the RMSE measure for both.</p>
<p><strong>Exercise 14.4 - Model evaluation for the training and test set </strong></p>
<div class="sourceCode" id="cb712"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb712-1"><a href="predictive_Modeling.html#cb712-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add a `predQualityLR` column to `wine.train` and `wine.test` </span></span>
<span id="cb712-2"><a href="predictive_Modeling.html#cb712-2" aria-hidden="true" tabindex="-1"></a><span class="co"># using the `predict` function and the `linMod` model</span></span>
<span id="cb712-3"><a href="predictive_Modeling.html#cb712-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb712-4"><a href="predictive_Modeling.html#cb712-4" aria-hidden="true" tabindex="-1"></a><span class="co"># print the value of the RMSE measure for both sets</span></span>
<span id="cb712-5"><a href="predictive_Modeling.html#cb712-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb712-6"><a href="predictive_Modeling.html#cb712-6" aria-hidden="true" tabindex="-1"></a><span class="co"># remove the `predQualityLR` column from the `wine.train` dataset</span></span></code></pre></div>
<div class="fold s o">
<div class="sourceCode" id="cb713"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb713-1"><a href="predictive_Modeling.html#cb713-1" aria-hidden="true" tabindex="-1"></a><span class="co"># add a `predQualityLR` column to `wine.train` and `wine.test` </span></span>
<span id="cb713-2"><a href="predictive_Modeling.html#cb713-2" aria-hidden="true" tabindex="-1"></a><span class="co"># using the `predict` function and the `linMod` model</span></span>
<span id="cb713-3"><a href="predictive_Modeling.html#cb713-3" aria-hidden="true" tabindex="-1"></a>wine.train<span class="sc">$</span>predQualityLR <span class="ot">&lt;-</span> <span class="fu">predict</span>(linMod, wine.train)</span>
<span id="cb713-4"><a href="predictive_Modeling.html#cb713-4" aria-hidden="true" tabindex="-1"></a>wine.test<span class="sc">$</span>predQualityLR <span class="ot">&lt;-</span> <span class="fu">predict</span>(linMod, wine.test)</span>
<span id="cb713-5"><a href="predictive_Modeling.html#cb713-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb713-6"><a href="predictive_Modeling.html#cb713-6" aria-hidden="true" tabindex="-1"></a><span class="co"># print the value of the RMSE measure for both sets</span></span>
<span id="cb713-7"><a href="predictive_Modeling.html#cb713-7" aria-hidden="true" tabindex="-1"></a><span class="fu">rmse</span>(wine.train<span class="sc">$</span>predQualityLR, wine.train<span class="sc">$</span>quality)</span>
<span id="cb713-8"><a href="predictive_Modeling.html#cb713-8" aria-hidden="true" tabindex="-1"></a><span class="fu">rmse</span>(wine.test<span class="sc">$</span>predQualityLR, wine.test<span class="sc">$</span>quality)</span>
<span id="cb713-9"><a href="predictive_Modeling.html#cb713-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb713-10"><a href="predictive_Modeling.html#cb713-10" aria-hidden="true" tabindex="-1"></a><span class="co"># remove the `predQualityLR` column from the `wine.train` dataset</span></span>
<span id="cb713-11"><a href="predictive_Modeling.html#cb713-11" aria-hidden="true" tabindex="-1"></a>wine.train<span class="sc">$</span>predQualityLR <span class="ot">&lt;-</span> <span class="cn">NULL</span></span></code></pre></div>
<pre><code>## [1] 0.7281868
## [1] 0.7435729</code></pre>
</div>
<p>We see that the RMSE for the training set roughly corresponds to the residual standard error obtained in the model summary (a small difference is a result of the fact that we used the total number of observations, while in the calculation of the residual standard error we used the number of degrees of freedom - “dependent” variables, i.e. predictors). What is interesting is the fact that the value of RMSE measure of the test set is not dramatically different than the RMSE measure, which means that the model works almost equally well (or badly) on new data.</p>
<hr />
</div>
<div id="classification-predictive-models---knn-classification" class="section level2 hasAnchor" number="14.3">
<h2><span class="header-section-number">14.3</span> Classification Predictive Models - kNN Classification<a href="predictive_Modeling.html#classification-predictive-models---knn-classification" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The linear regression method allowed us to “guess” the numerical target variable. It is reasonable to ask the question - can we build a predictive model that will try to estimate the value of a categoric variable? This is an extremely important goal of predictive modeling since many domains have problems with determining the value of a particular category (the patient is ill or not, the device is corrupt or correct, the transaction is regular ora result of fraud, the client will default on the loan or not etc.)</p>
<p>These are so-called “classification” problems for which - just as for the “regression” problems which use numerical goals - there is an extremely large set of developed methods. Very often, similar methods can be used for both purposes (sometimes with certain adjustments), so for example even if linear regression is not specifically suited for classification problems, its related “logistic regression” method is a very effective and popular approach to such a type of problem.</p>
<p>In this section, we will focus on another, very popular classification method, which is very intuitive and easy to understand, but which allows us to inspect a number of interesting predictive modeling elements that we have not mentioned so far. It is a method called “k nearest neighbors”, or “kNN classification”.</p>
<p>This method works in a very simple way. If we do not know the category of some observation, we simply find a number of observations which are the “closest” to that new observation. We then look at their categories, and then by majority vote determine which category to assign to the new observation. Let’s try to visualize this.</p>
<p><strong>Exercise 14.5 - Visualization of the main idea for the method of kNN classification </strong></p>
<div class="sourceCode" id="cb715"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb715-1"><a href="predictive_Modeling.html#cb715-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a scatterplot for `wine.train` dataset</span></span>
<span id="cb715-2"><a href="predictive_Modeling.html#cb715-2" aria-hidden="true" tabindex="-1"></a><span class="co"># put `chlorides` on `x` axis</span></span>
<span id="cb715-3"><a href="predictive_Modeling.html#cb715-3" aria-hidden="true" tabindex="-1"></a><span class="co"># and `volatile.acidity` on `y` axis</span></span>
<span id="cb715-4"><a href="predictive_Modeling.html#cb715-4" aria-hidden="true" tabindex="-1"></a><span class="co"># color point based on wine type</span></span>
<span id="cb715-5"><a href="predictive_Modeling.html#cb715-5" aria-hidden="true" tabindex="-1"></a><span class="co"># set the transparency of the points to 0.5</span></span></code></pre></div>
<div class="fold s o">
<div class="sourceCode" id="cb716"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb716-1"><a href="predictive_Modeling.html#cb716-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a scatterplot for `wine.train` dataset</span></span>
<span id="cb716-2"><a href="predictive_Modeling.html#cb716-2" aria-hidden="true" tabindex="-1"></a><span class="co"># put `chlorides` on `x` axis</span></span>
<span id="cb716-3"><a href="predictive_Modeling.html#cb716-3" aria-hidden="true" tabindex="-1"></a><span class="co"># and `volatile.acidity` on `y` axis</span></span>
<span id="cb716-4"><a href="predictive_Modeling.html#cb716-4" aria-hidden="true" tabindex="-1"></a><span class="co"># color point based on wine type</span></span>
<span id="cb716-5"><a href="predictive_Modeling.html#cb716-5" aria-hidden="true" tabindex="-1"></a><span class="co"># set the transparency of the points to 0.5</span></span>
<span id="cb716-6"><a href="predictive_Modeling.html#cb716-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(wine.train, <span class="fu">aes</span>(chlorides, volatile.acidity, <span class="at">col =</span> type)) <span class="sc">+</span> <span class="fu">geom_point</span>(<span class="at">alpha =</span> <span class="fl">0.5</span>)</span></code></pre></div>
<p><img src="osnoveR_files/figure-html/unnamed-chunk-531-1.png" width="672" /></p>
</div>
<p>On the graph we can clearly see how the points form “clusters” of the same colors in certain areas. If we take a new observation regarding the wine of an unknown type, but the known values of the measures set on the x and y axes, by looking at its immediate neighborhood, we could conclude which type of wine the new wine belongs to. Observations deep within one of the “clouds” are very likely to belong to the type shown (although we see that there are exceptions!). On the other hand, observation within the “boundary” areas is much more likely to be misdiagnosed, and the different choice of neighboring numbers could result in different classification results.</p>
<p>kNN classification is based on the <strong>concept of distance</strong>. Although there are different options for distance selection, we often rely on the so-called “Euclidean distance”, which is easily presented in the two- and three-dimensional Cartesian system by the shortest path between two points, and we can easily calculate its value using their coordinates and Pythagorean theorem. This distance is easily applied to n-dimensional spaces, so although we can not easily visualize points in a space whose dimension corresponds to the number of predictors, we can still easily calculate the value of the Euclidean distance between points.</p>
<p>So, the way kNN classification works can be easily described as follows:</p>
<ul>
<li>the training set itself represents “domain knowledge”, that is, the predictive model itself</li>
<li>for each new observation, we find <em>k</em> closest observations from the training set and assign the category to it by using the majority vote</li>
</ul>
<p>Although this process is relatively simple, there are some common questions which require answers. Firstly - how do we select the parameter <em>k</em>? Secondly, do we need to make some additional preparatory actions on the data set before we perform the kNN classification?</p>
<p>Let’s deal with the second question first. Examine the graph we have drawn, more precisely its coordinate axes. We can see that the lengths of the axes do not necessarily scale to their numerical values equally, i.e. the unit interval on the x axis is not necessarily equal to the unit interval on y axis. This is normal and expected, since the values on axes do not necessarily have to use the same scale nor even the measuring unit. But there is one problem - when we use Euclidean distance, it treats all axes equally, which means that variables with larger ranges will automatically gain greater importance (e.g. the maximum value of the chlorides is around 0.611 while the maximum value of sulfur dioxide variable reaches over 3000!)</p>
<p>Here we see the importance of <strong>data pre-processing </strong>. For kNN classification, <strong>normalization of numeric variables</strong> is recommended, i.e. the transformation of numeric variables in such a way that we deduct their average and divide them by standard deviation, which brings them all to the same scale. This process is somewhat more complex than it seems, because we have to make sure that <em>new observations are also normalized in the same way</em>. This means that we need to make sure both training and test numerical variables need to come from the same distribution. If we have enough data and both training and test observations are representative, then their means and standard deviations should be close enough so we can easily normalize each set independently. If we have small test sets, then the normalization of the test set should be done by remembering using the mean and standard deviation of the training set.</p>
<p>To simplify this process somwhat, we will return to the original <code>wine</code> dataset and simply normalize the numeric columns beforehand, and then store it in the <code>wineNorm</code> variable (we can pretend that we knew means and standard deviations of populations of these variables beforehand so we used them to normalize both training and test set independetly). Then, we will split this set into sets of <code>wineNorm.train</code> and<code>wineNorm.test</code> analogously to the previous procedure.</p>
<p>`r zadHead(“Normalization of numeric variables of the input dataset”)</p>
<div class="sourceCode" id="cb717"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb717-1"><a href="predictive_Modeling.html#cb717-1" aria-hidden="true" tabindex="-1"></a><span class="co"># normalize all numeric columns of the `wine` data frame</span></span>
<span id="cb717-2"><a href="predictive_Modeling.html#cb717-2" aria-hidden="true" tabindex="-1"></a><span class="co"># store the result in the `wineNorm` variable</span></span>
<span id="cb717-3"><a href="predictive_Modeling.html#cb717-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb717-4"><a href="predictive_Modeling.html#cb717-4" aria-hidden="true" tabindex="-1"></a><span class="co"># use the `train_ind` object to split `wineNorm` </span></span>
<span id="cb717-5"><a href="predictive_Modeling.html#cb717-5" aria-hidden="true" tabindex="-1"></a><span class="co"># into `wineNorm.train` and `wineNorm.test`</span></span></code></pre></div>
<div class="fold s o">
<div class="sourceCode" id="cb718"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb718-1"><a href="predictive_Modeling.html#cb718-1" aria-hidden="true" tabindex="-1"></a><span class="co"># normalize all the numeric columns of the `wine` data frame</span></span>
<span id="cb718-2"><a href="predictive_Modeling.html#cb718-2" aria-hidden="true" tabindex="-1"></a><span class="co"># store the result in the `wineNorm` variable</span></span>
<span id="cb718-3"><a href="predictive_Modeling.html#cb718-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb718-4"><a href="predictive_Modeling.html#cb718-4" aria-hidden="true" tabindex="-1"></a>wineNorm <span class="ot">&lt;-</span> <span class="fu">lapply</span>(wine, <span class="cf">function</span>(x) {</span>
<span id="cb718-5"><a href="predictive_Modeling.html#cb718-5" aria-hidden="true" tabindex="-1"></a>  <span class="cf">if</span>(<span class="fu">is.numeric</span>(x))(x <span class="sc">-</span> <span class="fu">mean</span>(x)) <span class="sc">/</span> <span class="fu">sd</span>(x)</span>
<span id="cb718-6"><a href="predictive_Modeling.html#cb718-6" aria-hidden="true" tabindex="-1"></a>  <span class="cf">else</span> x</span>
<span id="cb718-7"><a href="predictive_Modeling.html#cb718-7" aria-hidden="true" tabindex="-1"></a>})<span class="sc">%&gt;%</span> as.data.frame</span>
<span id="cb718-8"><a href="predictive_Modeling.html#cb718-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb718-9"><a href="predictive_Modeling.html#cb718-9" aria-hidden="true" tabindex="-1"></a><span class="co"># use the `train_ind` object to split `wineNorm` </span></span>
<span id="cb718-10"><a href="predictive_Modeling.html#cb718-10" aria-hidden="true" tabindex="-1"></a><span class="co"># into `wineNorm.train` and `wineNorm.test`</span></span>
<span id="cb718-11"><a href="predictive_Modeling.html#cb718-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb718-12"><a href="predictive_Modeling.html#cb718-12" aria-hidden="true" tabindex="-1"></a>wineNorm.train <span class="ot">&lt;-</span> wineNorm[train_ind,]</span>
<span id="cb718-13"><a href="predictive_Modeling.html#cb718-13" aria-hidden="true" tabindex="-1"></a>wineNorm.test <span class="ot">&lt;-</span> wineNorm[<span class="sc">-</span>train_ind,]</span></code></pre></div>
</div>
<p>Let’s go back to the problem of selecting the value of the <code>k</code> parameter. How to pick the right value? Generally speaking, this problem is called “choosing a hyperparameter of a model,” since the model besides input data requires some additional input parameters to perform its function. Usually, when no mathematical method of calculating optimal hyperparameters exist, the only option we have is training models with various combinations of hyperparameters and finally selecting those hyperparameter which result in models showing the best performance. In this case, we often need another (third) part of the original dataset, usually called a “validation” dataset, which represents an additional step before testing the set in which we select the values of the hyperparameters for the “final” model. Specifically, for kNN classification:</p>
<ul>
<li>we train a larger number of models for different parameter values <code>k</code> over the training set</li>
<li>with the help of the validation set, we find the model that has the best performance and choose its <code>k</code></li>
<li>we use chosen <code>k</code> and the entire training and validation set to train the model</li>
<li>we make a final evaluation on the test set</li>
</ul>
<p>Below we will use a simplified version of this process which uses only the training and test set while setting the <code>k</code> parameter arbitrarily to <code>5</code>. We will leave the entire process of possibly finding a better hyperparameter as an optional exercise. Likewise, as we will see at the end of this chapter, we often do not manually program instruction for finding best hyperparameters, but rather use high-level functions which allow us to only set things up declaratively, without having to deal with low-grade details about the implementation of the process itself.</p>
<p>Let’s try to see if we can properly classify wine as “white” or “red” with the help of variables describing its chemical composition. We will use the kNN method, with the number of neighbors set to 5. Predictors will be all available variables except <code>quality</code> and<code>type</code>.</p>
<p>For kNN classification the base R offers the <code>knn</code> function . For our needs we will use the <code>knn3</code> function from the<code>caret</code> package. The function <code>knn3</code> expands the basic <code>knn</code> function in a way that allows us to call it by following the standard programming conventions we have already learned when training linear regression models:</p>
<div class="sourceCode" id="cb719"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb719-1"><a href="predictive_Modeling.html#cb719-1" aria-hidden="true" tabindex="-1"></a>model <span class="ot">&lt;-</span> <span class="fu">selected_method</span>(formula, training_dataset, additional_parameters)</span>
<span id="cb719-2"><a href="predictive_Modeling.html#cb719-2" aria-hidden="true" tabindex="-1"></a>predictions <span class="ot">&lt;-</span> <span class="fu">predict</span>(model, test_dataset, additional_parameters)</span></code></pre></div>
<p>In the case of the <code>knn3</code> function, an additional parameter in training the model is <code>k</code>, set to <code>5</code>. When creating predictions, we will set the <code>type</code> parameter to <code>class</code>, meaning we want to predict the class itself (alternative is <code>probs</code>, returning predicted probabilities for each class).</p>
<p>Let’s try to create a kNN model with the help of <code>wineNorm.train</code> and then find wine type predictions for <code>wineNorm.test</code>.</p>
<p><strong>Exercise 14.6 - kNN Classification </strong></p>
<div class="sourceCode" id="cb720"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb720-1"><a href="predictive_Modeling.html#cb720-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create the variable `kNN5Mod` which will be</span></span>
<span id="cb720-2"><a href="predictive_Modeling.html#cb720-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the result of calling `knn3` over the `wineNorm.train` set</span></span>
<span id="cb720-3"><a href="predictive_Modeling.html#cb720-3" aria-hidden="true" tabindex="-1"></a><span class="co"># target variable is `type`</span></span>
<span id="cb720-4"><a href="predictive_Modeling.html#cb720-4" aria-hidden="true" tabindex="-1"></a><span class="co"># predictors are all other variables except `quality`</span></span>
<span id="cb720-5"><a href="predictive_Modeling.html#cb720-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb720-6"><a href="predictive_Modeling.html#cb720-6" aria-hidden="true" tabindex="-1"></a><span class="co"># add a `predictedType` column to `wineNorm.test`</span></span>
<span id="cb720-7"><a href="predictive_Modeling.html#cb720-7" aria-hidden="true" tabindex="-1"></a><span class="co"># which will store a result of calling the `predict` function</span></span>
<span id="cb720-8"><a href="predictive_Modeling.html#cb720-8" aria-hidden="true" tabindex="-1"></a><span class="co"># with `kNN5Mod` as a model and `wineNorm.test` as new data</span></span>
<span id="cb720-9"><a href="predictive_Modeling.html#cb720-9" aria-hidden="true" tabindex="-1"></a><span class="co"># set the `type` parameter to `class`</span></span></code></pre></div>
<div class="fold s o">
<div class="sourceCode" id="cb721"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb721-1"><a href="predictive_Modeling.html#cb721-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create the variable `kNN5Mod` which will be</span></span>
<span id="cb721-2"><a href="predictive_Modeling.html#cb721-2" aria-hidden="true" tabindex="-1"></a><span class="co"># the result of calling `knn3` over the `wineNorm.train` set</span></span>
<span id="cb721-3"><a href="predictive_Modeling.html#cb721-3" aria-hidden="true" tabindex="-1"></a><span class="co"># target variable is `type`</span></span>
<span id="cb721-4"><a href="predictive_Modeling.html#cb721-4" aria-hidden="true" tabindex="-1"></a><span class="co"># predictors are all other variables except `quality`</span></span>
<span id="cb721-5"><a href="predictive_Modeling.html#cb721-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb721-6"><a href="predictive_Modeling.html#cb721-6" aria-hidden="true" tabindex="-1"></a><span class="co">#library(caret) # if needed</span></span>
<span id="cb721-7"><a href="predictive_Modeling.html#cb721-7" aria-hidden="true" tabindex="-1"></a>kNN5Mod <span class="ot">&lt;-</span> <span class="fu">knn3</span>(type <span class="sc">~</span> . <span class="sc">-</span> quality, <span class="at">data =</span> wineNorm.train, <span class="at">k =</span> <span class="dv">5</span>)</span>
<span id="cb721-8"><a href="predictive_Modeling.html#cb721-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb721-9"><a href="predictive_Modeling.html#cb721-9" aria-hidden="true" tabindex="-1"></a><span class="co"># add a `predictedType` column to `wineNorm.test`</span></span>
<span id="cb721-10"><a href="predictive_Modeling.html#cb721-10" aria-hidden="true" tabindex="-1"></a><span class="co"># which will store a result of calling the `predict` function</span></span>
<span id="cb721-11"><a href="predictive_Modeling.html#cb721-11" aria-hidden="true" tabindex="-1"></a><span class="co"># with `kNN5Mod` as a model and `wineNorm.test` as new data</span></span>
<span id="cb721-12"><a href="predictive_Modeling.html#cb721-12" aria-hidden="true" tabindex="-1"></a><span class="co"># set the `type` parameter to `class`</span></span>
<span id="cb721-13"><a href="predictive_Modeling.html#cb721-13" aria-hidden="true" tabindex="-1"></a>wineNorm.test<span class="sc">$</span>predictedType <span class="ot">&lt;-</span> <span class="fu">predict</span>(kNN5Mod, wineNorm.test, <span class="at">type =</span> <span class="st">&quot;class&quot;</span>)</span></code></pre></div>
</div>
<p>Note that we did not look for a model summary since we can not get too much information in this case. The kNN classifier can not provide us with some aggregated information about “learned” knowledge, it is just a “map of the domain space” that is then used for each new observation to determine which category it belongs to.</p>
<p>How do we check the classifier’s performance? A typical procedure (with binary classifiers) is the creation of a so-calle “confusion matrix”. This simply means that we will create a table that will show how well the predicted values match the actual values. The easiest way to create this table is simply to call the <code>table</code> function with the prediction column and the actual value column as parameters.</p>
<p><strong>Exercise 14.7 - Simple Confusion Matrix </strong></p>
<div class="sourceCode" id="cb722"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb722-1"><a href="predictive_Modeling.html#cb722-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print a confusion matrix by calling the `table` function</span></span>
<span id="cb722-2"><a href="predictive_Modeling.html#cb722-2" aria-hidden="true" tabindex="-1"></a><span class="co"># over the appropriate columns of the `wineNorm.test` dataset</span></span></code></pre></div>
<div class="fold s o">
<div class="sourceCode" id="cb723"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb723-1"><a href="predictive_Modeling.html#cb723-1" aria-hidden="true" tabindex="-1"></a><span class="co"># print a confusion matrix by calling the `table` function</span></span>
<span id="cb723-2"><a href="predictive_Modeling.html#cb723-2" aria-hidden="true" tabindex="-1"></a><span class="co"># over the appropriate columns of the `wineNorm.test` dataset</span></span>
<span id="cb723-3"><a href="predictive_Modeling.html#cb723-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(wineNorm.test<span class="sc">$</span>predictedType, wineNorm.test<span class="sc">$</span>type)</span></code></pre></div>
<pre><code>##        
##          red white
##   red    464     5
##   white   10  1470</code></pre>
</div>
<p>Looking at the results we can intuitively conclude that in this case the classifier works extremely well, that is, white and red wines can be very easily classified by looking at their chemical properties. But we often want to describe the quality of the classifier using an objective, numerical measure. There are a number of such measures, and most of them can be directly calculated using values from the confusion matrix.</p>
<p>Specifically, if we call the confusion matrix cells <em>TP</em>, <em>TN</em>, <em>FP</em>, <em>FN</em> (<em>true positive, true negative, false positive, false negative</em>), where we treat one class as “positive” and hence assigning the names of the corresponding cells depending on whether the classifier correctly guessed the class (main diagonal) or not (side diagonal). In this case, from the confusion matrix we can directly calculate the following measures:</p>
<ul>
<li><em>accuracy</em>: <span class="math inline">\(\frac{TP + TN}{TP + FP + TN + FN}\)</span></li>
<li><em>sensitivity</em> (<em>recall</em>): <span class="math inline">\(\frac{TP}{TP + FN}\)</span></li>
<li><em>precision</em>: <span class="math inline">\(\frac{TP}{TP + FP}\)</span></li>
<li><em>false positive rate</em>/<em>false negative rate</em>: <span class="math inline">\(\frac{FP}{FP + TN}\)</span> ; <span class="math inline">\(\frac{FN}{TP + FN}\)</span></li>
<li>etc.</li>
</ul>
<p>These are just some of the possible measures. Although accuracy may be the most logical choice (because we actually get a percentage of correctly guessed observations), we often have to be careful because it can give us a distorted picture of the classifier’s effectiveness, especially in when there is a huge disbalance in categories or when one type of error is far more dangerous than the other. A typical example is the diagnosis of rare diseases - if the disease occurs in only 0.1% of cases, then the trivial classifier, which for all observations diagnoses that the disease is not present, works well in 99.9% cases. Also, if it is a dangerous disease, then <em>FP</em> error (the disease is diagnosed although not present) is far less important than <em>FN</em> errors (the disease is present but is not recognized). In these cases, selecting another measure (eg “sensitivity” or “false negative rate”) is often a much better quality indicator of the classifier.</p>
<p>We can very easily manually calculate all these measures using base R. However, the <code>confusionMatrix</code> function from the <code>caret</code> package (which also leverages the <code>e1071</code> package) gives us the same result as the <code>table</code> function but with the added convenience of computing a large number of measures that can help us evaluate the classifier’s quality.</p>
<p><strong>Exercise 14.8 - <code>confusionMatrix</code> function</strong></p>
<div class="sourceCode" id="cb725"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb725-1"><a href="predictive_Modeling.html#cb725-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a variable called `confMat` variable</span></span>
<span id="cb725-2"><a href="predictive_Modeling.html#cb725-2" aria-hidden="true" tabindex="-1"></a><span class="co"># which will stort the result of the `confusionMatrix` function</span></span>
<span id="cb725-3"><a href="predictive_Modeling.html#cb725-3" aria-hidden="true" tabindex="-1"></a><span class="co"># using the appropriate columns of the `wineNorm.test` dataset as parameters</span></span>
<span id="cb725-4"><a href="predictive_Modeling.html#cb725-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb725-5"><a href="predictive_Modeling.html#cb725-5" aria-hidden="true" tabindex="-1"></a><span class="co"># print the  `confMat` variable</span></span></code></pre></div>
<div class="fold s o">
<div class="sourceCode" id="cb726"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb726-1"><a href="predictive_Modeling.html#cb726-1" aria-hidden="true" tabindex="-1"></a><span class="co"># create a variable called `confMat` variable</span></span>
<span id="cb726-2"><a href="predictive_Modeling.html#cb726-2" aria-hidden="true" tabindex="-1"></a><span class="co"># which will stort the result of the `confusionMatrix` function</span></span>
<span id="cb726-3"><a href="predictive_Modeling.html#cb726-3" aria-hidden="true" tabindex="-1"></a><span class="co"># using the appropriate columns of the `wineNorm.test` dataset as parameters</span></span>
<span id="cb726-4"><a href="predictive_Modeling.html#cb726-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb726-5"><a href="predictive_Modeling.html#cb726-5" aria-hidden="true" tabindex="-1"></a><span class="co">#library(e1071) # if needed</span></span>
<span id="cb726-6"><a href="predictive_Modeling.html#cb726-6" aria-hidden="true" tabindex="-1"></a><span class="co">#library(caret) # if needed</span></span>
<span id="cb726-7"><a href="predictive_Modeling.html#cb726-7" aria-hidden="true" tabindex="-1"></a>confMat <span class="ot">&lt;-</span> <span class="fu">confusionMatrix</span>(wineNorm.test<span class="sc">$</span>predictedType, wineNorm.test<span class="sc">$</span>type)</span>
<span id="cb726-8"><a href="predictive_Modeling.html#cb726-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb726-9"><a href="predictive_Modeling.html#cb726-9" aria-hidden="true" tabindex="-1"></a><span class="co"># print the  `confMat` variable</span></span>
<span id="cb726-10"><a href="predictive_Modeling.html#cb726-10" aria-hidden="true" tabindex="-1"></a>confMat</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  red white
##      red    464     5
##      white   10  1470
##                                           
##                Accuracy : 0.9923          
##                  95% CI : (0.9873, 0.9957)
##     No Information Rate : 0.7568          
##     P-Value [Acc &gt; NIR] : &lt;2e-16          
##                                           
##                   Kappa : 0.979           
##                                           
##  Mcnemar&#39;s Test P-Value : 0.3017          
##                                           
##             Sensitivity : 0.9789          
##             Specificity : 0.9966          
##          Pos Pred Value : 0.9893          
##          Neg Pred Value : 0.9932          
##              Prevalence : 0.2432          
##          Detection Rate : 0.2381          
##    Detection Prevalence : 0.2406          
##       Balanced Accuracy : 0.9878          
##                                           
##        &#39;Positive&#39; Class : red             
## </code></pre>
</div>
<p>Since the variable <code>confMat</code> is an S3 object, with the help of the <code>unlist</code> function and the selection of the desired element, we can easily obtain only the numeric value of the measure we are interested in. This is useful if we want to integrate this function in our programming scripts.</p>
</div>
<div id="package-caret-and-predictive-modeling" class="section level2 hasAnchor" number="14.4">
<h2><span class="header-section-number">14.4</span> Package <code>caret</code> and predictive modeling<a href="predictive_Modeling.html#package-caret-and-predictive-modeling" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the previous chapter we were already introduced to the <code>caret</code> package, more specifically some of its functions that help us with predictive modeling. This package actually offers a lot more than we have seen. Specifically, the <code>caret</code> package provides a set of tools to effectively perform all elements of the predictive modeling process:</p>
<ul>
<li>splitting data</li>
<li>pre-processing of data</li>
<li>feature selection</li>
<li>adjusting the model with the help of re-sampling</li>
<li>variable importance estimation</li>
</ul>
<p>As the <code>dplyr</code> package actually changes the way we use the R language to manage the data frames, so the<code>caret</code> package enables a thorough modification of the predictive modeling approach used when programming in R. The functions of the <code>caret</code> package not only provide a cleaner syntax for low-level jobs, they also give the possibility of leveraging high-level approach for predictive modeling, where we declare declarative calls for <em>what</em> we want to do, and let R do low-level jobs returning us the corresponding result.</p>
<p>Details of this package can be found at [this link] (<a href="https://topepo.github.io/caret/index.html" class="uri">https://topepo.github.io/caret/index.html</a>), and below we will only give you a brief insight into some of the most useful features of this package.</p>
<p>To demonstrate the declarative nature of predictive modeling with this package, we will look at two functions: <code>train</code> and<code>trainControl</code>.</p>
<p>The <code>train</code> function is actually a generic interface to a large number of predictive models (a list of all the models currently supported by the function can be [found here] (<a href="http://topepo.github.io/caret/train-models-by-tag.html" class="uri">http://topepo.github.io/caret/train-models-by-tag.html</a>) . In a large number of cases, the this function call is not different from the call of the predictive modeling method functions we have learned so far, specifically <code>lm</code> and<code>knn3</code>. The biggest difference is that instead of calling a specific function, here we define the method of predictive modeling using the <code>method</code> parameter.</p>
<p>Let’s try to train a linear regression model using the <code>train</code> function and the previously prepared <code>wine.train</code> dataset.</p>
<p><strong>Exercise 14.9 - <code>train</code> function and linear regression </strong></p>
<div class="sourceCode" id="cb728"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb728-1"><a href="predictive_Modeling.html#cb728-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using the  `train` function from the `caret` package</span></span>
<span id="cb728-2"><a href="predictive_Modeling.html#cb728-2" aria-hidden="true" tabindex="-1"></a><span class="co"># train a linear regression model using the `wine.train` dataset</span></span>
<span id="cb728-3"><a href="predictive_Modeling.html#cb728-3" aria-hidden="true" tabindex="-1"></a><span class="co"># target variable is `quality` and all other variables are predictors</span></span>
<span id="cb728-4"><a href="predictive_Modeling.html#cb728-4" aria-hidden="true" tabindex="-1"></a><span class="co"># set the `method` `&quot;lm&quot;` </span></span>
<span id="cb728-5"><a href="predictive_Modeling.html#cb728-5" aria-hidden="true" tabindex="-1"></a><span class="co"># store the resuling model in a variable called `linMod`</span></span></code></pre></div>
<div class="fold s o">
<div class="sourceCode" id="cb729"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb729-1"><a href="predictive_Modeling.html#cb729-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using the  `train` function from the `caret` package</span></span>
<span id="cb729-2"><a href="predictive_Modeling.html#cb729-2" aria-hidden="true" tabindex="-1"></a><span class="co"># train a linear regression model using the `wine.train` dataset</span></span>
<span id="cb729-3"><a href="predictive_Modeling.html#cb729-3" aria-hidden="true" tabindex="-1"></a><span class="co"># target variable is `quality` and all other variables are predictors</span></span>
<span id="cb729-4"><a href="predictive_Modeling.html#cb729-4" aria-hidden="true" tabindex="-1"></a><span class="co"># set the `method` `&quot;lm&quot;` </span></span>
<span id="cb729-5"><a href="predictive_Modeling.html#cb729-5" aria-hidden="true" tabindex="-1"></a><span class="co"># store the resuling model in a variable called `linMod`</span></span>
<span id="cb729-6"><a href="predictive_Modeling.html#cb729-6" aria-hidden="true" tabindex="-1"></a>linMod <span class="ot">&lt;-</span> <span class="fu">train</span>(quality <span class="sc">~</span>., <span class="at">data =</span> wine.train, <span class="at">method =</span> <span class="st">&quot;lm&quot;</span>)</span>
<span id="cb729-7"><a href="predictive_Modeling.html#cb729-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb729-8"><a href="predictive_Modeling.html#cb729-8" aria-hidden="true" tabindex="-1"></a><span class="co"># read the summary of the obtained model</span></span>
<span id="cb729-9"><a href="predictive_Modeling.html#cb729-9" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(linMod)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6077 -0.4616 -0.0409  0.4637  3.0352 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)           1.226e+02  1.846e+01   6.643 3.44e-11 ***
## fixed.acidity         9.438e-02  1.955e-02   4.828 1.42e-06 ***
## volatile.acidity     -1.600e+00  9.558e-02 -16.736  &lt; 2e-16 ***
## citric.acid          -1.373e-01  9.414e-02  -1.459 0.144664    
## residual.sugar        6.943e-02  7.438e-03   9.335  &lt; 2e-16 ***
## chlorides            -6.106e-01  3.887e-01  -1.571 0.116274    
## free.sulfur.dioxide   5.488e-03  9.148e-04   5.999 2.14e-09 ***
## total.sulfur.dioxide -1.340e-03  3.805e-04  -3.522 0.000432 ***
## density              -1.218e+02  1.871e+01  -6.513 8.18e-11 ***
## pH                    5.275e-01  1.106e-01   4.770 1.90e-06 ***
## sulphates             7.499e-01  9.096e-02   8.244  &lt; 2e-16 ***
## alcohol               2.069e-01  2.335e-02   8.859  &lt; 2e-16 ***
## typewhite            -4.546e-01  6.927e-02  -6.563 5.87e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.7292 on 4533 degrees of freedom
## Multiple R-squared:  0.3106, Adjusted R-squared:  0.3088 
## F-statistic: 170.2 on 12 and 4533 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<p>We see that we get the same result as when we called the <code>lm</code> function directly at the beginning of this chapter.</p>
<p>Note that in this case we haven’t used a large number of parameters of the <code>train</code> function that we can see in the documentation. For example, using the <code>preprocess</code> parameter can automatically perform some data preparation procedures such as normalization, BoxCox transformation, imputation of missing values, and so on.</p>
<p>Let’s create a little more complex predictive model now. First, let’s examine a “control object” we can create using a function called <code>trainControl</code>.
This function provides us with a “control panel” which allows us to fine-tune all parameters related to the training of our predictive model. Some of these parameters relate to so-called “data re-sampling” - this means that we can get a better estimate of the behavior of developed predictive models if we perform a process called “cross-validation”, where the training set is split multiple times and then a model is trained over and over again, always using a separate holdover part as the test set. In this way we actually get a number of models, each with their own results, which give us information not only about the quality but also the stability of the model.</p>
<p>Additionally, we have the option of tweaking additional parameters, such as which summary funcation we want to apply on the resulting model, or whether we want the model (in the case of classification) to return the probabilities or just the resulting category. When finally create such a “control object”, we can recycle it as much as we want in the future steps of predictive modeling without the need to enter a large number of training-related parameters every time.</p>
<p>A simple control object can look like this:</p>
<div class="sourceCode" id="cb731"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb731-1"><a href="predictive_Modeling.html#cb731-1" aria-hidden="true" tabindex="-1"></a><span class="co"># We use repeated cross-validation with 5 repeats</span></span>
<span id="cb731-2"><a href="predictive_Modeling.html#cb731-2" aria-hidden="true" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb731-3"><a href="predictive_Modeling.html#cb731-3" aria-hidden="true" tabindex="-1"></a>                     <span class="at">repeats =</span> <span class="dv">5</span>)</span></code></pre></div>
<p>Finally, let’s try to use two slightly more advanced methods of predictive modeling - the random forest method (the <code>ranger</code> method from the package of a same name) and the support vector method (the<code>svmRadial</code> method from the <code>kernlab</code>) package. We will not go deeper into the details of these methods, just focus on how to call them with the help of the <code>trainCtrl</code> and<code>train</code> functions.</p>
<p>In the following code we will also leverage a function called <code>expand.grid</code>. This function requires a vector different values of the hyperparameters we have provided, and will result in a dataframe containing all combinations of these hyperparameters. It is most commonly used in conjunction with the <code>tuneGrid</code> parameter to assign the candidate sets for predictive model hyperparameter - in this way, the <code>train</code> function will try out all combinations of the default parameters and (in conjunction with the cross validation method) select the parameters that showcase the best performance for the final model.</p>
<div class="sourceCode" id="cb732"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb732-1"><a href="predictive_Modeling.html#cb732-1" aria-hidden="true" tabindex="-1"></a><span class="co">#library(ranger) # if needed</span></span>
<span id="cb732-2"><a href="predictive_Modeling.html#cb732-2" aria-hidden="true" tabindex="-1"></a><span class="co">#library(kernlab) # if needed</span></span>
<span id="cb732-3"><a href="predictive_Modeling.html#cb732-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb732-4"><a href="predictive_Modeling.html#cb732-4" aria-hidden="true" tabindex="-1"></a><span class="co"># we will use the same control object for both models</span></span>
<span id="cb732-5"><a href="predictive_Modeling.html#cb732-5" aria-hidden="true" tabindex="-1"></a><span class="co"># set `verboseIter` to TRUE</span></span>
<span id="cb732-6"><a href="predictive_Modeling.html#cb732-6" aria-hidden="true" tabindex="-1"></a><span class="co"># for  better insight into training speed!</span></span>
<span id="cb732-7"><a href="predictive_Modeling.html#cb732-7" aria-hidden="true" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(</span>
<span id="cb732-8"><a href="predictive_Modeling.html#cb732-8" aria-hidden="true" tabindex="-1"></a>  <span class="at">method =</span> <span class="st">&quot;repeatedcv&quot;</span>,</span>
<span id="cb732-9"><a href="predictive_Modeling.html#cb732-9" aria-hidden="true" tabindex="-1"></a>  <span class="at">number =</span> <span class="dv">5</span>,</span>
<span id="cb732-10"><a href="predictive_Modeling.html#cb732-10" aria-hidden="true" tabindex="-1"></a>  <span class="at">repeats =</span> <span class="dv">2</span>,</span>
<span id="cb732-11"><a href="predictive_Modeling.html#cb732-11" aria-hidden="true" tabindex="-1"></a>  <span class="at">verboseIter =</span> <span class="cn">FALSE</span>)</span>
<span id="cb732-12"><a href="predictive_Modeling.html#cb732-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb732-13"><a href="predictive_Modeling.html#cb732-13" aria-hidden="true" tabindex="-1"></a><span class="co"># random forest model</span></span>
<span id="cb732-14"><a href="predictive_Modeling.html#cb732-14" aria-hidden="true" tabindex="-1"></a>rfMod <span class="ot">&lt;-</span> <span class="fu">train</span>(quality <span class="sc">~</span>.,</span>
<span id="cb732-15"><a href="predictive_Modeling.html#cb732-15" aria-hidden="true" tabindex="-1"></a>                 <span class="at">data =</span> wine.train,</span>
<span id="cb732-16"><a href="predictive_Modeling.html#cb732-16" aria-hidden="true" tabindex="-1"></a>                 <span class="at">method =</span> <span class="st">&#39;ranger&#39;</span>,</span>
<span id="cb732-17"><a href="predictive_Modeling.html#cb732-17" aria-hidden="true" tabindex="-1"></a>                 <span class="at">tuneLength =</span> <span class="dv">10</span>,</span>
<span id="cb732-18"><a href="predictive_Modeling.html#cb732-18" aria-hidden="true" tabindex="-1"></a>                 <span class="at">trControl =</span> ctrl,</span>
<span id="cb732-19"><a href="predictive_Modeling.html#cb732-19" aria-hidden="true" tabindex="-1"></a>                 <span class="at">num.trees =</span> <span class="dv">10</span>)</span>
<span id="cb732-20"><a href="predictive_Modeling.html#cb732-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb732-21"><a href="predictive_Modeling.html#cb732-21" aria-hidden="true" tabindex="-1"></a><span class="co"># support vector model</span></span>
<span id="cb732-22"><a href="predictive_Modeling.html#cb732-22" aria-hidden="true" tabindex="-1"></a><span class="co"># we use a grid of hyperparameters</span></span>
<span id="cb732-23"><a href="predictive_Modeling.html#cb732-23" aria-hidden="true" tabindex="-1"></a><span class="co"># and pre-process data with normalization</span></span>
<span id="cb732-24"><a href="predictive_Modeling.html#cb732-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb732-25"><a href="predictive_Modeling.html#cb732-25" aria-hidden="true" tabindex="-1"></a>svm.grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">C =</span> <span class="fu">c</span>(<span class="dv">2</span>, <span class="dv">4</span>, <span class="dv">8</span>), <span class="at">sigma =</span> <span class="fu">c</span>(<span class="fl">0.25</span>, <span class="dv">1</span>, <span class="dv">2</span>))</span>
<span id="cb732-26"><a href="predictive_Modeling.html#cb732-26" aria-hidden="true" tabindex="-1"></a>svmMod <span class="ot">&lt;-</span> <span class="fu">train</span>(quality <span class="sc">~</span>.,</span>
<span id="cb732-27"><a href="predictive_Modeling.html#cb732-27" aria-hidden="true" tabindex="-1"></a>                   <span class="at">data =</span> wine.train,</span>
<span id="cb732-28"><a href="predictive_Modeling.html#cb732-28" aria-hidden="true" tabindex="-1"></a>                   <span class="at">method =</span> <span class="st">&quot;svmRadial&quot;</span>,</span>
<span id="cb732-29"><a href="predictive_Modeling.html#cb732-29" aria-hidden="true" tabindex="-1"></a>                   <span class="at">trControl =</span> ctrl,</span>
<span id="cb732-30"><a href="predictive_Modeling.html#cb732-30" aria-hidden="true" tabindex="-1"></a>                   <span class="at">tuneGrid =</span> svm.grid,</span>
<span id="cb732-31"><a href="predictive_Modeling.html#cb732-31" aria-hidden="true" tabindex="-1"></a>                   <span class="at">preProcess =</span> <span class="fu">c</span>(<span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>))</span></code></pre></div>
<p>Finally, we make a simple evaluation of the model with the help of RMSE measures.</p>
<p><strong>Exercise 14.10 - Simple Model Evaluation</strong></p>
<div class="sourceCode" id="cb733"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb733-1"><a href="predictive_Modeling.html#cb733-1" aria-hidden="true" tabindex="-1"></a><span class="co"># with the help of the `predict` function and` rfMod` and `svmMod` models</span></span>
<span id="cb733-2"><a href="predictive_Modeling.html#cb733-2" aria-hidden="true" tabindex="-1"></a><span class="co"># add `predQualityRF` and `predQualitySVM` columns to `wine.test` dataset</span></span>
<span id="cb733-3"><a href="predictive_Modeling.html#cb733-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb733-4"><a href="predictive_Modeling.html#cb733-4" aria-hidden="true" tabindex="-1"></a><span class="co"># print the value of the RMSE measure for all obtained models</span></span></code></pre></div>
<div class="fold s o">
<div class="sourceCode" id="cb734"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb734-1"><a href="predictive_Modeling.html#cb734-1" aria-hidden="true" tabindex="-1"></a><span class="co"># with the help of the `predict` function and` rfMod` and `svmMod` models</span></span>
<span id="cb734-2"><a href="predictive_Modeling.html#cb734-2" aria-hidden="true" tabindex="-1"></a><span class="co"># add `predQualityRF` and `predQualitySVM` columns to `wine.test` dataset</span></span>
<span id="cb734-3"><a href="predictive_Modeling.html#cb734-3" aria-hidden="true" tabindex="-1"></a>wine.test<span class="sc">$</span>predQualityRF <span class="ot">&lt;-</span> <span class="fu">predict</span>(rfMod, wine.test)</span></code></pre></div>
<pre><code>## Warning in predict.ranger.forest(forest, data, predict.all, num.trees, type, :
## Forest grown in ranger version &lt;0.11.5, converting ...</code></pre>
<div class="sourceCode" id="cb736"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb736-1"><a href="predictive_Modeling.html#cb736-1" aria-hidden="true" tabindex="-1"></a>wine.test<span class="sc">$</span>predQualitySVM <span class="ot">&lt;-</span> <span class="fu">predict</span>(svmMod, wine.test)</span>
<span id="cb736-2"><a href="predictive_Modeling.html#cb736-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb736-3"><a href="predictive_Modeling.html#cb736-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb736-4"><a href="predictive_Modeling.html#cb736-4" aria-hidden="true" tabindex="-1"></a><span class="co"># print the value of the RMSE measure for all obtained models</span></span>
<span id="cb736-5"><a href="predictive_Modeling.html#cb736-5" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;RMSE Linear regression:&quot;</span>, <span class="fu">rmse</span>(wine.test<span class="sc">$</span>predQualityLR, wine.test<span class="sc">$</span>quality))</span>
<span id="cb736-6"><a href="predictive_Modeling.html#cb736-6" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">RMSE Random Forest:&quot;</span>, <span class="fu">rmse</span>(wine.test<span class="sc">$</span>predQualityRF, wine.test<span class="sc">$</span>quality))</span>
<span id="cb736-7"><a href="predictive_Modeling.html#cb736-7" aria-hidden="true" tabindex="-1"></a><span class="fu">cat</span>(<span class="st">&quot;</span><span class="sc">\n</span><span class="st">RMSE Support Vectors:&quot;</span>, <span class="fu">rmse</span>(wine.test<span class="sc">$</span>predQualitySVM, wine.test<span class="sc">$</span>quality))</span></code></pre></div>
<pre><code>## RMSE Linear regression: 0.7435729
## RMSE Random Forest: 0.4237221
## RMSE Support Vectors: 0.4010154</code></pre>
</div>
<p><a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-nd/4.0/88x31.png" /></a><br /><span xmlns:dct="http://purl.org/dc/terms/" property="dct:title">Programirajmo u R-u</span> by <a xmlns:cc="http://creativecommons.org/ns#" href="https://www.fer.unizg.hr/en/damir.pintar" property="cc:attributionName" rel="cc:attributionURL">Damir Pintar</a> is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-nc-nd/4.0/">Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License</a>.<br />Based on a work at <a xmlns:dct="http://purl.org/dc/terms/" href="https://ratnip.github.io/FER_OPJR/" rel="dct:source">https://ratnip.github.io/FER_OPJR/</a></p>

<div id="refs" class="references csl-bib-body hanging-indent">
<div class="csl-entry">
Bache, Stefan Milton, and Hadley Wickham. 2014. <em>Magrittr: A Forward-Pipe Operator for r</em>. <a href="https://CRAN.R-project.org/package=magrittr">https://CRAN.R-project.org/package=magrittr</a>.
</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="regression.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"],
"google": false,
"instapper": false
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section",
"scroll_highlight": true
},
"toolbar": {
"position": "fixed",
"edit": null,
"download": null,
"search": true
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
